{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "centerfusion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bIRP7RxAnxRK",
        "Gd6uoarYXmmZ",
        "_R8ISQlBZPvg",
        "MnZKhEDOiLKN",
        "VjJq9-l818ju",
        "fEHSAMrPLAna",
        "T16D30JO0NKG",
        "kqhhuADy89kl",
        "-JoDqBsSC7_K",
        "zSj7TX8EDLB8",
        "_cCxeISBOx87",
        "D4ZXROYiOZDw",
        "ononB62gOMG6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CenterFusion - radar camera fusion for object detection\n",
        "TODO: Add citations"
      ],
      "metadata": {
        "id": "VqQCsxnp7jrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla Network"
      ],
      "metadata": {
        "id": "bIRP7RxAnxRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "Gd6uoarYXmmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/mrnabati/CenterFusion.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXlLGZ8DXyMb",
        "outputId": "19507592-70ce-4746-981d-229fe24c75ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CenterFusion'...\n",
            "remote: Enumerating objects: 178, done.\u001b[K\n",
            "remote: Counting objects: 100% (178/178), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 178 (delta 33), reused 170 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (178/178), 16.82 MiB | 12.48 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "Submodule 'src/tools/nuscenes-devkit' (https://github.com/nutonomy/nuscenes-devkit) registered for path 'src/tools/nuscenes-devkit'\n",
            "Cloning into '/content/CenterFusion/src/tools/nuscenes-devkit'...\n",
            "remote: Enumerating objects: 2143, done.        \n",
            "remote: Counting objects: 100% (327/327), done.        \n",
            "remote: Compressing objects: 100% (230/230), done.        \n",
            "remote: Total 2143 (delta 170), reused 196 (delta 97), pack-reused 1816\n",
            "Receiving objects: 100% (2143/2143), 5.19 MiB | 12.42 MiB/s, done.\n",
            "Resolving deltas: 100% (1221/1221), done.\n",
            "Submodule path 'src/tools/nuscenes-devkit': checked out '0fddd0cf7378f76574493d402ad431325d3223d4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install cython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVKyMC5CX5VF",
        "outputId": "f968eefd-5752-4d5a-9df3-8057caa40f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQHUsyMYYRam",
        "outputId": "98b6fcf8-4bdd-41f1-faea-9604a411b97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-it85hzgn\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-it85hzgn\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.28)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=264354 sha256=f8d743b829f5aa86df34f7d5b25ace519625f94f2ce318efe988b27677b193f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-371i1u2k/wheels/e2/6b/1d/344ac773c7495ea0b85eb228bc66daec7400a143a92d36b7b1\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.4\n",
            "    Uninstalling pycocotools-2.0.4:\n",
            "      Successfully uninstalled pycocotools-2.0.4\n",
            "Successfully installed pycocotools-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CenterFusion\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQxiN7YQYmQV",
        "outputId": "98a1145b-7e33-41c6-fe56-277404d75717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CenterFusion\n",
            "Collecting scikit-learn==0.21.0\n",
            "  Downloading scikit_learn-0.21.0-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.1.2.30)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.29.28)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.51.2)\n",
            "Collecting progress\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.9.9)\n",
            "Collecting nuscenes-devkit\n",
            "  Downloading nuscenes_devkit-1.1.9-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (3.13)\n",
            "Collecting motmetrics==1.1.3\n",
            "  Downloading motmetrics-1.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 47.1 MB/s \n",
            "\u001b[?25hCollecting tensorboardx\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0->-r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3->-r requirements.txt (line 12)) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3->-r requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3->-r requirements.txt (line 12)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.1->motmetrics==1.1.3->-r requirements.txt (line 12)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->-r requirements.txt (line 4)) (0.34.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.7)\n",
            "Requirement already satisfied: Pillow>6.2.1 in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit->-r requirements.txt (line 10)) (7.1.2)\n",
            "Collecting pycocotools>=2.0.1\n",
            "  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 67.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit->-r requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: descartes in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit->-r requirements.txt (line 10)) (1.8.1.post1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit->-r requirements.txt (line 10)) (4.2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit->-r requirements.txt (line 10)) (4.63.0)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx->-r requirements.txt (line 13)) (3.17.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->nuscenes-devkit->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (7.6.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (4.10.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.2.2)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.1.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.2.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (3.5.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (1.0.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (4.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (5.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (4.11.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (3.7.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (4.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (21.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nuscenes-devkit->-r requirements.txt (line 10)) (2.0.1)\n",
            "Building wheels for collected packages: progress, pycocotools, fire\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9632 sha256=ada1b5e5dc81a87272e67a8a0f0d96c33b72b3d4525d99d8dd8a259b77183847\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/d7/61/498d8e27dc11e9805b01eb3539e2ee344436fc226daeb5fe87\n",
            "  Building wheel for pycocotools (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp37-cp37m-linux_x86_64.whl size=265272 sha256=f8bb9d5204bd62dce9a6051b0da737c5419cb3992c33d2bf853fd3daaab031e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/5f/fa/f011e578cc76e1fc5be8dce30b3eb9fd00f337e744b3bba59b\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=bc34ab9616bb11cb3f499d3db839b6fcaed7998a4cca757596ecafd7e99eba0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built progress pycocotools fire\n",
            "Installing collected packages: scikit-learn, pycocotools, fire, tensorboardx, progress, nuscenes-devkit, motmetrics\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0\n",
            "    Uninstalling pycocotools-2.0:\n",
            "      Successfully uninstalled pycocotools-2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.21.0 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.0 which is incompatible.\u001b[0m\n",
            "Successfully installed fire-0.4.0 motmetrics-1.1.3 nuscenes-devkit-1.1.9 progress-1.6 pycocotools-2.0.4 scikit-learn-0.21.0 tensorboardx-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CenterFusion/src/lib/model/networks\n",
        "!git clone https://github.com/CharlesShang/DCNv2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojnxx6wPY6HF",
        "outputId": "249aa6b9-3bb4-4318-9f09-d5e61bcfe53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CenterFusion/src/lib/model/networks\n",
            "Cloning into 'DCNv2'...\n",
            "remote: Enumerating objects: 214, done.\u001b[K\n",
            "remote: Total 214 (delta 0), reused 0 (delta 0), pack-reused 214\u001b[K\n",
            "Receiving objects: 100% (214/214), 1.42 MiB | 26.86 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tCMIUrmbWQn",
        "outputId": "4c52f659-d4e0-4dae-a5d5-0a750e7fd38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 4.7 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0\n",
            "  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CenterFusion/src/lib/model/networks/DCNv2/\n",
        "!./make.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C01l6UXZJQP",
        "outputId": "e86cac6d-49e7-4b3c-9cba-22bd8cd31057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CenterFusion/src/lib/model/networks/DCNv2\n",
            "running build\n",
            "running build_ext\n",
            "building '_ext' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/content\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu\n",
            "creating build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda\n",
            "g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/CenterFusion/src/lib/model/networks/DCNv2/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/CenterFusion/src/lib/model/networks/DCNv2/src/vision.cpp -o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/CenterFusion/src/lib/model/networks/DCNv2/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp -o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:103:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:327:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         input.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:333:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         bbox.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:334:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         trans.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:343:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         out.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:344:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         top_count.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:327:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         input.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:333:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         bbox.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:334:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         trans.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:343:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         out.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:344:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         top_count.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:324:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(input.type(), \"dcn_v2_psroi_pooling_cpu_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::tuple<at::Tensor, at::Tensor> dcn_v2_psroi_pooling_cpu_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, int, float, int, int, int, int, int, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:295:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbatch\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   const int \u001b[01;35m\u001b[Kbatch\u001b[m\u001b[K = input.size(0);\n",
            "             \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:103:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:401:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         out_grad.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:402:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         top_count.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:411:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         input_grad.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:412:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         trans_grad.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:413:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         input.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:414:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         bbox.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:415:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         trans.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:401:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         out_grad.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:402:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         top_count.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:411:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         input_grad.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:412:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         trans_grad.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:413:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         input.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:414:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         bbox.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:415:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         trans.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:398:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(out_grad.type(), \"dcn_v2_psroi_pooling_cpu_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.cpp:15\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/CenterFusion/src/lib/model/networks/DCNv2/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_im2col_cpu.cpp -o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_im2col_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/CenterFusion/src/lib/model/networks/DCNv2/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp -o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor dcn_v2_cpu_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:83:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          ones.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, k_,\n",
            "                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:84:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          bias.contiguous().data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, k_, 0.0f,\n",
            "                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:85:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          output_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, n_);\n",
            "                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:87:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_im2col_cpu(input_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:88:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          offset_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:89:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          mask_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:94:65:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          columns.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:102:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          columns.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, n,\n",
            "                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:103:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          weight.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, k, 1.0f,\n",
            "                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:104:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          output_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, n);\n",
            "                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> dcn_v2_cpu_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:177:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          grad_output_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, n,\n",
            "                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:178:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          weight.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, m, 0.0f,\n",
            "                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:179:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          columns.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, n);\n",
            "                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:182:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_coord_cpu(columns.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:183:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                                input_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:184:72:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                                offset_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:185:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                                mask_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:190:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                                grad_offset_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:191:75:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                                grad_mask_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:193:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_cpu(columns.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:194:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          offset_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:195:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          mask_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:200:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          grad_input_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:203:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_im2col_cpu(input_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:204:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          offset_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:205:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          mask_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K,\n",
            "                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:210:65:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                                          columns.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:217:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          columns.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, k_,\n",
            "                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:218:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          grad_output_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, k_, 1.0f,\n",
            "                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:219:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          grad_weight.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, n_);\n",
            "                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:225:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          grad_output_n.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, k_,\n",
            "                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:226:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          ones.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, 1, 1.0f,\n",
            "                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:227:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "                          grad_bias.data<scalar_t>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, 1);\n",
            "                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.cpp:4\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/CenterFusion/src/lib/model/networks/DCNv2/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu -o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor dcn_v2_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:107:191:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     createBatchGemmBuffer<<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n",
            "                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:107:215:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     createBatchGemmBuffer<<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n",
            "                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:107:240:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     createBatchGemmBuffer<<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n",
            "                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:107:262:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     createBatchGemmBuffer<<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n",
            "                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:107:286:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     createBatchGemmBuffer<<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n",
            "                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:107:308:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     createBatchGemmBuffer<<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n",
            "                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:139:88:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:139:112:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:139:134:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:139:310:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> dcn_v2_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:273:80:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemm(state, 'n', 't', n, m, k, 1.0f,\n",
            "                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:273:107:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemm(state, 'n', 't', n, m, k, 1.0f,\n",
            "                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:273:143:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemm(state, 'n', 't', n, m, k, 1.0f,\n",
            "                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:279:96:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:279:121:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:279:147:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:279:171:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:279:349:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:279:378:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_coord_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:291:90:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:291:116:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:291:140:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:291:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_col2im_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:302:90:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:302:116:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:302:140:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:302:312:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         modulated_deformable_im2col_cuda(THCState_getCurrentStream(state),\n",
            "                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:316:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemm(state, 't', 'n', n_, m_, k_, 1.0f,\n",
            "                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:316:112:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemm(state, 't', 'n', n_, m_, k_, 1.0f,\n",
            "                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:316:153:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemm(state, 't', 'n', n_, m_, k_, 1.0f,\n",
            "                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:324:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemv(state,\n",
            "                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:324:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemv(state,\n",
            "                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.cu:324:138:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         THCudaBlas_Sgemv(state,\n",
            "                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/CenterFusion/src/lib/model/networks/DCNv2/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu -o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:98:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:456:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:497:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:628:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:658:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:898:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:1006:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:1046:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:1176:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:317:1205:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(input.type(), \"dcn_v2_psroi_pooling_cuda_forward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:357:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:402:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:539:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:585:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:626:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:666:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:707:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:1049:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:1093:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:1229:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:1274:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:1314:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:1353:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.cu:391:1393:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES(out_grad.type(), \"dcn_v2_psroi_pooling_cuda_backward\", [&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/CenterFusion/src/lib/model/networks/DCNv2/src -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c /content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.cu -o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/vision.o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_psroi_pooling_cpu.o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_im2col_cpu.o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cpu/dcn_v2_cpu.o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_cuda.o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_psroi_pooling_cuda.o build/temp.linux-x86_64-3.7/content/CenterFusion/src/lib/model/networks/DCNv2/src/cuda/dcn_v2_im2col_cuda.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.7/_ext.cpython-37m-x86_64-linux-gnu.so\n",
            "running develop\n",
            "running egg_info\n",
            "creating DCNv2.egg-info\n",
            "writing DCNv2.egg-info/PKG-INFO\n",
            "writing dependency_links to DCNv2.egg-info/dependency_links.txt\n",
            "writing top-level names to DCNv2.egg-info/top_level.txt\n",
            "writing manifest file 'DCNv2.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'DCNv2.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.7/_ext.cpython-37m-x86_64-linux-gnu.so -> \n",
            "Creating /usr/local/lib/python3.7/dist-packages/DCNv2.egg-link (link to .)\n",
            "Adding DCNv2 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content/CenterFusion/src/lib/model/networks/DCNv2\n",
            "Processing dependencies for DCNv2==0.1\n",
            "Finished processing dependencies for DCNv2==0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started"
      ],
      "metadata": {
        "id": "_R8ISQlBZPvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W89EzyAZdyGk",
        "outputId": "3f3068d5-aa6e-4781-bdf2-0848afecf80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Data preparation\n",
        "# ${CF_ROOT}\n",
        "# `-- data\n",
        "#     `-- nuscenes\n",
        "#         |-- maps\n",
        "#         |-- samples\n",
        "#         |   |-- CAM_BACK\n",
        "#         |   |   | -- xxx.jpg\n",
        "#         |   |   ` -- ...\n",
        "#         |   |-- CAM_BACK_LEFT\n",
        "#         |   |-- CAM_BACK_RIGHT\n",
        "#         |   |-- CAM_FRONT\n",
        "#         |   |-- CAM_FRONT_LEFT\n",
        "#         |   |-- CAM_FRONT_RIGHT\n",
        "#         |   |-- RADAR_BACK_LEFT\n",
        "#         |   |   | -- xxx.pcd\n",
        "#         |   |   ` -- ...\n",
        "#         |   |-- RADAR_BACK_RIGHT\n",
        "#         |   |-- RADAR_FRON\n",
        "#         |   |-- RADAR_FRONT_LEFT\n",
        "#         |   `-- RADAR_FRONT_RIGHT\n",
        "#         |-- sweeps\n",
        "#         |-- v1.0-mini\n",
        "#         |-- v1.0-test\n",
        "#         `-- v1.0-trainval"
      ],
      "metadata": {
        "id": "GhnI8zxed84f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CenterFusion/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjkHYjgieDim",
        "outputId": "3154fdb6-4ccb-4620-e087-3ff78740bc3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CenterFusion/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/gdrive/MyDrive/Colab\\ Notebooks/Project/nuscenes"
      ],
      "metadata": {
        "id": "kWjyQxP6eSFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CenterFusion/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx4HgNDbeonx",
        "outputId": "99cfbc96-7297-4a4e-f772-2cd3a92c90ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CenterFusion/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/gdrive/MyDrive/Colab\\ Notebooks/Project/models/centernet_baseline_e170.pth"
      ],
      "metadata": {
        "id": "dD0cbdBUiB2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "MnZKhEDOiLKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/CenterFusion\n",
        "# !bash experiments/train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_oLIysOiMis",
        "outputId": "96a5a626-613b-4737-f77e-269801f89200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CenterFusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "%cd src\n",
        "# train\n",
        "!python main.py \\\n",
        "    ddd \\\n",
        "    --exp_id centerfusion \\\n",
        "    --shuffle_train \\\n",
        "    --train_split mini_train \\\n",
        "    --val_split mini_val \\\n",
        "    --val_intervals 1 \\\n",
        "    --run_dataset_eval \\\n",
        "    --nuscenes_att \\\n",
        "    --velocity \\\n",
        "    --batch_size 8 \\\n",
        "    --lr 2.5e-4 \\\n",
        "    --num_epochs 40 \\\n",
        "    --lr_step 50 \\\n",
        "    --save_point 20,40,50 \\\n",
        "    --gpus 0 \\\n",
        "    --not_rand_crop \\\n",
        "    --flip 0.5 \\\n",
        "    --shift 0.1 \\\n",
        "    --pointcloud \\\n",
        "    --radar_sweeps 3 \\\n",
        "    --pc_z_offset 0.0 \\\n",
        "    --pillar_dims 1.0,0.2,0.2 \\\n",
        "    --max_pc_dist 60.0 \\\n",
        "    --load_model ../models/centernet_baseline_e170.pth \\\n",
        "    --freeze_backbone \\\n",
        "    --resume \\\n",
        "\n",
        "%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQO2ueZD8-_v",
        "outputId": "5352d942-b82e-4acc-8388-b85567bac35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CenterFusion/src\n",
            "Using tensorboardX\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n",
            "Fix size testing.\n",
            "training chunk_sizes: [8]\n",
            "input h w: 448 800\n",
            "heads {'hm': 10, 'reg': 2, 'wh': 2, 'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2, 'dep_sec': 1, 'rot_sec': 8, 'nuscenes_att': 8, 'velocity': 3}\n",
            "weights {'hm': 1, 'reg': 1, 'wh': 0.1, 'dep': 1, 'rot': 1, 'dim': 1, 'amodel_offset': 1, 'dep_sec': 1, 'rot_sec': 1, 'nuscenes_att': 1, 'velocity': 1}\n",
            "head conv {'hm': [256], 'reg': [256], 'wh': [256], 'dep': [256], 'rot': [256], 'dim': [256], 'amodel_offset': [256], 'dep_sec': [256, 256, 256], 'rot_sec': [256, 256, 256], 'nuscenes_att': [256, 256, 256], 'velocity': [256, 256, 256]}\n",
            "Namespace(K=100, amodel_offset_weight=1, arch='dla_34', aug_rot=0, backbone='dla34', batch_size=8, chunk_sizes=[8], custom_dataset_ann_path='', custom_dataset_img_path='', custom_head_convs={'dep_sec': 3, 'rot_sec': 3, 'velocity': 3, 'nuscenes_att': 3}, data_dir='/content/CenterFusion/src/lib/../../data', dataset='nuscenes', dataset_version='', debug=0, debug_dir='/content/CenterFusion/src/lib/../../exp/ddd/centerfusion/debug', debugger_theme='white', demo='', dense_reg=1, dep_res_weight=1, dep_weight=1, depth_scale=1, dim_weight=1, disable_frustum=False, dla_node='dcn', down_ratio=4, eval=False, eval_n_plots=0, eval_render_curves=False, exp_dir='/content/CenterFusion/src/lib/../../exp/ddd', exp_id='centerfusion', fix_res=True, fix_short=-1, flip=0.5, flip_test=False, fp_disturb=0, freeze_backbone=True, frustumExpansionRatio=0.0, gpus=[0], gpus_str='0', head_conv={'hm': [256], 'reg': [256], 'wh': [256], 'dep': [256], 'rot': [256], 'dim': [256], 'amodel_offset': [256], 'dep_sec': [256, 256, 256], 'rot_sec': [256, 256, 256], 'nuscenes_att': [256, 256, 256], 'velocity': [256, 256, 256]}, head_kernel=3, heads={'hm': 10, 'reg': 2, 'wh': 2, 'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2, 'dep_sec': 1, 'rot_sec': 8, 'nuscenes_att': 8, 'velocity': 3}, hm_dist_thresh={0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0}, hm_disturb=0, hm_hp_weight=1, hm_to_box_ratio=0.3, hm_transparency=0.7, hm_weight=1, hp_weight=1, hungarian=False, ignore_loaded_cats=[], img_format='jpg', input_h=448, input_res=800, input_w=800, iou_thresh=0, keep_res=False, kitti_split='3dop', layers_to_freeze=['base', 'dla_up', 'ida_up'], load_model='../models/centernet_baseline_e170.pth', load_results='', lost_disturb=0, lr=0.00025, lr_step=[50], ltrb=False, ltrb_amodal=False, ltrb_amodal_weight=0.1, ltrb_weight=0.1, master_batch_size=8, max_age=-1, max_frame_dist=3, max_pc=1000, max_pc_dist=60.0, model_output_list=False, msra_outchannel=256, neck='dlaup', new_thresh=0.3, nms=False, no_color_aug=False, no_pause=False, no_pre_img=False, non_block_test=False, normalize_depth=True, not_cuda_benchmark=False, not_max_crop=False, not_prefetch_test=False, not_rand_crop=True, not_set_cuda_env=False, not_show_bbox=False, not_show_number=False, num_classes=10, num_epochs=40, num_head_conv=1, num_img_channels=3, num_iters=-1, num_resnet_layers=101, num_stacks=1, num_workers=4, nuscenes_att=True, nuscenes_att_weight=1, off_weight=1, optim='adam', out_thresh=-1, output_h=112, output_res=200, output_w=200, pad=31, pc_atts=['x', 'y', 'z', 'dyn_prop', 'id', 'rcs', 'vx', 'vy', 'vx_comp', 'vy_comp', 'is_quality_valid', 'ambig_state', 'x_rms', 'y_rms', 'invalid_state', 'pdh0', 'vx_rms', 'vy_rms'], pc_feat_channels={'pc_dep': 0, 'pc_vx': 1, 'pc_vz': 2}, pc_feat_lvl=['pc_dep', 'pc_vx', 'pc_vz'], pc_roi_method='pillars', pc_z_offset=0.0, pillar_dims=[1.5, 0.2, 0.2], pointcloud=True, pre_hm=False, pre_img=False, pre_thresh=-1, print_iter=0, prior_bias=-4.6, public_det=False, qualitative=False, r_a=250, r_b=5, radar_sweeps=3, reg_loss='l1', reset_hm=False, resize_video=False, resume=True, reuse_hm=False, root_dir='/content/CenterFusion/src/lib/../..', rot_weight=1, rotate=0, run_dataset_eval=True, same_aug_pre=False, save_all=False, save_dir='/content/CenterFusion/src/lib/../../exp/ddd/centerfusion', save_framerate=30, save_img_suffix='', save_imgs=[], save_point=[20, 40, 50], save_results=False, save_video=False, scale=0, secondary_heads=['velocity', 'nuscenes_att', 'dep_sec', 'rot_sec'], seed=317, shift=0.1, show_track_color=False, show_velocity=False, shuffle_train=True, sigmoid_dep_sec=True, skip_first=-1, sort_det_by_dist=False, tango_color=False, task='ddd', test_dataset='nuscenes', test_focal_length=-1, test_scales=[1.0], track_thresh=0.3, tracking=False, tracking_weight=1, train_split='mini_train', trainval=False, transpose_video=False, use_loaded_results=False, val_intervals=1, val_split='mini_val', velocity=True, velocity_weight=1, video_h=512, video_w=512, vis_gt_bev='', vis_thresh=0.3, warm_start_weights=False, weights={'hm': 1, 'reg': 1, 'wh': 0.1, 'dep': 1, 'rot': 1, 'dim': 1, 'amodel_offset': 1, 'dep_sec': 1, 'rot_sec': 1, 'nuscenes_att': 1, 'velocity': 1}, wh_weight=0.1, zero_pre_hm=False, zero_tracking=False)\n",
            "Creating model...\n",
            "Using node type: (<class 'model.networks.dla.DeformConv'>, <class 'model.networks.dla.DeformConv'>)\n",
            "Warning: No ImageNet pretrain!!\n",
            "loaded ../models/centernet_baseline_e170.pth, epoch 28\n",
            "Skip loading parameter nuscenes_att.0.weight, required shapetorch.Size([256, 67, 3, 3]), loaded shapetorch.Size([256, 64, 3, 3]).\n",
            "Skip loading parameter nuscenes_att.2.weight, required shapetorch.Size([256, 256, 1, 1]), loaded shapetorch.Size([8, 256, 1, 1]).\n",
            "Skip loading parameter nuscenes_att.2.bias, required shapetorch.Size([256]), loaded shapetorch.Size([8]).\n",
            "Skip loading parameter velocity.0.weight, required shapetorch.Size([256, 67, 3, 3]), loaded shapetorch.Size([256, 64, 3, 3]).\n",
            "Skip loading parameter velocity.2.weight, required shapetorch.Size([256, 256, 1, 1]), loaded shapetorch.Size([3, 256, 1, 1]).\n",
            "Skip loading parameter velocity.2.bias, required shapetorch.Size([256]), loaded shapetorch.Size([3]).\n",
            "No param dep_sec.0.weight.\n",
            "No param dep_sec.0.bias.\n",
            "No param dep_sec.2.weight.\n",
            "No param dep_sec.2.bias.\n",
            "No param dep_sec.4.weight.\n",
            "No param dep_sec.4.bias.\n",
            "No param dep_sec.6.weight.\n",
            "No param dep_sec.6.bias.\n",
            "No param rot_sec.0.weight.\n",
            "No param rot_sec.0.bias.\n",
            "No param rot_sec.2.weight.\n",
            "No param rot_sec.2.bias.\n",
            "No param rot_sec.4.weight.\n",
            "No param rot_sec.4.bias.\n",
            "No param rot_sec.6.weight.\n",
            "No param rot_sec.6.bias.\n",
            "No param nuscenes_att.4.weight.\n",
            "No param nuscenes_att.4.bias.\n",
            "No param nuscenes_att.6.weight.\n",
            "No param nuscenes_att.6.bias.\n",
            "No param velocity.4.weight.\n",
            "No param velocity.4.bias.\n",
            "No param velocity.6.weight.\n",
            "No param velocity.6.bias.\n",
            "Resumed optimizer with start lr 0.00025\n",
            "Setting up validation data...\n",
            "Dataset version \n",
            "==> initializing mini_val data from /content/CenterFusion/src/lib/../../data/nuscenes/annotations_3sweeps/mini_val.json, \n",
            " images from /content/CenterFusion/src/lib/../../data/nuscenes ...\n",
            "loading annotations into memory...\n",
            "Done (t=1.43s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded mini_val 486 samples\n",
            "Setting up train data...\n",
            "Dataset version \n",
            "==> initializing mini_train data from /content/CenterFusion/src/lib/../../data/nuscenes/annotations_3sweeps/mini_train.json, \n",
            " images from /content/CenterFusion/src/lib/../../data/nuscenes ...\n",
            "loading annotations into memory...\n",
            "Done (t=2.89s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded mini_train 1212 samples\n",
            "Starting training...\n",
            "ddd/centerfusion |################################| train: [29][150/151]|Tot: 0:02:25 |ETA: 0:00:01 |tot 18.8635 |hm 1.0670 |wh 1.5009 |reg 0.2136 |dep 2.2125 |dep_sec 9.5463 |dim 0.2265 |rot 1.6394 |rot_sec 2.0816 |amodel_offset 0.7596 |nuscenes_att 0.4495 |velocity 0.5173 |Data 0.184s(0.620s) |Net 0.965s  \n",
            "ddd/centerfusion |################################| val: [29][485/486]|Tot: 0:03:16 |ETA: 0:00:01 |tot 10.0766 |hm 1.1186 |wh 1.8554 |reg 0.2270 |dep 1.4749 |dep_sec 2.0203 |dim 0.2058 |rot 1.6032 |rot_sec 1.8200 |amodel_offset 0.8911 |nuscenes_att 0.2987 |velocity 0.2316 |Data 0.122s(0.154s) |Net 0.404s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 4.625 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 352.05it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 33269\n",
            "=> After LIDAR and RADAR points based filtering: 33269\n",
            "=> After bike rack filtering: 33212\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.1991\n",
            "mATE: 0.9292\n",
            "mASE: 0.4933\n",
            "mAOE: 0.9507\n",
            "mAVE: 1.0387\n",
            "mAAE: 0.4693\n",
            "NDS: 0.2153\n",
            "Eval time: 7.3s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.305\t0.779\t0.175\t0.891\t0.267\t0.093\n",
            "truck\t0.196\t1.152\t0.198\t0.378\t0.227\t0.170\n",
            "bus\t0.311\t0.902\t0.137\t0.210\t2.052\t0.013\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.400\t0.741\t0.309\t1.048\t0.896\t0.186\n",
            "motorcycle\t0.240\t1.017\t0.367\t1.659\t0.065\t0.960\n",
            "bicycle\t0.043\t0.943\t0.378\t1.371\t2.802\t0.333\n",
            "traffic_cone\t0.494\t0.759\t0.370\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [30][150/151]|Tot: 0:01:22 |ETA: 0:00:01 |tot 11.8903 |hm 1.0182 |wh 1.5016 |reg 0.2138 |dep 2.2466 |dep_sec 3.0459 |dim 0.2182 |rot 1.6319 |rot_sec 1.8282 |amodel_offset 0.7988 |nuscenes_att 0.2851 |velocity 0.4533 |Data 0.184s(0.189s) |Net 0.543s\n",
            "ddd/centerfusion |################################| val: [30][485/486]|Tot: 0:02:25 |ETA: 0:00:01 |tot 10.5749 |hm 1.1142 |wh 1.8126 |reg 0.2273 |dep 1.5601 |dep_sec 2.5979 |dim 0.2130 |rot 1.6048 |rot_sec 1.7049 |amodel_offset 0.8861 |nuscenes_att 0.2545 |velocity 0.2308 |Data 0.017s(0.014s) |Net 0.299s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.563 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 342.80it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 29759\n",
            "=> After LIDAR and RADAR points based filtering: 29759\n",
            "=> After bike rack filtering: 29737\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.1246\n",
            "mATE: 1.0452\n",
            "mASE: 0.4810\n",
            "mAOE: 0.9245\n",
            "mAVE: 1.1014\n",
            "mAAE: 0.3441\n",
            "NDS: 0.1873\n",
            "Eval time: 7.2s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.236\t0.967\t0.181\t0.877\t0.264\t0.074\n",
            "truck\t0.116\t1.051\t0.181\t0.590\t0.337\t0.231\n",
            "bus\t0.049\t1.161\t0.146\t0.625\t2.181\t0.050\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.300\t0.813\t0.296\t0.745\t0.822\t0.194\n",
            "motorcycle\t0.147\t1.010\t0.343\t1.375\t0.061\t0.003\n",
            "bicycle\t0.030\t1.130\t0.367\t1.108\t3.147\t0.200\n",
            "traffic_cone\t0.368\t1.320\t0.297\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [31][150/151]|Tot: 0:01:21 |ETA: 0:00:01 |tot 11.7482 |hm 1.0018 |wh 1.4789 |reg 0.2149 |dep 2.2128 |dep_sec 3.1648 |dim 0.2136 |rot 1.6239 |rot_sec 1.7272 |amodel_offset 0.7344 |nuscenes_att 0.2594 |velocity 0.4477 |Data 0.184s(0.193s) |Net 0.541s\n",
            "ddd/centerfusion |################################| val: [31][485/486]|Tot: 0:02:26 |ETA: 0:00:01 |tot 9.7698 |hm 1.1116 |wh 1.8009 |reg 0.2275 |dep 1.5717 |dep_sec 1.7870 |dim 0.2188 |rot 1.6077 |rot_sec 1.6763 |amodel_offset 0.8839 |nuscenes_att 0.2733 |velocity 0.2320 |Data 0.012s(0.014s) |Net 0.301s \n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.556 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 347.85it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 31273\n",
            "=> After LIDAR and RADAR points based filtering: 31273\n",
            "=> After bike rack filtering: 31243\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.2209\n",
            "mATE: 0.8640\n",
            "mASE: 0.5108\n",
            "mAOE: 0.8128\n",
            "mAVE: 1.0817\n",
            "mAAE: 0.3602\n",
            "NDS: 0.2557\n",
            "Eval time: 6.6s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.429\t0.591\t0.184\t0.814\t0.255\t0.104\n",
            "truck\t0.300\t0.714\t0.181\t0.341\t0.252\t0.279\n",
            "bus\t0.308\t0.789\t0.138\t0.186\t2.261\t0.031\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.417\t0.676\t0.333\t0.816\t0.802\t0.176\n",
            "motorcycle\t0.195\t0.973\t0.404\t1.297\t0.061\t0.000\n",
            "bicycle\t0.074\t1.012\t0.393\t0.861\t3.023\t0.290\n",
            "traffic_cone\t0.488\t0.886\t0.474\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [32][150/151]|Tot: 0:01:21 |ETA: 0:00:01 |tot 11.1607 |hm 1.0045 |wh 1.4457 |reg 0.2158 |dep 2.1499 |dep_sec 2.7030 |dim 0.2120 |rot 1.6284 |rot_sec 1.7010 |amodel_offset 0.7322 |nuscenes_att 0.2365 |velocity 0.4329 |Data 0.183s(0.187s) |Net 0.539s\n",
            "ddd/centerfusion |################################| val: [32][485/486]|Tot: 0:02:26 |ETA: 0:00:01 |tot 9.9107 |hm 1.1142 |wh 1.8230 |reg 0.2266 |dep 1.4687 |dep_sec 2.0141 |dim 0.2218 |rot 1.6057 |rot_sec 1.6649 |amodel_offset 0.8906 |nuscenes_att 0.2868 |velocity 0.2351 |Data 0.012s(0.014s) |Net 0.302s \n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.563 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 332.69it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 30611\n",
            "=> After LIDAR and RADAR points based filtering: 30611\n",
            "=> After bike rack filtering: 30558\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.2037\n",
            "mATE: 0.8765\n",
            "mASE: 0.5148\n",
            "mAOE: 0.9159\n",
            "mAVE: 0.8483\n",
            "mAAE: 0.3593\n",
            "NDS: 0.2504\n",
            "Eval time: 6.7s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.352\t0.741\t0.184\t0.845\t0.211\t0.106\n",
            "truck\t0.261\t0.797\t0.193\t1.049\t0.188\t0.444\n",
            "bus\t0.216\t1.189\t0.183\t0.254\t1.977\t0.053\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.348\t0.755\t0.348\t0.717\t0.734\t0.202\n",
            "motorcycle\t0.202\t1.048\t0.383\t1.393\t0.069\t0.002\n",
            "bicycle\t0.057\t0.829\t0.377\t0.985\t1.608\t0.067\n",
            "traffic_cone\t0.601\t0.406\t0.479\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [33][150/151]|Tot: 0:01:21 |ETA: 0:00:01 |tot 11.1487 |hm 0.9896 |wh 1.4315 |reg 0.2146 |dep 2.2478 |dep_sec 2.6649 |dim 0.2099 |rot 1.6252 |rot_sec 1.6917 |amodel_offset 0.6867 |nuscenes_att 0.2331 |velocity 0.4420 |Data 0.180s(0.192s) |Net 0.541s\n",
            "ddd/centerfusion |################################| val: [33][485/486]|Tot: 0:02:27 |ETA: 0:00:01 |tot 9.6736 |hm 1.1388 |wh 1.8049 |reg 0.2262 |dep 1.5004 |dep_sec 1.6934 |dim 0.2217 |rot 1.6129 |rot_sec 1.6623 |amodel_offset 0.9044 |nuscenes_att 0.2963 |velocity 0.2366 |Data 0.018s(0.014s) |Net 0.304s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.555 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 356.11it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 31097\n",
            "=> After LIDAR and RADAR points based filtering: 31097\n",
            "=> After bike rack filtering: 31044\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.2478\n",
            "mATE: 0.8127\n",
            "mASE: 0.5163\n",
            "mAOE: 0.7981\n",
            "mAVE: 0.8178\n",
            "mAAE: 0.3994\n",
            "NDS: 0.2895\n",
            "Eval time: 6.2s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.439\t0.599\t0.180\t0.791\t0.237\t0.138\n",
            "truck\t0.302\t0.865\t0.183\t0.275\t0.237\t0.567\n",
            "bus\t0.405\t0.827\t0.118\t0.104\t1.148\t0.083\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.408\t0.691\t0.356\t0.764\t0.744\t0.191\n",
            "motorcycle\t0.218\t0.978\t0.410\t1.367\t0.063\t0.001\n",
            "bicycle\t0.090\t0.736\t0.428\t0.883\t2.112\t0.215\n",
            "traffic_cone\t0.615\t0.431\t0.488\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [34][150/151]|Tot: 0:01:22 |ETA: 0:00:01 |tot 10.9963 |hm 0.9775 |wh 1.4628 |reg 0.2126 |dep 2.2073 |dep_sec 2.5748 |dim 0.2056 |rot 1.6211 |rot_sec 1.6682 |amodel_offset 0.7461 |nuscenes_att 0.2260 |velocity 0.4107 |Data 0.185s(0.195s) |Net 0.544s\n",
            "ddd/centerfusion |################################| val: [34][485/486]|Tot: 0:02:28 |ETA: 0:00:01 |tot 9.5132 |hm 1.1213 |wh 1.7960 |reg 0.2260 |dep 1.4372 |dep_sec 1.6942 |dim 0.2180 |rot 1.6119 |rot_sec 1.6503 |amodel_offset 0.8857 |nuscenes_att 0.2528 |velocity 0.2361 |Data 0.013s(0.014s) |Net 0.305s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.541 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 348.70it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 32328\n",
            "=> After LIDAR and RADAR points based filtering: 32328\n",
            "=> After bike rack filtering: 32277\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.2426\n",
            "mATE: 0.8439\n",
            "mASE: 0.5236\n",
            "mAOE: 0.7499\n",
            "mAVE: 0.8442\n",
            "mAAE: 0.3525\n",
            "NDS: 0.2899\n",
            "Eval time: 6.7s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.384\t0.777\t0.179\t0.292\t0.188\t0.084\n",
            "truck\t0.222\t1.256\t0.207\t0.404\t0.312\t0.312\n",
            "bus\t0.351\t0.834\t0.154\t0.102\t1.787\t0.024\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.431\t0.687\t0.339\t0.709\t0.703\t0.198\n",
            "motorcycle\t0.297\t0.878\t0.433\t1.447\t0.081\t0.004\n",
            "bicycle\t0.092\t0.735\t0.426\t0.795\t1.682\t0.198\n",
            "traffic_cone\t0.649\t0.274\t0.498\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [35][150/151]|Tot: 0:01:21 |ETA: 0:00:01 |tot 11.2552 |hm 0.9748 |wh 1.4490 |reg 0.2122 |dep 2.2379 |dep_sec 2.8171 |dim 0.2066 |rot 1.6185 |rot_sec 1.6639 |amodel_offset 0.7550 |nuscenes_att 0.2140 |velocity 0.4104 |Data 0.184s(0.186s) |Net 0.539s\n",
            "ddd/centerfusion |################################| val: [35][485/486]|Tot: 0:02:26 |ETA: 0:00:01 |tot 10.3547 |hm 1.1362 |wh 1.8631 |reg 0.2264 |dep 1.4559 |dep_sec 2.4430 |dim 0.2149 |rot 1.6115 |rot_sec 1.6409 |amodel_offset 0.8885 |nuscenes_att 0.3126 |velocity 0.2387 |Data 0.012s(0.014s) |Net 0.301s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.566 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 344.64it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 29199\n",
            "=> After LIDAR and RADAR points based filtering: 29199\n",
            "=> After bike rack filtering: 29176\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.1897\n",
            "mATE: 0.9130\n",
            "mASE: 0.4989\n",
            "mAOE: 0.7594\n",
            "mAVE: 0.9234\n",
            "mAAE: 0.3850\n",
            "NDS: 0.2469\n",
            "Eval time: 7.3s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.239\t0.938\t0.179\t0.222\t0.227\t0.154\n",
            "truck\t0.221\t0.869\t0.180\t0.470\t0.255\t0.523\n",
            "bus\t0.348\t0.807\t0.150\t0.243\t1.902\t0.082\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.341\t0.740\t0.323\t0.693\t0.698\t0.167\n",
            "motorcycle\t0.159\t1.003\t0.341\t1.465\t0.070\t0.003\n",
            "bicycle\t0.054\t1.112\t0.397\t0.741\t2.235\t0.151\n",
            "traffic_cone\t0.536\t0.661\t0.419\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [36][150/151]|Tot: 0:01:20 |ETA: 0:00:01 |tot 11.2012 |hm 0.9709 |wh 1.4665 |reg 0.2140 |dep 2.1170 |dep_sec 2.9372 |dim 0.2019 |rot 1.6110 |rot_sec 1.6468 |amodel_offset 0.7414 |nuscenes_att 0.2059 |velocity 0.4083 |Data 0.184s(0.187s) |Net 0.536s\n",
            "ddd/centerfusion |################################| val: [36][485/486]|Tot: 0:02:27 |ETA: 0:00:01 |tot 10.3769 |hm 1.1443 |wh 1.8300 |reg 0.2276 |dep 1.4350 |dep_sec 2.5430 |dim 0.2113 |rot 1.6133 |rot_sec 1.6433 |amodel_offset 0.8840 |nuscenes_att 0.2525 |velocity 0.2396 |Data 0.012s(0.014s) |Net 0.303s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.543 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 342.47it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 28797\n",
            "=> After LIDAR and RADAR points based filtering: 28797\n",
            "=> After bike rack filtering: 28774\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.1718\n",
            "mATE: 0.9611\n",
            "mASE: 0.4933\n",
            "mAOE: 0.7864\n",
            "mAVE: 0.8086\n",
            "mAAE: 0.3675\n",
            "NDS: 0.2442\n",
            "Eval time: 7.4s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.221\t1.059\t0.170\t0.254\t0.229\t0.072\n",
            "truck\t0.192\t0.978\t0.173\t0.257\t0.315\t0.398\n",
            "bus\t0.207\t1.180\t0.139\t0.240\t1.552\t0.110\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.303\t0.800\t0.324\t0.709\t0.667\t0.186\n",
            "motorcycle\t0.191\t1.028\t0.358\t1.471\t0.068\t0.000\n",
            "bicycle\t0.048\t0.975\t0.382\t1.146\t1.638\t0.174\n",
            "traffic_cone\t0.556\t0.591\t0.386\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [37][150/151]|Tot: 0:01:21 |ETA: 0:00:01 |tot 11.6331 |hm 0.9547 |wh 1.4138 |reg 0.2157 |dep 2.2622 |dep_sec 3.2975 |dim 0.2018 |rot 1.6157 |rot_sec 1.6467 |amodel_offset 0.7003 |nuscenes_att 0.2103 |velocity 0.3868 |Data 0.184s(0.194s) |Net 0.542s\n",
            "ddd/centerfusion |################################| val: [37][485/486]|Tot: 0:02:26 |ETA: 0:00:01 |tot 10.7053 |hm 1.1375 |wh 1.8336 |reg 0.2273 |dep 1.6205 |dep_sec 2.6374 |dim 0.2179 |rot 1.6162 |rot_sec 1.6494 |amodel_offset 0.8924 |nuscenes_att 0.2792 |velocity 0.2442 |Data 0.014s(0.014s) |Net 0.301s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.557 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 349.30it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 28588\n",
            "=> After LIDAR and RADAR points based filtering: 28588\n",
            "=> After bike rack filtering: 28566\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.1772\n",
            "mATE: 0.9261\n",
            "mASE: 0.5025\n",
            "mAOE: 0.7286\n",
            "mAVE: 0.8658\n",
            "mAAE: 0.3693\n",
            "NDS: 0.2494\n",
            "Eval time: 7.1s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.247\t0.936\t0.181\t0.253\t0.241\t0.116\n",
            "truck\t0.203\t0.838\t0.189\t0.162\t0.248\t0.477\n",
            "bus\t0.374\t0.905\t0.115\t0.114\t1.690\t0.109\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.258\t0.888\t0.325\t0.775\t0.690\t0.168\n",
            "motorcycle\t0.148\t1.021\t0.331\t1.362\t0.069\t0.008\n",
            "bicycle\t0.057\t0.998\t0.414\t0.892\t1.989\t0.075\n",
            "traffic_cone\t0.486\t0.674\t0.470\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [38][150/151]|Tot: 0:01:21 |ETA: 0:00:01 |tot 10.8350 |hm 0.9668 |wh 1.4523 |reg 0.2138 |dep 2.1024 |dep_sec 2.5571 |dim 0.2019 |rot 1.6249 |rot_sec 1.6472 |amodel_offset 0.7700 |nuscenes_att 0.1986 |velocity 0.4070 |Data 0.184s(0.192s) |Net 0.540s\n",
            "ddd/centerfusion |################################| val: [38][485/486]|Tot: 0:02:26 |ETA: 0:00:01 |tot 10.5709 |hm 1.1416 |wh 1.8113 |reg 0.2263 |dep 1.5791 |dep_sec 2.5970 |dim 0.2157 |rot 1.6141 |rot_sec 1.6295 |amodel_offset 0.8856 |nuscenes_att 0.2515 |velocity 0.2493 |Data 0.013s(0.014s) |Net 0.302s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.548 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 329.05it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 28479\n",
            "=> After LIDAR and RADAR points based filtering: 28479\n",
            "=> After bike rack filtering: 28466\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.1747\n",
            "mATE: 0.9053\n",
            "mASE: 0.5066\n",
            "mAOE: 0.7255\n",
            "mAVE: 0.9874\n",
            "mAAE: 0.3586\n",
            "NDS: 0.2390\n",
            "Eval time: 7.0s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.222\t0.821\t0.175\t0.206\t0.219\t0.077\n",
            "truck\t0.215\t0.853\t0.187\t0.173\t0.238\t0.391\n",
            "bus\t0.250\t0.978\t0.111\t0.257\t1.791\t0.037\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.324\t0.739\t0.338\t0.687\t0.680\t0.161\n",
            "motorcycle\t0.136\t1.007\t0.337\t1.259\t0.072\t0.001\n",
            "bicycle\t0.043\t1.097\t0.413\t0.947\t2.901\t0.202\n",
            "traffic_cone\t0.557\t0.558\t0.505\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [39][150/151]|Tot: 0:01:22 |ETA: 0:00:01 |tot 11.0177 |hm 0.9533 |wh 1.4359 |reg 0.2147 |dep 2.1099 |dep_sec 2.8165 |dim 0.1998 |rot 1.6133 |rot_sec 1.6353 |amodel_offset 0.7498 |nuscenes_att 0.1951 |velocity 0.3864 |Data 0.184s(0.195s) |Net 0.543s\n",
            "ddd/centerfusion |################################| val: [39][485/486]|Tot: 0:02:26 |ETA: 0:00:01 |tot 9.5118 |hm 1.1569 |wh 1.8236 |reg 0.2269 |dep 1.4262 |dep_sec 1.6602 |dim 0.2181 |rot 1.6196 |rot_sec 1.6357 |amodel_offset 0.8859 |nuscenes_att 0.2660 |velocity 0.2339 |Data 0.012s(0.014s) |Net 0.302s\n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.570 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 322.05it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 30856\n",
            "=> After LIDAR and RADAR points based filtering: 30856\n",
            "=> After bike rack filtering: 30808\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.2379\n",
            "mATE: 0.8129\n",
            "mASE: 0.5009\n",
            "mAOE: 0.7450\n",
            "mAVE: 0.9368\n",
            "mAAE: 0.3589\n",
            "NDS: 0.2835\n",
            "Eval time: 6.7s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.432\t0.590\t0.184\t0.188\t0.178\t0.105\n",
            "truck\t0.339\t0.642\t0.175\t0.128\t0.255\t0.468\n",
            "bus\t0.391\t0.770\t0.089\t0.142\t1.590\t0.011\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.388\t0.753\t0.340\t0.764\t0.724\t0.144\n",
            "motorcycle\t0.248\t0.984\t0.398\t1.502\t0.067\t0.018\n",
            "bicycle\t0.066\t0.786\t0.418\t0.982\t2.681\t0.126\n",
            "traffic_cone\t0.515\t0.604\t0.406\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "ddd/centerfusion |################################| train: [40][150/151]|Tot: 0:01:22 |ETA: 0:00:01 |tot 10.8267 |hm 0.9450 |wh 1.3960 |reg 0.2157 |dep 2.1630 |dep_sec 2.6332 |dim 0.2061 |rot 1.6084 |rot_sec 1.6256 |amodel_offset 0.6911 |nuscenes_att 0.1959 |velocity 0.4032 |Data 0.182s(0.194s) |Net 0.546s\n",
            "ddd/centerfusion |################################| val: [40][485/486]|Tot: 0:02:28 |ETA: 0:00:01 |tot 9.6613 |hm 1.1490 |wh 1.8097 |reg 0.2280 |dep 1.4364 |dep_sec 1.8099 |dim 0.2154 |rot 1.6175 |rot_sec 1.6387 |amodel_offset 0.8865 |nuscenes_att 0.2619 |velocity 0.2370 |Data 0.012s(0.014s) |Net 0.305s \n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.560 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 344.12it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 30157\n",
            "=> After LIDAR and RADAR points based filtering: 30157\n",
            "=> After bike rack filtering: 30110\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.2400\n",
            "mATE: 0.8099\n",
            "mASE: 0.4963\n",
            "mAOE: 0.7586\n",
            "mAVE: 1.0076\n",
            "mAAE: 0.3634\n",
            "NDS: 0.2772\n",
            "Eval time: 6.8s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.399\t0.631\t0.178\t0.179\t0.195\t0.087\n",
            "truck\t0.319\t0.645\t0.183\t0.104\t0.220\t0.448\n",
            "bus\t0.446\t0.760\t0.123\t0.248\t2.416\t0.152\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.354\t0.752\t0.340\t0.716\t0.686\t0.160\n",
            "motorcycle\t0.237\t0.957\t0.352\t1.550\t0.075\t0.002\n",
            "bicycle\t0.063\t0.896\t0.424\t1.031\t2.470\t0.057\n",
            "traffic_cone\t0.581\t0.458\t0.362\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "\u001b[?25h/content/CenterFusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/CenterFusion/exp/exp/ddd/centerfusion/model_40.pth models/"
      ],
      "metadata": {
        "id": "h43FsfOtd1bH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735cbf6c-12dd-457c-d0ca-073f74920c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/CenterFusion/exp/exp/ddd/centerfusion/model_40.pth': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "!export CUDA_VISIBLE_DEVICES=1\n",
        "%cd src\n",
        "\n",
        "## Perform detection and evaluation\n",
        "!python test.py ddd \\\n",
        "    --exp_id centerfusion \\\n",
        "    --dataset nuscenes \\\n",
        "    --val_split mini_val \\\n",
        "    --run_dataset_eval \\\n",
        "    --num_workers 1 \\\n",
        "    --nuscenes_att \\\n",
        "    --velocity \\\n",
        "    --gpus 0 \\\n",
        "    --pointcloud \\\n",
        "    --radar_sweeps 3 \\\n",
        "    --max_pc_dist 40.0 \\\n",
        "    --pc_z_offset -0.0 \\\n",
        "    --load_model ../models/model_40.pth \\\n",
        "    --flip_test \\\n",
        "    --resume \\\n",
        "    --debug 4\\"
      ],
      "metadata": {
        "id": "G56L9f1bjbeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e5d9dc-18b3-4f59-b743-91cb0d906e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'src'\n",
            "/content/CenterFusion/src\n",
            "Using tensorboardX\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n",
            "Fix size testing.\n",
            "training chunk_sizes: [32]\n",
            "input h w: 448 800\n",
            "heads {'hm': 10, 'reg': 2, 'wh': 2, 'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2, 'dep_sec': 1, 'rot_sec': 8, 'nuscenes_att': 8, 'velocity': 3}\n",
            "weights {'hm': 1, 'reg': 1, 'wh': 0.1, 'dep': 1, 'rot': 1, 'dim': 1, 'amodel_offset': 1, 'dep_sec': 1, 'rot_sec': 1, 'nuscenes_att': 1, 'velocity': 1}\n",
            "head conv {'hm': [256], 'reg': [256], 'wh': [256], 'dep': [256], 'rot': [256], 'dim': [256], 'amodel_offset': [256], 'dep_sec': [256, 256, 256], 'rot_sec': [256, 256, 256], 'nuscenes_att': [256, 256, 256], 'velocity': [256, 256, 256]}\n",
            "Namespace(K=100, amodel_offset_weight=1, arch='dla_34', aug_rot=0, backbone='dla34', batch_size=1, chunk_sizes=[32], custom_dataset_ann_path='', custom_dataset_img_path='', custom_head_convs={'dep_sec': 3, 'rot_sec': 3, 'velocity': 3, 'nuscenes_att': 3}, data_dir='/content/CenterFusion/src/lib/../../data', dataset='nuscenes', dataset_version='', debug=4, debug_dir='/content/CenterFusion/src/lib/../../exp/ddd/centerfusion/debug', debugger_theme='white', demo='', dense_reg=1, dep_res_weight=1, dep_weight=1, depth_scale=1, dim_weight=1, disable_frustum=False, dla_node='dcn', down_ratio=4, eval=False, eval_n_plots=0, eval_render_curves=False, exp_dir='/content/CenterFusion/src/lib/../../exp/ddd', exp_id='centerfusion', fix_res=True, fix_short=-1, flip=0.5, flip_test=True, fp_disturb=0, freeze_backbone=False, frustumExpansionRatio=0.0, gpus=[0], gpus_str='0', head_conv={'hm': [256], 'reg': [256], 'wh': [256], 'dep': [256], 'rot': [256], 'dim': [256], 'amodel_offset': [256], 'dep_sec': [256, 256, 256], 'rot_sec': [256, 256, 256], 'nuscenes_att': [256, 256, 256], 'velocity': [256, 256, 256]}, head_kernel=3, heads={'hm': 10, 'reg': 2, 'wh': 2, 'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2, 'dep_sec': 1, 'rot_sec': 8, 'nuscenes_att': 8, 'velocity': 3}, hm_dist_thresh={0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0}, hm_disturb=0, hm_hp_weight=1, hm_to_box_ratio=0.3, hm_transparency=0.7, hm_weight=1, hp_weight=1, hungarian=False, ignore_loaded_cats=[], img_format='jpg', input_h=448, input_res=800, input_w=800, iou_thresh=0, keep_res=False, kitti_split='3dop', layers_to_freeze=['base', 'dla_up', 'ida_up'], load_model='../models/model_40.pth', load_results='', lost_disturb=0, lr=0.000125, lr_step=[60], ltrb=False, ltrb_amodal=False, ltrb_amodal_weight=0.1, ltrb_weight=0.1, master_batch_size=-1, max_age=-1, max_frame_dist=3, max_pc=1000, max_pc_dist=40.0, model_output_list=False, msra_outchannel=256, neck='dlaup', new_thresh=0.3, nms=False, no_color_aug=False, no_pause=False, no_pre_img=False, non_block_test=False, normalize_depth=True, not_cuda_benchmark=False, not_max_crop=False, not_prefetch_test=False, not_rand_crop=False, not_set_cuda_env=False, not_show_bbox=False, not_show_number=False, num_classes=10, num_epochs=70, num_head_conv=1, num_img_channels=3, num_iters=-1, num_resnet_layers=101, num_stacks=1, num_workers=0, nuscenes_att=True, nuscenes_att_weight=1, off_weight=1, optim='adam', out_thresh=-1, output_h=112, output_res=200, output_w=200, pad=31, pc_atts=['x', 'y', 'z', 'dyn_prop', 'id', 'rcs', 'vx', 'vy', 'vx_comp', 'vy_comp', 'is_quality_valid', 'ambig_state', 'x_rms', 'y_rms', 'invalid_state', 'pdh0', 'vx_rms', 'vy_rms'], pc_feat_channels={'pc_dep': 0, 'pc_vx': 1, 'pc_vz': 2}, pc_feat_lvl=['pc_dep', 'pc_vx', 'pc_vz'], pc_roi_method='pillars', pc_z_offset=-0.0, pillar_dims=[1.5, 0.2, 0.2], pointcloud=True, pre_hm=False, pre_img=False, pre_thresh=-1, print_iter=0, prior_bias=-4.6, public_det=False, qualitative=False, r_a=250, r_b=5, radar_sweeps=3, reg_loss='l1', reset_hm=False, resize_video=False, resume=True, reuse_hm=False, root_dir='/content/CenterFusion/src/lib/../..', rot_weight=1, rotate=0, run_dataset_eval=True, same_aug_pre=False, save_all=False, save_dir='/content/CenterFusion/src/lib/../../exp/ddd/centerfusion', save_framerate=30, save_img_suffix='', save_imgs=[], save_point=[90], save_results=False, save_video=False, scale=0, secondary_heads=['velocity', 'nuscenes_att', 'dep_sec', 'rot_sec'], seed=317, shift=0, show_track_color=False, show_velocity=False, shuffle_train=False, sigmoid_dep_sec=True, skip_first=-1, sort_det_by_dist=False, tango_color=False, task='ddd', test_dataset='nuscenes', test_focal_length=-1, test_scales=[1.0], track_thresh=0.3, tracking=False, tracking_weight=1, train_split='train', trainval=False, transpose_video=False, use_loaded_results=False, val_intervals=10, val_split='mini_val', velocity=True, velocity_weight=1, video_h=512, video_w=512, vis_gt_bev='', vis_thresh=0.3, warm_start_weights=False, weights={'hm': 1, 'reg': 1, 'wh': 0.1, 'dep': 1, 'rot': 1, 'dim': 1, 'amodel_offset': 1, 'dep_sec': 1, 'rot_sec': 1, 'nuscenes_att': 1, 'velocity': 1}, wh_weight=0.1, zero_pre_hm=False, zero_tracking=False)\n",
            "Dataset version \n",
            "==> initializing mini_val data from /content/CenterFusion/src/lib/../../data/nuscenes/annotations_3sweeps/mini_val.json, \n",
            " images from /content/CenterFusion/src/lib/../../data/nuscenes ...\n",
            "loading annotations into memory...\n",
            "Done (t=0.70s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded mini_val 486 samples\n",
            "Creating model...\n",
            "Using node type: (<class 'model.networks.dla.DeformConv'>, <class 'model.networks.dla.DeformConv'>)\n",
            "Warning: No ImageNet pretrain!!\n",
            "loaded ../models/model_40.pth, epoch 40\n",
            "centerfusion |################################| [485/486]|Tot: 0:05:48 |ETA: 0:00:01 |tot 0.408s (0.563s) |load 0.004s (0.005s) |pre 0.001s (0.001s) |net 0.360s (0.518s) |dec 0.002s (0.002s) |post 0.010s (0.011s) |merge 0.032s (0.027s) |track 0.000s (0.000s) \n",
            "Converting nuscenes format...\n",
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.594 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n",
            "Initializing nuScenes detection evaluation\n",
            "Loaded results from /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/results_nuscenes_det_mini_val.json. Found detections for 81 samples.\n",
            "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
            "100% 81/81 [00:00<00:00, 345.69it/s]\n",
            "Loaded ground truth annotations for 81 samples.\n",
            "Filtering predictions\n",
            "=> Original number of boxes: 40500\n",
            "=> After distance based filtering: 29814\n",
            "=> After LIDAR and RADAR points based filtering: 29814\n",
            "=> After bike rack filtering: 29771\n",
            "Filtering ground truth annotations\n",
            "=> Original number of boxes: 4441\n",
            "=> After distance based filtering: 3785\n",
            "=> After LIDAR and RADAR points based filtering: 3393\n",
            "=> After bike rack filtering: 3393\n",
            "Accumulating metric data...\n",
            "Calculating metrics...\n",
            "Saving metrics to: /content/CenterFusion/src/lib/../../exp/ddd/centerfusion/nuscenes_eval_det_output_mini_val/\n",
            "mAP: 0.2470\n",
            "mATE: 0.8069\n",
            "mASE: 0.4923\n",
            "mAOE: 0.7379\n",
            "mAVE: 0.9889\n",
            "mAAE: 0.3860\n",
            "NDS: 0.2823\n",
            "Eval time: 7.1s\n",
            "\n",
            "Per-class results:\n",
            "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
            "car\t0.409\t0.609\t0.176\t0.182\t0.204\t0.099\n",
            "truck\t0.352\t0.617\t0.173\t0.132\t0.212\t0.496\n",
            "bus\t0.512\t0.563\t0.125\t0.239\t2.513\t0.163\n",
            "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
            "pedestrian\t0.361\t0.786\t0.337\t0.696\t0.674\t0.167\n",
            "motorcycle\t0.209\t0.972\t0.339\t1.538\t0.068\t0.003\n",
            "bicycle\t0.084\t0.933\t0.403\t0.854\t2.240\t0.160\n",
            "traffic_cone\t0.543\t0.590\t0.369\tnan\tnan\tnan\n",
            "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/CenterFusion/exp /content/gdrive/MyDrive/Colab\\ Notebooks/Project/CenterFusion/Experiments/colab_40 "
      ],
      "metadata": {
        "id": "xkHgjzdmctLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/CenterFusion/exp/ddd/centerfusion/debug /content/gdrive/MyDrive/Colab\\ Notebooks/Project/CenterFusion/Experiments/colab_40/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRq93oLnhbDb",
        "outputId": "35ddb95e-b290-43c1-ec50-7bdfde2efe88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Results (Vanilla)"
      ],
      "metadata": {
        "id": "6EEH5OBGn6Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "#cv.imread('/content/gdrive/MyDrive/Colab\\ Notebooks/Project/CenterFusion/Experiments/colab_40/debug/.jpg')"
      ],
      "metadata": {
        "id": "-fRTCzvroUOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifications"
      ],
      "metadata": {
        "id": "aCVHl8C-mZ_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration"
      ],
      "metadata": {
        "id": "7iQD8GPtCbkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "P64Rj6ufEqGu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        # basic experiment setting\n",
        "        self.task = 'ddd' # help='ctdet | ddd | multi_pose | tracking or combined with ,'\n",
        "        self.dataset = 'nuscenes'\n",
        "        self.test_dataset = '' # help='coco | kitti | coco_hp | pascal'\n",
        "        self.exp_id = 'default'\n",
        "        self.eval = True # help='only evaluate the val split and quit'\n",
        "        self.debug = 0 # help='level of visualization. 1: only show the final detection results 2: show the network output features\n",
        "                        # 3: use matplot to display # useful when launching training with ipython notebook 4: save all visualizations to disk\n",
        "        self.no_pause = True\n",
        "        self.demo = '' # help='path to image/ image folders/ video or \"webcam\"\n",
        "        self.load_model = '' # help='path to pretrained model'\n",
        "        self.resume = True # help='resume an experiment.'\n",
        "\n",
        "        # system \n",
        "        self.gpus = '0' # help='-1 for CPU, use comma for multiple gpus'\n",
        "        self.num_workers = 4 # help='dataloader threads. 0 for single-thread.'\n",
        "        self.not_cuda_benchmark = True # help='disable when the input size is not fixed.'\n",
        "        self.seed = 317\n",
        "        self.not_set_cuda_env = True # help='used when training in slurm clusters.'\n",
        "\n",
        "        # log\n",
        "        self.print_iter = 0 # help='disable progress bar and print to screen.'\n",
        "        self.save_all = True # help='save model to disk every 5 epochs.'\n",
        "        self.vis_thresh = 0.3 # help='visualization threshold.'\n",
        "        self.debugger_theme = 'white' # white | black\n",
        "        self.run_dataset_eval = True # help='use dataset specific evaluation function in eval'\n",
        "        self.save_imgs = '' #help='list of images to save in debug. empty to save all'\n",
        "        self.save_img_suffix = ''\n",
        "        self.skip_first = -1 # help='skip first n images in demo mode'\n",
        "        self.save_video = True\n",
        "        self.save_framerate = 30\n",
        "        self.resize_video = True\n",
        "        self.video_h = 512\n",
        "        self.video_w = 512\n",
        "        self.transpose_video = True\n",
        "        self.show_track_color = True\n",
        "        self.not_show_bbox = True\n",
        "        self.not_show_number = True\n",
        "        self.qualitative = True\n",
        "        self.tango_color = True\n",
        "\n",
        "        # model\n",
        "        self.arch = 'dla_34' # help='model architecture. Currently tested'\n",
        "                             #     'res_18 | res_101 | resdcn_18 | resdcn_101 |'\n",
        "                             #     'dlav0_34 | dla_34 | hourglass')\n",
        "        self.dla_node = 'dcn'\n",
        "        self.head_conv = -1 # help='conv layer channels for output head'\n",
        "                            #      '0 for no conv layer'\n",
        "                            #      '-1 for default setting: '\n",
        "                            #      '64 for resnets and 256 for dla.'\n",
        "        self.num_head_conv = 1 # help='number of conv layers before each output head'\n",
        "        self.head_kernel = 3\n",
        "        self.down_ratio = 4 # help='output stride. Currently only supports 4.'\n",
        "        self.num_classes = -1\n",
        "        self.num_resnet_layers = 101\n",
        "        self.backbone = 'dla34' # help='backbone for the generic detection network'\n",
        "        self.neck = 'dlaup' # help='neck for the generic detection network'\n",
        "        self.msra_outchannel = 256\n",
        "        self.prior_bias = -4.6\n",
        "        self.heads=''\n",
        "        \n",
        "        # input\n",
        "        self.input_res = -1 # help='input height and width. -1 for default from '\n",
        "                            # 'dataset. Will be overriden by input_h | input_w'\n",
        "        self.input_h = -1 # help='input height. -1 for default from dataset.'\n",
        "        self.input_w = -1 # help='input width. -1 for default from dataset.'\n",
        "        self.dataset_version = ''\n",
        "\n",
        "        # train\n",
        "        self.optim = 'adam'\n",
        "        self.lr = 1.25e-4 # help='learning rate for batch size 32.'\n",
        "        self.lr_step = '60' # help='drop learning rate by 10.'\n",
        "        self.save_point = '90' # help='when to save the model to disk.'\n",
        "        self.num_epochs = 70 # help='total training epochs.'\n",
        "        self.batch_size = 32 # batch size\n",
        "        self.master_batch_size = -1 # help='batch size on the master gpu.'\n",
        "        self.num_iters = -1 # help='default: #samples / batch_size.'\n",
        "        self.val_intervals = 10 # help='number of epochs to run validation.'\n",
        "        self.trainval = True # help='include validation in training and test on test set')\n",
        "        self.ltrb = True\n",
        "        self.ltrb_weight = 0.1\n",
        "        self.reset_hm = True\n",
        "        self.reuse_hm = True\n",
        "        self.dense_reg = 1\n",
        "        self.shuffle_train = True\n",
        "\n",
        "        # test\n",
        "        self.flip_test = True # help='flip data augmentation.'\n",
        "        self.test_scales = '1' # help='multi scale test augmentation.'\n",
        "        self.nms = True\n",
        "        self.K = 100 # help='max number of output objects.'\n",
        "        self.not_prefetch_test = True # help='not use parallal data pre-processing.'\n",
        "        self.fix_short = -1\n",
        "        self.keep_res = True # help='keep the original resolution during validation.'\n",
        "        self.out_thresh = -1\n",
        "        self.depth_scale = 1\n",
        "        self.save_results = True\n",
        "        self.load_results = ''\n",
        "        self.use_loaded_results = True\n",
        "        self.ignore_loaded_cats = ''\n",
        "        self.model_output_list = True # help='Used when convert to onnx'\n",
        "        self.non_block_test = True\n",
        "        self.vis_gt_bev = '' # help='path to gt bev images'\n",
        "        self.test_focal_length = -1\n",
        "\n",
        "        # dataset\n",
        "        self.not_rand_crop = True # help='not use the random crop data augmentation from CornerNet.'\n",
        "        self.not_max_crop = True # help='used when the training dataset has inbalanced aspect ratios.\n",
        "        self. shift = 0 # help='when not using random crop, 0.1 apply shift augmentation.'\n",
        "        self.scale = 0 # help='when not using random crop, 0.4 apply scale augmentation.'\n",
        "        self.aug_rot = 0 # help='probability of applying rotation augmentation.'\n",
        "        self.rotate = 0 # help='when not using random crop apply rotation augmentation.'\n",
        "        self.flip = 0.5\n",
        "        self.no_color_aug = True\n",
        "\n",
        "        # Tracking\n",
        "        self.tracking = True\n",
        "        self.pre_hm = True\n",
        "        self.same_aug_pre = True\n",
        "        self.zero_pre_hm = True\n",
        "        self.hm_disturb = 0\n",
        "        self.lost_disturb = 0\n",
        "        self.fp_disturb = 0\n",
        "        self.pre_thresh = -1\n",
        "        self.track_thresh = 0.3\n",
        "        self.new_thresh = 0.3\n",
        "        self.max_frame_dist = 3\n",
        "        self.ltrb_amodal = True\n",
        "        self.ltrb_amodal_weight = 0.1\n",
        "        self.public_det = True\n",
        "        self.zero_tracking = True\n",
        "        self.hungarian = True\n",
        "        self.max_age = -1\n",
        "\n",
        "        # loss\n",
        "        self.tracking_weight = 1\n",
        "        self.reg_loss = 'l1' # help='regression loss: sl1 | l1 | l2'\n",
        "        self.hm_weight = 1 # help='loss weight for keypoint heatmaps.'\n",
        "        self.off_weight = 1 # help='loss weight for keypoint local offsets.'\n",
        "        self.wh_weight = 0.1 # help='loss weight for bounding box size.'\n",
        "        self.hp_weight = 1 # help='loss weight for human pose offset.'\n",
        "        self.hm_hp_weight = 1 # help='loss weight for human keypoint heatmap.'\n",
        "        self.amodel_offset_weight = 1\n",
        "        self.dep_weight = 1 # loss weight for depth\n",
        "        self.dep_res_weight = 1 # help='loss weight for depth residual.'\n",
        "        self.dim_weight = 1 # help='loss weight for 3d bounding box size.'\n",
        "        self.rot_weight = 1 # help='loss weight for orientation.'\n",
        "        self.nuscenes_att = True\n",
        "        self.nuscenes_att_weight = 1\n",
        "        self.velocity = True\n",
        "        self.velocity_weight = 1\n",
        "\n",
        "        # custom dataset\n",
        "        self.custom_dataset_img_path = ''\n",
        "        self.custom_dataset_ann_path = ''\n",
        "\n",
        "        # pointclouds and nuscenes dataset\n",
        "        self.pointcloud = True\n",
        "        self.train_split = 'mini_train' # choices=['train','mini_train', 'train_detect', 'train_track', 'mini_train_2', 'trainval']\n",
        "        self.val_split = 'mini_val' # choices=['val','mini_val','test']\n",
        "        self.max_pc = 1000 # help='maximum number of points in the point cloud'\n",
        "        self.r_a = 250 # help='alpha parameter for hm size calculation'\n",
        "        self.r_b = 5 # help='beta parameter for hm size calculation'\n",
        "        self.img_format = 'jpg'\n",
        "        self.max_pc_dist = 100.0 # help='remove points beyond max_pc_dist meters'\n",
        "        self.freeze_backbone = True # help='freeze the backbone network and only train heads'\n",
        "        self.radar_sweeps = 1 # help='number of radar sweeps in point cloud'\n",
        "        self.warm_start_weights = True # help='try to reuse weights even if dimensions dont match'\n",
        "        self.pc_z_offset = 0 # help='raise all Radar points in z direction'\n",
        "        self.eval_n_plots = 0 # help='number of sample plots drawn in eval'\n",
        "        self.eval_render_curves = True # help='render and save evaluation curves'\n",
        "        self.hm_transparency = 0.7 # help='heatmap visualization transparency'\n",
        "        self.iou_thresh = 0 # help='IOU threshold for filtering overlapping detections'\n",
        "        self.pillar_dims = '2,0.5,0.5' # help='Radar pillar dimensions (h,w,l)'\n",
        "        self.show_velocity = True\n",
        "    \n",
        "    def parse(self, opt=''):\n",
        "      # if args == '':\n",
        "      #   opt = self.parser.parse_args()\n",
        "      # else:\n",
        "      #   opt = self.parser.parse_args(args)\n",
        "    \n",
        "      if opt.test_dataset == '':\n",
        "        opt.test_dataset = opt.dataset\n",
        "      \n",
        "      opt.gpus_str = opt.gpus\n",
        "      opt.gpus = [int(gpu) for gpu in opt.gpus.split(',')]\n",
        "      opt.gpus = [i for i in range(len(opt.gpus))] if opt.gpus[0] >=0 else [-1]\n",
        "      opt.lr_step = [int(i) for i in opt.lr_step.split(',')]\n",
        "      opt.save_point = [int(i) for i in opt.save_point.split(',')]\n",
        "      opt.test_scales = [float(i) for i in opt.test_scales.split(',')]\n",
        "      opt.save_imgs = [i for i in opt.save_imgs.split(',')] \\\n",
        "        if opt.save_imgs != '' else []\n",
        "      opt.ignore_loaded_cats = \\\n",
        "        [int(i) for i in opt.ignore_loaded_cats.split(',')] \\\n",
        "        if opt.ignore_loaded_cats != '' else []\n",
        "\n",
        "      opt.num_workers = max(opt.num_workers, 2 * len(opt.gpus))\n",
        "      opt.pre_img = False\n",
        "      if 'tracking' in opt.task:\n",
        "        print('Running tracking')\n",
        "        opt.tracking = True\n",
        "        opt.out_thresh = max(opt.track_thresh, opt.out_thresh)\n",
        "        opt.pre_thresh = max(opt.track_thresh, opt.pre_thresh)\n",
        "        opt.new_thresh = max(opt.track_thresh, opt.new_thresh)\n",
        "        opt.pre_img = not opt.no_pre_img\n",
        "        print('Using tracking threshold for out threshold!', opt.track_thresh)\n",
        "        if 'ddd' in opt.task:\n",
        "          opt.show_track_color = True\n",
        "\n",
        "      opt.fix_res = not opt.keep_res\n",
        "      print('Fix size testing.' if opt.fix_res else 'Keep resolution testing.')\n",
        "\n",
        "      if opt.head_conv == -1: # init default head_conv\n",
        "        opt.head_conv = 256 if 'dla' in opt.arch else 64\n",
        "\n",
        "      opt.pad = 127 if 'hourglass' in opt.arch else 31\n",
        "      opt.num_stacks = 2 if opt.arch == 'hourglass' else 1\n",
        "\n",
        "      if opt.master_batch_size == -1:\n",
        "        opt.master_batch_size = opt.batch_size // len(opt.gpus)\n",
        "      rest_batch_size = (opt.batch_size - opt.master_batch_size)\n",
        "      opt.chunk_sizes = [opt.master_batch_size]\n",
        "      for i in range(len(opt.gpus) - 1):\n",
        "        slave_chunk_size = rest_batch_size // (len(opt.gpus) - 1)\n",
        "        if i < rest_batch_size % (len(opt.gpus) - 1):\n",
        "          slave_chunk_size += 1\n",
        "        opt.chunk_sizes.append(slave_chunk_size)\n",
        "      print('training chunk_sizes:', opt.chunk_sizes)\n",
        "\n",
        "      if opt.debug > 0:\n",
        "        opt.num_workers = 0\n",
        "        opt.batch_size = 1\n",
        "        opt.gpus = [opt.gpus[0]]\n",
        "        opt.master_batch_size = -1\n",
        "\n",
        "      # log dirs\n",
        "      opt.root_dir = os.path.join(os.path.dirname(opt.root))\n",
        "      opt.data_dir = os.path.join(opt.root_dir, 'data')\n",
        "      opt.exp_dir = os.path.join(opt.root_dir, 'exp', opt.task)\n",
        "      opt.save_dir = os.path.join(opt.exp_dir, opt.exp_id)\n",
        "      opt.debug_dir = os.path.join(opt.save_dir, 'debug')\n",
        "      \n",
        "      if opt.resume and opt.load_model == '':\n",
        "        opt.load_model = os.path.join(opt.save_dir, 'model_last.pth')\n",
        "\n",
        "      # point cloud settings\n",
        "      opt.pc_atts = ['x', 'y', 'z', 'dyn_prop', 'id', 'rcs', 'vx', 'vy', \n",
        "                      'vx_comp', 'vy_comp', 'is_quality_valid', \n",
        "                      'ambig_state', 'x_rms', 'y_rms', 'invalid_state', \n",
        "                      'pdh0', 'vx_rms', 'vy_rms']\n",
        "      pc_attr_ind = {x:i for i,x in enumerate(opt.pc_atts)}\n",
        "      opt.pillar_dims = [float(i) for i in opt.pillar_dims.split(',')]\n",
        "      opt.num_img_channels = 3\n",
        "      opt.hm_dist_thresh = None\n",
        "      opt.sigmoid_dep_sec = False\n",
        "      opt.hm_to_box_ratio = 0.3\n",
        "      opt.secondary_heads = []\n",
        "      opt.custom_head_convs = {}\n",
        "      opt.normalize_depth = False\n",
        "      opt.disable_frustum = False\n",
        "      opt.layers_to_freeze = [\n",
        "        'base', \n",
        "        'dla_up',\n",
        "        'ida_up',\n",
        "        # 'hm'\n",
        "        # 'reg'\n",
        "        # 'wh'\n",
        "        # 'dep'\n",
        "        # 'rot'\n",
        "        # 'dim'\n",
        "        # 'amodel_offset'\n",
        "        # 'dep_sec'\n",
        "        # 'nuscenes_att'\n",
        "        # 'velocity'\n",
        "      ]\n",
        "    \n",
        "\n",
        "      if opt.pointcloud:\n",
        "        extra_pc_feats = []\n",
        "        ##------------------------------------------------------------------------\n",
        "        opt.pc_roi_method = \"pillars\" # \"hm\"\n",
        "        opt.pillar_dims = [1.5,0.2,0.2]\n",
        "        opt.pc_feat_lvl = [\n",
        "          'pc_dep',\n",
        "          'pc_vx',\n",
        "          'pc_vz',\n",
        "        ]\n",
        "        opt.frustumExpansionRatio = 0.0\n",
        "        opt.disable_frustum = False\n",
        "        opt.sort_det_by_dist = False\n",
        "        opt.sigmoid_dep_sec = True\n",
        "        opt.normalize_depth = True\n",
        "        opt.secondary_heads = ['velocity', 'nuscenes_att', 'dep_sec', 'rot_sec']\n",
        "        opt.hm_dist_thresh = {\n",
        "          'car': 0, \n",
        "          'truck': 0,\n",
        "          'bus': 0,\n",
        "          'trailer': 0, \n",
        "          'construction_vehicle': 0, \n",
        "          'pedestrian': 1,\n",
        "          'motorcycle': 1,\n",
        "          'bicycle': 1, \n",
        "          'traffic_cone': 0, \n",
        "          'barrier': 0\n",
        "        }\n",
        "\n",
        "        opt.custom_head_convs = {\n",
        "          'dep_sec': 3,\n",
        "          'rot_sec': 3,\n",
        "          'velocity': 3,\n",
        "          'nuscenes_att': 3,\n",
        "        }\n",
        "        \n",
        "        opt.pc_feat_channels = {feat: i for i,feat in enumerate(opt.pc_feat_lvl)}\n",
        "\n",
        "      CATS = ['car', 'truck', 'bus', 'trailer', 'construction_vehicle', \n",
        "          'pedestrian', 'motorcycle', 'bicycle', 'traffic_cone', 'barrier']\n",
        "      CAT_IDS = {v: i for i, v in enumerate(CATS)}\n",
        "      \n",
        "      if opt.hm_dist_thresh is not None:\n",
        "        temp = {}\n",
        "        for (k,v) in opt.hm_dist_thresh.items():\n",
        "          temp[CAT_IDS[k]] = v\n",
        "        opt.hm_dist_thresh = temp\n",
        "      \n",
        "      return opt\n",
        "\n",
        "    def update_dataset_info_and_set_heads(self, opt, dataset):\n",
        "      opt.num_classes = dataset.num_categories \\\n",
        "                        if opt.num_classes < 0 else opt.num_classes\n",
        "      # input_h(w): opt.input_h overrides opt.input_res overrides dataset default\n",
        "      input_h, input_w = dataset.default_resolution\n",
        "      input_h = opt.input_res if opt.input_res > 0 else input_h\n",
        "      input_w = opt.input_res if opt.input_res > 0 else input_w\n",
        "      opt.input_h = opt.input_h if opt.input_h > 0 else input_h\n",
        "      opt.input_w = opt.input_w if opt.input_w > 0 else input_w\n",
        "      opt.output_h = opt.input_h // opt.down_ratio\n",
        "      opt.output_w = opt.input_w // opt.down_ratio\n",
        "      opt.input_res = max(opt.input_h, opt.input_w)\n",
        "      opt.output_res = max(opt.output_h, opt.output_w)\n",
        "    \n",
        "      opt.heads = {'hm': opt.num_classes, 'reg': 2, 'wh': 2}\n",
        "\n",
        "      if 'tracking' in opt.task:\n",
        "        opt.heads.update({'tracking': 2})\n",
        "\n",
        "      if 'ddd' in opt.task:\n",
        "        opt.heads.update({'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2})\n",
        "\n",
        "      if opt.pointcloud:\n",
        "        opt.heads.update({'dep_sec': 1})\n",
        "        opt.heads.update({'rot_sec': 8})\n",
        "      \n",
        "      if 'multi_pose' in opt.task:\n",
        "        opt.heads.update({\n",
        "          'hps': dataset.num_joints * 2, 'hm_hp': dataset.num_joints,\n",
        "          'hp_offset': 2})\n",
        "\n",
        "      if opt.ltrb:\n",
        "        opt.heads.update({'ltrb': 4})\n",
        "      if opt.ltrb_amodal:\n",
        "        opt.heads.update({'ltrb_amodal': 4})\n",
        "      if opt.nuscenes_att:\n",
        "        opt.heads.update({'nuscenes_att': 8})\n",
        "      if opt.velocity:\n",
        "        opt.heads.update({'velocity': 3})\n",
        "\n",
        "      weight_dict = {'hm': opt.hm_weight, 'wh': opt.wh_weight,\n",
        "                    'reg': opt.off_weight, 'hps': opt.hp_weight,\n",
        "                    'hm_hp': opt.hm_hp_weight, 'hp_offset': opt.off_weight,\n",
        "                    'dep': opt.dep_weight, 'dep_res': opt.dep_res_weight,\n",
        "                    'rot': opt.rot_weight, 'dep_sec': opt.dep_weight,\n",
        "                    'dim': opt.dim_weight, 'rot_sec': opt.rot_weight,\n",
        "                    'amodel_offset': opt.amodel_offset_weight,\n",
        "                    'ltrb': opt.ltrb_weight,\n",
        "                    'tracking': opt.tracking_weight,\n",
        "                    'ltrb_amodal': opt.ltrb_amodal_weight,\n",
        "                    'nuscenes_att': opt.nuscenes_att_weight,\n",
        "                    'velocity': opt.velocity_weight}\n",
        "      opt.weights = {head: weight_dict[head] for head in opt.heads}\n",
        "        \n",
        "      for head in opt.weights:\n",
        "        if opt.weights[head] == 0:\n",
        "          del opt.heads[head]\n",
        "      \n",
        "      temp_head_conv = opt.head_conv\n",
        "      opt.head_conv = {head: [opt.head_conv \\\n",
        "        for i in range(opt.num_head_conv if head != 'reg' else 1)] for head in opt.heads}\n",
        "      \n",
        "      ## update custom head convs\n",
        "      if opt.pointcloud:\n",
        "        temp = {k: [temp_head_conv for i in range(v)] for k,v in opt.custom_head_convs.items()}\n",
        "        opt.head_conv.update(temp)\n",
        "      \n",
        "      print('input h w:', opt.input_h, opt.input_w)\n",
        "      print('heads', opt.heads)\n",
        "      print('weights', opt.weights)\n",
        "      print('head conv', opt.head_conv)\n",
        "\n",
        "      return opt\n"
      ],
      "metadata": {
        "id": "bH8SJc3ZGpnX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "VjJq9-l818ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####install required dependencies"
      ],
      "metadata": {
        "id": "NmoDhX1q8EA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==0.21.0\n",
        "!pip install opencv-python\n",
        "!pip install Cython\n",
        "!pip install numba\n",
        "!pip install progress\n",
        "!pip install matplotlib\n",
        "!pip install easydict\n",
        "!pip install scipy\n",
        "!pip install pyquaternion\n",
        "!pip install nuscenes-devkit\n",
        "!pip install pyyaml\n",
        "!pip install motmetrics==1.1.3\n",
        "!pip install tensorboardx"
      ],
      "metadata": {
        "id": "YLjtNh5v2WvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0238b71a-de00-4079-b2e6-d4f968d4b6b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==0.21.0 in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0) (1.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.5)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.28)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Requirement already satisfied: progress in /usr/local/lib/python3.7/dist-packages (1.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.5)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.7/dist-packages (0.9.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyquaternion) (1.21.5)\n",
            "Requirement already satisfied: nuscenes-devkit in /usr/local/lib/python3.7/dist-packages (1.1.9)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (1.8.1.post1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (1.21.5)\n",
            "Requirement already satisfied: pycocotools>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (2.0.4)\n",
            "Requirement already satisfied: descartes in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (0.4.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (4.1.2.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (4.63.0)\n",
            "Requirement already satisfied: Pillow>6.2.1 in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (0.21.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (4.2.4)\n",
            "Requirement already satisfied: pyquaternion>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from nuscenes-devkit) (0.9.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nuscenes-devkit) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nuscenes-devkit) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nuscenes-devkit) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nuscenes-devkit) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->nuscenes-devkit) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit) (5.3.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit) (7.6.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->nuscenes-devkit) (5.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit) (3.5.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit) (1.0.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nuscenes-devkit) (5.1.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (4.9.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (4.11.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (3.7.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nuscenes-devkit) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nuscenes-devkit) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nuscenes-devkit) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->nuscenes-devkit) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->nuscenes-devkit) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->nuscenes-devkit) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (4.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->nuscenes-devkit) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->nuscenes-devkit) (21.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nuscenes-devkit) (2.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: motmetrics==1.1.3 in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.1->motmetrics==1.1.3) (1.15.0)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (2.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model Utils"
      ],
      "metadata": {
        "id": "JYQ1bYXf6PwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _sigmoid(x):\n",
        "  y = torch.clamp(x.sigmoid_(), min=1e-4, max=1-1e-4)\n",
        "  return y\n",
        "\n",
        "def _sigmoid12(x):\n",
        "  y = torch.clamp(x.sigmoid_(), 1e-12)\n",
        "  return y\n",
        "\n",
        "def _gather_feat(feat, ind):\n",
        "  dim = feat.size(2)\n",
        "  ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n",
        "  feat = feat.gather(1, ind)\n",
        "  return feat\n",
        "\n",
        "def _tranpose_and_gather_feat(feat, ind):\n",
        "  feat = feat.permute(0, 2, 3, 1).contiguous()\n",
        "  feat = feat.view(feat.size(0), -1, feat.size(3))\n",
        "  feat = _gather_feat(feat, ind)\n",
        "  return feat\n",
        "\n",
        "def flip_tensor(x):\n",
        "  return torch.flip(x, [3])\n",
        "  # tmp = x.detach().cpu().numpy()[..., ::-1].copy()\n",
        "  # return torch.from_numpy(tmp).to(x.device)\n",
        "\n",
        "def flip_lr(x, flip_idx):\n",
        "  tmp = x.detach().cpu().numpy()[..., ::-1].copy()\n",
        "  shape = tmp.shape\n",
        "  for e in flip_idx:\n",
        "    tmp[:, e[0], ...], tmp[:, e[1], ...] = \\\n",
        "      tmp[:, e[1], ...].copy(), tmp[:, e[0], ...].copy()\n",
        "  return torch.from_numpy(tmp.reshape(shape)).to(x.device)\n",
        "\n",
        "def flip_lr_off(x, flip_idx):\n",
        "  tmp = x.detach().cpu().numpy()[..., ::-1].copy()\n",
        "  shape = tmp.shape\n",
        "  tmp = tmp.reshape(tmp.shape[0], 17, 2, \n",
        "                    tmp.shape[2], tmp.shape[3])\n",
        "  tmp[:, :, 0, :, :] *= -1\n",
        "  for e in flip_idx:\n",
        "    tmp[:, e[0], ...], tmp[:, e[1], ...] = \\\n",
        "      tmp[:, e[1], ...].copy(), tmp[:, e[0], ...].copy()\n",
        "  return torch.from_numpy(tmp.reshape(shape)).to(x.device)\n",
        "\n",
        "def _nms(heat, kernel=3):\n",
        "  pad = (kernel - 1) // 2\n",
        "\n",
        "  hmax = nn.functional.max_pool2d(\n",
        "      heat, (kernel, kernel), stride=1, padding=pad)\n",
        "  keep = (hmax == heat).float()\n",
        "  return heat * keep\n",
        "\n",
        "def _topk_channel(scores, K=100):\n",
        "  batch, cat, height, width = scores.size()\n",
        "  \n",
        "  topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), K)\n",
        "\n",
        "  topk_inds = topk_inds % (height * width)\n",
        "  topk_ys   = (topk_inds / width).int().float()\n",
        "  topk_xs   = (topk_inds % width).int().float()\n",
        "\n",
        "  return topk_scores, topk_inds, topk_ys, topk_xs\n",
        "\n",
        "def _topk(scores, K=100):\n",
        "  batch, cat, height, width = scores.size()\n",
        "    \n",
        "  topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), K)\n",
        "\n",
        "  topk_inds = topk_inds % (height * width)\n",
        "  topk_ys   = (topk_inds / width).int().float()\n",
        "  topk_xs   = (topk_inds % width).int().float()\n",
        "    \n",
        "  topk_score, topk_ind = torch.topk(topk_scores.view(batch, -1), K)\n",
        "  topk_clses = (topk_ind / K).int()\n",
        "  topk_inds = _gather_feat(\n",
        "      topk_inds.view(batch, -1, 1), topk_ind).view(batch, K)\n",
        "  topk_ys = _gather_feat(topk_ys.view(batch, -1, 1), topk_ind).view(batch, K)\n",
        "  topk_xs = _gather_feat(topk_xs.view(batch, -1, 1), topk_ind).view(batch, K)\n",
        "\n",
        "  return topk_score, topk_inds, topk_clses, topk_ys, topk_xs\n"
      ],
      "metadata": {
        "id": "nRLTGNJW6USN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utils Image"
      ],
      "metadata": {
        "id": "Pluz5xO-5UFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def flip(img):\n",
        "  return img[:, :, ::-1].copy()  \n",
        "\n",
        "# @numba.jit(nopython=True, nogil=True)\n",
        "def transform_preds_with_trans(coords, trans):\n",
        "    # target_coords = np.concatenate(\n",
        "    #   [coords, np.ones((coords.shape[0], 1), np.float32)], axis=1)\n",
        "    target_coords = np.ones((coords.shape[0], 3), np.float32)\n",
        "    target_coords[:, :2] = coords\n",
        "    target_coords = np.dot(trans, target_coords.transpose()).transpose()\n",
        "    return target_coords[:, :2]\n",
        "\n",
        "\n",
        "def transform_preds(coords, center, scale, output_size):\n",
        "    target_coords = np.zeros(coords.shape)\n",
        "    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n",
        "    for p in range(coords.shape[0]):\n",
        "        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n",
        "    return target_coords\n",
        "\n",
        "\n",
        "def get_affine_transform(center,\n",
        "                         scale,\n",
        "                         rot,\n",
        "                         output_size,\n",
        "                         shift=np.array([0, 0], dtype=np.float32),\n",
        "                         inv=0):\n",
        "    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n",
        "        scale = np.array([scale, scale], dtype=np.float32)\n",
        "\n",
        "    scale_tmp = scale\n",
        "    src_w = scale_tmp[0]\n",
        "    dst_w = output_size[0]\n",
        "    dst_h = output_size[1]\n",
        "\n",
        "    rot_rad = np.pi * rot / 180\n",
        "    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n",
        "    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n",
        "\n",
        "    src = np.zeros((3, 2), dtype=np.float32)\n",
        "    dst = np.zeros((3, 2), dtype=np.float32)\n",
        "    src[0, :] = center + scale_tmp * shift\n",
        "    src[1, :] = center + src_dir + scale_tmp * shift\n",
        "    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n",
        "    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5], np.float32) + dst_dir\n",
        "\n",
        "    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n",
        "    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n",
        "\n",
        "    if inv:\n",
        "        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n",
        "    else:\n",
        "        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n",
        "\n",
        "    return trans\n",
        "\n",
        "\n",
        "def affine_transform(pt, t):\n",
        "    new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32).T\n",
        "    new_pt = np.dot(t, new_pt)\n",
        "    return new_pt[:2]\n",
        "\n",
        "\n",
        "def get_3rd_point(a, b):\n",
        "    direct = a - b\n",
        "    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n",
        "\n",
        "\n",
        "def get_dir(src_point, rot_rad):\n",
        "    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n",
        "\n",
        "    src_result = [0, 0]\n",
        "    src_result[0] = src_point[0] * cs - src_point[1] * sn\n",
        "    src_result[1] = src_point[0] * sn + src_point[1] * cs\n",
        "\n",
        "    return src_result\n",
        "\n",
        "\n",
        "def crop(img, center, scale, output_size, rot=0):\n",
        "    trans = get_affine_transform(center, scale, rot, output_size)\n",
        "\n",
        "    dst_img = cv2.warpAffine(img,\n",
        "                             trans,\n",
        "                             (int(output_size[0]), int(output_size[1])),\n",
        "                             flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    return dst_img\n",
        "\n",
        "# @numba.jit(nopython=True, nogil=True)\n",
        "def gaussian_radius(det_size, min_overlap=0.7):\n",
        "  height, width = det_size\n",
        "\n",
        "  a1  = 1\n",
        "  b1  = (height + width)\n",
        "  c1  = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
        "  sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
        "  r1  = (b1 + sq1) / 2\n",
        "\n",
        "  a2  = 4\n",
        "  b2  = 2 * (height + width)\n",
        "  c2  = (1 - min_overlap) * width * height\n",
        "  sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n",
        "  r2  = (b2 + sq2) / 2\n",
        "\n",
        "  a3  = 4 * min_overlap\n",
        "  b3  = -2 * min_overlap * (height + width)\n",
        "  c3  = (min_overlap - 1) * width * height\n",
        "  sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n",
        "  r3  = (b3 + sq3) / 2\n",
        "  return min(r1, r2, r3)\n",
        "\n",
        "\n",
        "# @numba.jit(nopython=True, nogil=True)\n",
        "def gaussian2D(shape, sigma=1):\n",
        "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
        "    y, x = np.ogrid[-m:m+1,-n:n+1]\n",
        "    # y, x = np.arange(-m, m + 1).reshape(-1, 1), np.arange(-n, n + 1).reshape(1, -1)\n",
        "    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
        "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
        "    return h\n",
        "\n",
        "# @numba.jit(nopython=True, nogil=True)\n",
        "def draw_umich_gaussian(heatmap, center, radius, k=1):\n",
        "  # import pdb; pdb.set_trace()\n",
        "  diameter = 2 * radius + 1\n",
        "  gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
        "  \n",
        "  x, y = int(center[0]), int(center[1])\n",
        "\n",
        "  height, width = heatmap.shape[0:2]\n",
        "    \n",
        "  left, right = min(x, radius), min(width - x, radius + 1)\n",
        "  top, bottom = min(y, radius), min(height - y, radius + 1)\n",
        "  # import pdb; pdb.set_trace()\n",
        "  masked_heatmap  = heatmap[y - top:y + bottom, x - left:x + right]\n",
        "  masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
        "  if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: # TODO debug\n",
        "    np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
        "  return heatmap\n",
        "\n",
        "def draw_dense_reg(regmap, heatmap, center, value, radius, is_offset=False):\n",
        "  diameter = 2 * radius + 1\n",
        "  gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
        "  value = np.array(value, dtype=np.float32).reshape(-1, 1, 1)\n",
        "  dim = value.shape[0]\n",
        "  reg = np.ones((dim, diameter*2+1, diameter*2+1), dtype=np.float32) * value\n",
        "  if is_offset and dim == 2:\n",
        "    delta = np.arange(diameter*2+1) - radius\n",
        "    reg[0] = reg[0] - delta.reshape(1, -1)\n",
        "    reg[1] = reg[1] - delta.reshape(-1, 1)\n",
        "  \n",
        "  x, y = int(center[0]), int(center[1])\n",
        "\n",
        "  height, width = heatmap.shape[0:2]\n",
        "    \n",
        "  left, right = min(x, radius), min(width - x, radius + 1)\n",
        "  top, bottom = min(y, radius), min(height - y, radius + 1)\n",
        "\n",
        "  masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
        "  masked_regmap = regmap[:, y - top:y + bottom, x - left:x + right]\n",
        "  masked_gaussian = gaussian[radius - top:radius + bottom,\n",
        "                             radius - left:radius + right]\n",
        "  masked_reg = reg[:, radius - top:radius + bottom,\n",
        "                      radius - left:radius + right]\n",
        "  if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: # TODO debug\n",
        "    idx = (masked_gaussian >= masked_heatmap).reshape(\n",
        "      1, masked_gaussian.shape[0], masked_gaussian.shape[1])\n",
        "    masked_regmap = (1-idx) * masked_regmap + idx * masked_reg\n",
        "  regmap[:, y - top:y + bottom, x - left:x + right] = masked_regmap\n",
        "  return regmap\n",
        "\n",
        "\n",
        "def draw_msra_gaussian(heatmap, center, sigma):\n",
        "  tmp_size = sigma * 3\n",
        "  mu_x = int(center[0] + 0.5)\n",
        "  mu_y = int(center[1] + 0.5)\n",
        "  w, h = heatmap.shape[0], heatmap.shape[1]\n",
        "  ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "  br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "  if ul[0] >= h or ul[1] >= w or br[0] < 0 or br[1] < 0:\n",
        "    return heatmap\n",
        "  size = 2 * tmp_size + 1\n",
        "  x = np.arange(0, size, 1, np.float32)\n",
        "  y = x[:, np.newaxis]\n",
        "  x0 = y0 = size // 2\n",
        "  g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "  g_x = max(0, -ul[0]), min(br[0], h) - ul[0]\n",
        "  g_y = max(0, -ul[1]), min(br[1], w) - ul[1]\n",
        "  img_x = max(0, ul[0]), min(br[0], h)\n",
        "  img_y = max(0, ul[1]), min(br[1], w)\n",
        "  heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]],\n",
        "    g[g_y[0]:g_y[1], g_x[0]:g_x[1]])\n",
        "  return heatmap\n",
        "\n",
        "def grayscale(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "def lighting_(data_rng, image, alphastd, eigval, eigvec):\n",
        "    alpha = data_rng.normal(scale=alphastd, size=(3, ))\n",
        "    image += np.dot(eigvec, eigval * alpha)\n",
        "\n",
        "def blend_(alpha, image1, image2):\n",
        "    image1 *= alpha\n",
        "    image2 *= (1 - alpha)\n",
        "    image1 += image2\n",
        "\n",
        "def saturation_(data_rng, image, gs, gs_mean, var):\n",
        "    alpha = 1. + data_rng.uniform(low=-var, high=var)\n",
        "    blend_(alpha, image, gs[:, :, None])\n",
        "\n",
        "def brightness_(data_rng, image, gs, gs_mean, var):\n",
        "    alpha = 1. + data_rng.uniform(low=-var, high=var)\n",
        "    image *= alpha\n",
        "\n",
        "def contrast_(data_rng, image, gs, gs_mean, var):\n",
        "    alpha = 1. + data_rng.uniform(low=-var, high=var)\n",
        "    blend_(alpha, image, gs_mean)\n",
        "\n",
        "def color_aug(data_rng, image, eig_val, eig_vec):\n",
        "    functions = [brightness_, contrast_, saturation_]\n",
        "    random.shuffle(functions)\n",
        "\n",
        "    gs = grayscale(image)\n",
        "    gs_mean = gs.mean()\n",
        "    for f in functions:\n",
        "        f(data_rng, image, gs, gs_mean, 0.4)\n",
        "    lighting_(data_rng, image, 0.1, eig_val, eig_vec)\n"
      ],
      "metadata": {
        "id": "pWCieDbW1_mc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utils PointCloud"
      ],
      "metadata": {
        "id": "BTmY_e9S55RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from nuscenes.utils.data_classes import RadarPointCloud\n",
        "from nuscenes.utils.geometry_utils import view_points, transform_matrix\n",
        "from nuscenes.utils.data_classes import RadarPointCloud\n",
        "from functools import reduce\n",
        "from typing import Tuple, Dict\n",
        "import os.path as osp\n",
        "import torch\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "from pyquaternion import Quaternion\n",
        "\n",
        "def map_pointcloud_to_image(pc, cam_intrinsic, img_shape=(1600,900)):\n",
        "    \"\"\"\n",
        "    Map point cloud from camera coordinates to the image\n",
        "    \n",
        "    :param pc (PointCloud): point cloud in vehicle or global coordinates\n",
        "    :param cam_cs_record (dict): Camera calibrated sensor record\n",
        "    :param img_shape: shape of the image (width, height)\n",
        "    :param coordinates (str): Point cloud coordinates ('vehicle', 'global') \n",
        "    :return points (nparray), depth, mask: Mapped and filtered points with depth and mask\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(pc, RadarPointCloud):\n",
        "        points = pc.points[:3,:]\n",
        "    else:\n",
        "        points = pc\n",
        "\n",
        "    (width, height) = img_shape\n",
        "    depths = points[2, :]\n",
        "    \n",
        "    ## Take the actual picture\n",
        "    points = view_points(points[:3, :], cam_intrinsic, normalize=True)\n",
        "\n",
        "    ## Remove points that are either outside or behind the camera. \n",
        "    mask = np.ones(depths.shape[0], dtype=bool)\n",
        "    mask = np.logical_and(mask, depths > 0)\n",
        "    mask = np.logical_and(mask, points[0, :] > 1)\n",
        "    mask = np.logical_and(mask, points[0, :] < width - 1)\n",
        "    mask = np.logical_and(mask, points[1, :] > 1)\n",
        "    mask = np.logical_and(mask, points[1, :] < height - 1)\n",
        "    points = points[:, mask]\n",
        "    points[2,:] = depths[mask]\n",
        "\n",
        "    return points, mask\n",
        "\n",
        "\n",
        "## A RadarPointCloud class where Radar velocity values are correctly \n",
        "# transformed to the target coordinate system\n",
        "class RadarPointCloudWithVelocity(RadarPointCloud):\n",
        "    \n",
        "    @classmethod\n",
        "    def rotate_velocity(cls, pointcloud, transform_matrix):\n",
        "        n_points = pointcloud.shape[1]\n",
        "        third_dim = np.zeros(n_points)\n",
        "        pc_velocity = np.vstack((pointcloud[[8,9], :], third_dim, np.ones(n_points)))\n",
        "        pc_velocity = transform_matrix.dot(pc_velocity)\n",
        "        \n",
        "        ## in camera coordinates, x is right, z is front\n",
        "        pointcloud[[8,9],:] = pc_velocity[[0,2],:]\n",
        "\n",
        "        return pointcloud\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_file_multisweep(cls,\n",
        "                             nusc: 'NuScenes',\n",
        "                             sample_rec: Dict,\n",
        "                             chan: str,\n",
        "                             ref_chan: str,\n",
        "                             nsweeps: int = 5,\n",
        "                             min_distance: float = 1.0) -> Tuple['PointCloud', np.ndarray]:\n",
        "        \"\"\"\n",
        "        Return a point cloud that aggregates multiple sweeps.\n",
        "        As every sweep is in a different coordinate frame, we need to map the coordinates to a single reference frame.\n",
        "        As every sweep has a different timestamp, we need to account for that in the transformations and timestamps.\n",
        "        :param nusc: A NuScenes instance.\n",
        "        :param sample_rec: The current sample.\n",
        "        :param chan: The lidar/radar channel from which we track back n sweeps to aggregate the point cloud.\n",
        "        :param ref_chan: The reference channel of the current sample_rec that the point clouds are mapped to.\n",
        "        :param nsweeps: Number of sweeps to aggregated.\n",
        "        :param min_distance: Distance below which points are discarded.\n",
        "        :return: (all_pc, all_times). The aggregated point cloud and timestamps.\n",
        "        \"\"\"\n",
        "        # Init.\n",
        "        points = np.zeros((cls.nbr_dims(), 0))\n",
        "        all_pc = cls(points)\n",
        "        all_times = np.zeros((1, 0))\n",
        "\n",
        "        # Get reference pose and timestamp.\n",
        "        ref_sd_token = sample_rec['data'][ref_chan]\n",
        "        ref_sd_rec = nusc.get('sample_data', ref_sd_token)\n",
        "        ref_pose_rec = nusc.get('ego_pose', ref_sd_rec['ego_pose_token'])\n",
        "        ref_cs_rec = nusc.get('calibrated_sensor', ref_sd_rec['calibrated_sensor_token'])\n",
        "        ref_time = 1e-6 * ref_sd_rec['timestamp']\n",
        "\n",
        "        # Homogeneous transform from ego car frame to reference frame.\n",
        "        ref_from_car = transform_matrix(ref_cs_rec['translation'], Quaternion(ref_cs_rec['rotation']), inverse=True)\n",
        "        ref_from_car_rot = transform_matrix([0.0, 0.0, 0.0], Quaternion(ref_cs_rec['rotation']), inverse=True)\n",
        "\n",
        "        # Homogeneous transformation matrix from global to _current_ ego car frame.\n",
        "        car_from_global = transform_matrix(ref_pose_rec['translation'], Quaternion(ref_pose_rec['rotation']),\n",
        "                                           inverse=True)\n",
        "        car_from_global_rot = transform_matrix([0.0, 0.0, 0.0], Quaternion(ref_pose_rec['rotation']),\n",
        "                                           inverse=True)\n",
        "\n",
        "        # Aggregate current and previous sweeps.\n",
        "        sample_data_token = sample_rec['data'][chan]\n",
        "        current_sd_rec = nusc.get('sample_data', sample_data_token)\n",
        "        for _ in range(nsweeps):\n",
        "            # Load up the pointcloud and remove points close to the sensor.\n",
        "            current_pc = cls.from_file(osp.join(nusc.dataroot, current_sd_rec['filename']))\n",
        "            current_pc.remove_close(min_distance)\n",
        "\n",
        "            # Get past pose.\n",
        "            current_pose_rec = nusc.get('ego_pose', current_sd_rec['ego_pose_token'])\n",
        "            global_from_car = transform_matrix(current_pose_rec['translation'],\n",
        "                                               Quaternion(current_pose_rec['rotation']), inverse=False)\n",
        "            global_from_car_rot = transform_matrix([0.0, 0.0, 0.0],\n",
        "                                               Quaternion(current_pose_rec['rotation']), inverse=False)\n",
        "\n",
        "            # Homogeneous transformation matrix from sensor coordinate frame to ego car frame.\n",
        "            current_cs_rec = nusc.get('calibrated_sensor', current_sd_rec['calibrated_sensor_token'])\n",
        "            car_from_current = transform_matrix(current_cs_rec['translation'], Quaternion(current_cs_rec['rotation']),\n",
        "                                                inverse=False)\n",
        "            car_from_current_rot = transform_matrix([0.0, 0.0, 0.0], Quaternion(current_cs_rec['rotation']), inverse=False)\n",
        "\n",
        "            # Fuse four transformation matrices into one and perform transform.\n",
        "            trans_matrix = reduce(np.dot, [ref_from_car, car_from_global, global_from_car, car_from_current])\n",
        "            velocity_trans_matrix = reduce(np.dot, [ref_from_car_rot, car_from_global_rot, global_from_car_rot, car_from_current_rot])\n",
        "            current_pc.transform(trans_matrix)\n",
        "\n",
        "            # Do the required rotations to the Radar velocity values\n",
        "            current_pc.points = cls.rotate_velocity(current_pc.points, velocity_trans_matrix)\n",
        "\n",
        "            # Add time vector which can be used as a temporal feature.\n",
        "            time_lag = ref_time - 1e-6 * current_sd_rec['timestamp']  # Positive difference.\n",
        "            times = time_lag * np.ones((1, current_pc.nbr_points()))\n",
        "            all_times = np.hstack((all_times, times))\n",
        "\n",
        "            # Merge with key pc.\n",
        "            all_pc.points = np.hstack((all_pc.points, current_pc.points))\n",
        "\n",
        "            # Abort if there are no previous sweeps.\n",
        "            if current_sd_rec['prev'] == '':\n",
        "                break\n",
        "            else:\n",
        "                current_sd_rec = nusc.get('sample_data', current_sd_rec['prev'])\n",
        "\n",
        "        return all_pc, all_times\n",
        "\n",
        "\n",
        "def get_alpha(rot):\n",
        "  # output: (B, 8) [bin1_cls[0], bin1_cls[1], bin1_sin, bin1_cos, \n",
        "  #                 bin2_cls[0], bin2_cls[1], bin2_sin, bin2_cos]\n",
        "  # return rot[:, 0]\n",
        "  idx = rot[:, 1] > rot[:, 5]\n",
        "  alpha1 = torch.atan2(rot[:, 2], rot[:, 3]) + (-0.5 * 3.14159)\n",
        "  alpha2 = torch.atan2(rot[:, 6], rot[:, 7]) + ( 0.5 * 3.14159)\n",
        "  # return alpha1 * idx + alpha2 * (~idx)\n",
        "  alpha = alpha1 * idx.float() + alpha2 * (~idx).float()\n",
        "  return alpha\n",
        "\n",
        "\n",
        "def alpha2rot_y(alpha, x, cx, fx):\n",
        "    \"\"\"\n",
        "    Get rotation_y by alpha + theta - 180\n",
        "    alpha : Observation angle of object, ranging [-pi..pi]\n",
        "    x : Object center x to the camera center (x-W/2), in pixels\n",
        "    rotation_y : Rotation ry around Y-axis in camera coordinates [-pi..pi]\n",
        "    \"\"\"\n",
        "    rot_y = alpha + torch.atan2(x - cx, fx)\n",
        "    if rot_y > 3.14159:\n",
        "      rot_y -= 2 * 3.14159\n",
        "    if rot_y < -3.14159:\n",
        "      rot_y += 2 * 3.14159\n",
        "    return rot_y\n",
        "\n",
        "\n",
        "def comput_corners_3d(dim, rotation_y):\n",
        "    # dim: 3\n",
        "    # location: 3\n",
        "    # rotation_y: 1\n",
        "    # return: 8 x 3\n",
        "    c, s = torch.cos(rotation_y), torch.sin(rotation_y)\n",
        "    R = torch.tensor([[c, 0, s], [0, 1, 0], [-s, 0, c]], dtype=torch.float32)\n",
        "    l, w, h = dim[2], dim[1], dim[0]\n",
        "    x_corners = [l/2, l/2, -l/2, -l/2, l/2, l/2, -l/2, -l/2]\n",
        "    y_corners = [0,0,0,0,-h,-h,-h,-h]\n",
        "    z_corners = [w/2, -w/2, -w/2, w/2, w/2, -w/2, -w/2, w/2]\n",
        "\n",
        "    corners = torch.tensor([x_corners, y_corners, z_corners], dtype=torch.float32)\n",
        "    corners_3d = torch.mm(R, corners).transpose(1, 0)\n",
        "    return corners_3d\n",
        "\n",
        "\n",
        "def get_dist_thresh(calib, ct, dim, alpha):\n",
        "    rotation_y = alpha2rot_y(alpha, ct[0], calib[0, 2], calib[0, 0])\n",
        "    corners_3d = comput_corners_3d(dim, rotation_y)\n",
        "    dist_thresh = max(corners_3d[:,2]) - min(corners_3d[:,2]) / 2.0\n",
        "    return dist_thresh\n",
        "\n",
        "\n",
        "def generate_pc_hm(output, pc_dep, calib, opt):\n",
        "      K = opt.K\n",
        "      # K = 100\n",
        "      heat = output['hm']\n",
        "      wh = output['wh']\n",
        "      pc_hm = torch.zeros_like(pc_dep)\n",
        "\n",
        "      batch, cat, height, width = heat.size()\n",
        "      scores, inds, clses, ys0, xs0 = _topk(heat, K=K)\n",
        "      xs = xs0.view(batch, K, 1) + 0.5\n",
        "      ys = ys0.view(batch, K, 1) + 0.5\n",
        "      \n",
        "      ## Initialize pc_feats\n",
        "      pc_feats = torch.zeros((batch, len(opt.pc_feat_lvl), height, width), device=heat.device)\n",
        "      dep_ind = opt.pc_feat_channels['pc_dep']\n",
        "      vx_ind = opt.pc_feat_channels['pc_vx']\n",
        "      vz_ind = opt.pc_feat_channels['pc_vz']\n",
        "      to_log = opt.sigmoid_dep_sec\n",
        "      \n",
        "      ## get estimated depths\n",
        "      out_dep = 1. / (output['dep'].sigmoid() + 1e-6) - 1.\n",
        "      dep = _tranpose_and_gather_feat(out_dep, inds) # B x K x (C)\n",
        "      if dep.size(2) == cat:\n",
        "        cats = clses.view(batch, K, 1, 1)\n",
        "        dep = dep.view(batch, K, -1, 1) # B x K x C x 1\n",
        "        dep = dep.gather(2, cats.long()).squeeze(2) # B x K x 1\n",
        "\n",
        "      ## get top bounding boxes\n",
        "      wh = _tranpose_and_gather_feat(wh, inds) # B x K x 2\n",
        "      wh = wh.view(batch, K, 2)\n",
        "      wh[wh < 0] = 0\n",
        "      if wh.size(2) == 2 * cat: # cat spec\n",
        "        wh = wh.view(batch, K, -1, 2)\n",
        "        cats = clses.view(batch, K, 1, 1).expand(batch, K, 1, 2)\n",
        "        wh = wh.gather(2, cats.long()).squeeze(2) # B x K x 2\n",
        "      bboxes = torch.cat([xs - wh[..., 0:1] / 2, \n",
        "                          ys - wh[..., 1:2] / 2,\n",
        "                          xs + wh[..., 0:1] / 2, \n",
        "                          ys + wh[..., 1:2] / 2], dim=2)  # B x K x 4\n",
        "      \n",
        "      ## get dimensions\n",
        "      dims = _tranpose_and_gather_feat(output['dim'], inds).view(batch, K, -1)\n",
        "\n",
        "      ## get rotation\n",
        "      rot = _tranpose_and_gather_feat(output['rot'], inds).view(batch, K, -1)\n",
        "\n",
        "      ## Calculate values for the new pc_hm\n",
        "      clses = clses.cpu().numpy()\n",
        "\n",
        "      for i, [pc_dep_b, bboxes_b, depth_b, dim_b, rot_b] in enumerate(zip(pc_dep, bboxes, dep, dims, rot)):\n",
        "        alpha_b = get_alpha(rot_b).unsqueeze(1)\n",
        "\n",
        "        if opt.sort_det_by_dist:\n",
        "          idx = torch.argsort(depth_b[:,0])\n",
        "          bboxes_b = bboxes_b[idx,:]\n",
        "          depth_b = depth_b[idx,:]\n",
        "          dim_b = dim_b[idx,:]\n",
        "          rot_b = rot_b[idx,:]\n",
        "          alpha_b = alpha_b[idx,:]\n",
        "\n",
        "        for j, [bbox, depth, dim, alpha] in enumerate(zip(bboxes_b, depth_b, dim_b, alpha_b)):\n",
        "          clss = clses[i,j].tolist()\n",
        "          ct = torch.tensor([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], device=pc_dep_b.device)\n",
        "          dist_thresh = get_dist_thresh(calib, ct, dim, alpha)\n",
        "          dist_thresh += dist_thresh * opt.frustumExpansionRatio\n",
        "          pc_dep_to_hm_torch(pc_hm[i], pc_dep_b, depth, bbox, dist_thresh, opt)\n",
        "      return pc_hm\n",
        "\n",
        "\n",
        "def pc_dep_to_hm_torch(pc_hm, pc_dep, dep, bbox, dist_thresh, opt):\n",
        "    if isinstance(dep, list) and len(dep) > 0:\n",
        "      dep = dep[0]\n",
        "    ct = torch.tensor(\n",
        "      [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=torch.float32)\n",
        "    bbox_int = torch.tensor([torch.floor(bbox[0]), \n",
        "                         torch.floor(bbox[1]), \n",
        "                         torch.ceil(bbox[2]), \n",
        "                         torch.ceil(bbox[3])], dtype=torch.int32)# format: xyxy\n",
        "\n",
        "    roi = pc_dep[:, bbox_int[1]:bbox_int[3]+1, bbox_int[0]:bbox_int[2]+1]\n",
        "    pc_dep = roi[opt.pc_feat_channels['pc_dep']]\n",
        "    pc_vx = roi[opt.pc_feat_channels['pc_vx']]\n",
        "    pc_vz = roi[opt.pc_feat_channels['pc_vz']]\n",
        "\n",
        "    pc_dep.sum().data\n",
        "    nonzero_inds = torch.nonzero(pc_dep, as_tuple=True)\n",
        "    \n",
        "    if len(nonzero_inds) and len(nonzero_inds[0]) > 0:\n",
        "    #   nonzero_pc_dep = torch.exp(-pc_dep[nonzero_inds])\n",
        "      nonzero_pc_dep = pc_dep[nonzero_inds]\n",
        "      nonzero_pc_vx = pc_vx[nonzero_inds]\n",
        "      nonzero_pc_vz = pc_vz[nonzero_inds]\n",
        "\n",
        "      ## Get points within dist threshold\n",
        "      within_thresh = (nonzero_pc_dep < dep+dist_thresh) \\\n",
        "              & (nonzero_pc_dep > max(0, dep-dist_thresh))\n",
        "      pc_dep_match = nonzero_pc_dep[within_thresh]\n",
        "      pc_vx_match = nonzero_pc_vx[within_thresh]\n",
        "      pc_vz_match = nonzero_pc_vz[within_thresh]\n",
        "\n",
        "      if len(pc_dep_match) > 0:\n",
        "        arg_min = torch.argmin(pc_dep_match)\n",
        "        dist = pc_dep_match[arg_min]\n",
        "        vx = pc_vx_match[arg_min]\n",
        "        vz = pc_vz_match[arg_min]\n",
        "        if opt.normalize_depth:\n",
        "          dist /= opt.max_pc_dist\n",
        "\n",
        "        w = bbox[2] - bbox[0]\n",
        "        w_interval = opt.hm_to_box_ratio*(w)\n",
        "        w_min = int(ct[0] - w_interval/2.)\n",
        "        w_max = int(ct[0] + w_interval/2.)\n",
        "        \n",
        "        h = bbox[3] - bbox[1]\n",
        "        h_interval = opt.hm_to_box_ratio*(h)\n",
        "        h_min = int(ct[1] - h_interval/2.)\n",
        "        h_max = int(ct[1] + h_interval/2.)\n",
        "\n",
        "        pc_hm[opt.pc_feat_channels['pc_dep'],\n",
        "               h_min:h_max+1, \n",
        "               w_min:w_max+1+1] = dist\n",
        "        pc_hm[opt.pc_feat_channels['pc_vx'],\n",
        "               h_min:h_max+1, \n",
        "               w_min:w_max+1+1] = vx\n",
        "        pc_hm[opt.pc_feat_channels['pc_vz'],\n",
        "               h_min:h_max+1, \n",
        "               w_min:w_max+1+1] = vz\n",
        "\n",
        "\n",
        "\n",
        "def pc_dep_to_hm(pc_hm, pc_dep, dep, bbox, dist_thresh, opt):\n",
        "    if isinstance(dep, list) and len(dep) > 0:\n",
        "      dep = dep[0]\n",
        "    ct = np.array(\n",
        "      [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)\n",
        "    bbox_int = np.array([np.floor(bbox[0]), \n",
        "                         np.floor(bbox[1]), \n",
        "                         np.ceil(bbox[2]), \n",
        "                         np.ceil(bbox[3])], np.int32)# format: xyxy\n",
        "\n",
        "    roi = pc_dep[:, bbox_int[1]:bbox_int[3]+1, bbox_int[0]:bbox_int[2]+1]\n",
        "    pc_dep = roi[opt.pc_feat_channels['pc_dep']]\n",
        "    pc_vx = roi[opt.pc_feat_channels['pc_vx']]\n",
        "    pc_vz = roi[opt.pc_feat_channels['pc_vz']]\n",
        "\n",
        "    nonzero_inds = np.nonzero(pc_dep)\n",
        "    \n",
        "    if len(nonzero_inds[0]) > 0:\n",
        "    #   nonzero_pc_dep = np.exp(-pc_dep[nonzero_inds])\n",
        "      nonzero_pc_dep = pc_dep[nonzero_inds]\n",
        "      nonzero_pc_vx = pc_vx[nonzero_inds]\n",
        "      nonzero_pc_vz = pc_vz[nonzero_inds]\n",
        "\n",
        "      ## Get points within dist threshold\n",
        "      within_thresh = (nonzero_pc_dep < dep+dist_thresh) \\\n",
        "              & (nonzero_pc_dep > max(0, dep-dist_thresh))\n",
        "      pc_dep_match = nonzero_pc_dep[within_thresh]\n",
        "      pc_vx_match = nonzero_pc_vx[within_thresh]\n",
        "      pc_vz_match = nonzero_pc_vz[within_thresh]\n",
        "\n",
        "      if len(pc_dep_match) > 0:\n",
        "        arg_min = np.argmin(pc_dep_match)\n",
        "        dist = pc_dep_match[arg_min]\n",
        "        vx = pc_vx_match[arg_min]\n",
        "        vz = pc_vz_match[arg_min]\n",
        "        if opt.normalize_depth:\n",
        "          dist /= opt.max_pc_dist\n",
        "\n",
        "        w = bbox[2] - bbox[0]\n",
        "        w_interval = opt.hm_to_box_ratio*(w)\n",
        "        w_min = int(ct[0] - w_interval/2.)\n",
        "        w_max = int(ct[0] + w_interval/2.)\n",
        "        \n",
        "        h = bbox[3] - bbox[1]\n",
        "        h_interval = opt.hm_to_box_ratio*(h)\n",
        "        h_min = int(ct[1] - h_interval/2.)\n",
        "        h_max = int(ct[1] + h_interval/2.)\n",
        "\n",
        "        pc_hm[opt.pc_feat_channels['pc_dep'],\n",
        "               h_min:h_max+1, \n",
        "               w_min:w_max+1+1] = dist\n",
        "        pc_hm[opt.pc_feat_channels['pc_vx'],\n",
        "               h_min:h_max+1, \n",
        "               w_min:w_max+1+1] = vx\n",
        "        pc_hm[opt.pc_feat_channels['pc_vz'],\n",
        "               h_min:h_max+1, \n",
        "               w_min:w_max+1+1] = vz\n",
        "    "
      ],
      "metadata": {
        "id": "HZvk7yva6AMm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ddd utils"
      ],
      "metadata": {
        "id": "vzakwm6C6h1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from scipy.spatial import ConvexHull\n",
        "from pyquaternion import Quaternion\n",
        "from nuscenes.utils.data_classes import Box\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def comput_corners_3d(dim, rotation_y):\n",
        "  # dim: 3\n",
        "  # location: 3\n",
        "  # rotation_y: 1\n",
        "  # return: 8 x 3\n",
        "  c, s = np.cos(rotation_y), np.sin(rotation_y)\n",
        "  R = np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]], dtype=np.float32)\n",
        "  l, w, h = dim[2], dim[1], dim[0]\n",
        "  x_corners = [l/2, l/2, -l/2, -l/2, l/2, l/2, -l/2, -l/2]\n",
        "  y_corners = [0,0,0,0,-h,-h,-h,-h]\n",
        "  z_corners = [w/2, -w/2, -w/2, w/2, w/2, -w/2, -w/2, w/2]\n",
        "\n",
        "  corners = np.array([x_corners, y_corners, z_corners], dtype=np.float32)\n",
        "  corners_3d = np.dot(R, corners).transpose(1, 0)\n",
        "  return corners_3d\n",
        "\n",
        "def compute_box_3d(dim, location, rotation_y):\n",
        "  # dim: 3\n",
        "  # location: 3\n",
        "  # rotation_y: 1\n",
        "  # return: 8 x 3\n",
        "  corners_3d = comput_corners_3d(dim, rotation_y)\n",
        "  corners_3d = corners_3d + np.array(location, dtype=np.float32).reshape(1, 3)\n",
        "  return corners_3d\n",
        "\n",
        "def project_to_image(pts_3d, P):\n",
        "  # pts_3d: n x 3\n",
        "  # P: 3 x 4\n",
        "  # return: n x 2\n",
        "  pts_3d_homo = np.concatenate(\n",
        "    [pts_3d, np.ones((pts_3d.shape[0], 1), dtype=np.float32)], axis=1)\n",
        "  pts_2d = np.dot(P, pts_3d_homo.transpose(1, 0)).transpose(1, 0)\n",
        "  pts_2d = pts_2d[:, :2] / pts_2d[:, 2:]\n",
        "  # import pdb; pdb.set_trace()\n",
        "  return pts_2d\n",
        "\n",
        "def compute_orientation_3d(dim, location, rotation_y):\n",
        "  # dim: 3\n",
        "  # location: 3\n",
        "  # rotation_y: 1\n",
        "  # return: 2 x 3\n",
        "  c, s = np.cos(rotation_y), np.sin(rotation_y)\n",
        "  R = np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]], dtype=np.float32)\n",
        "  orientation_3d = np.array([[0, dim[2]], [0, 0], [0, 0]], dtype=np.float32)\n",
        "  orientation_3d = np.dot(R, orientation_3d)\n",
        "  orientation_3d = orientation_3d + \\\n",
        "                   np.array(location, dtype=np.float32).reshape(3, 1)\n",
        "  return orientation_3d.transpose(1, 0)\n",
        "\n",
        "def draw_box_3d(image, corners, c=(255, 0, 255), same_color=False):\n",
        "  face_idx = [[0,1,5,4],\n",
        "              [1,2,6, 5],\n",
        "              [3,0,4,7],\n",
        "              [2,3,7,6]]\n",
        "  right_corners = [1, 2, 6, 5] if not same_color else []\n",
        "  left_corners = [0, 3, 7, 4] if not same_color else []\n",
        "  thickness = 4 if same_color else 2\n",
        "  corners = corners.astype(np.int32)\n",
        "  for ind_f in range(3, -1, -1):\n",
        "    f = face_idx[ind_f]\n",
        "    for j in range(4):\n",
        "      # print('corners', corners)\n",
        "      cc = c\n",
        "      if (f[j] in left_corners) and (f[(j+1)%4] in left_corners):\n",
        "        cc = (255, 0, 0)\n",
        "      if (f[j] in right_corners) and (f[(j+1)%4] in right_corners):\n",
        "        cc = (0, 0, 255)\n",
        "      try:\n",
        "        cv2.line(image, (corners[f[j], 0], corners[f[j], 1]),\n",
        "            (corners[f[(j+1)%4], 0], corners[f[(j+1)%4], 1]), cc, thickness, lineType=cv2.LINE_AA)\n",
        "      except:\n",
        "        pass\n",
        "    if ind_f == 0:\n",
        "      try:\n",
        "        cv2.line(image, (corners[f[0], 0], corners[f[0], 1]),\n",
        "                 (corners[f[2], 0], corners[f[2], 1]), c, 1, lineType=cv2.LINE_AA)\n",
        "        cv2.line(image, (corners[f[1], 0], corners[f[1], 1]),\n",
        "                 (corners[f[3], 0], corners[f[3], 1]), c, 1, lineType=cv2.LINE_AA)\n",
        "      except:\n",
        "        pass\n",
        "    # top_idx = [0, 1, 2, 3]\n",
        "  return image\n",
        "\n",
        "def unproject_2d_to_3d(pt_2d, depth, P):\n",
        "  # pts_2d: 2\n",
        "  # depth: 1\n",
        "  # P: 3 x 4\n",
        "  # return: 3\n",
        "  z = depth - P[2, 3]\n",
        "  x = (pt_2d[0] * depth - P[0, 3] - P[0, 2] * z) / P[0, 0]\n",
        "  y = (pt_2d[1] * depth - P[1, 3] - P[1, 2] * z) / P[1, 1]\n",
        "  pt_3d = np.array([x, y, z], dtype=np.float32).reshape(3)\n",
        "  return pt_3d\n",
        "\n",
        "def alpha2rot_y(alpha, x, cx, fx):\n",
        "    \"\"\"\n",
        "    Get rotation_y by alpha + theta - 180\n",
        "    alpha : Observation angle of object, ranging [-pi..pi]\n",
        "    x : Object center x to the camera center (x-W/2), in pixels\n",
        "    rotation_y : Rotation ry around Y-axis in camera coordinates [-pi..pi]\n",
        "    \"\"\"\n",
        "    rot_y = alpha + np.arctan2(x - cx, fx)\n",
        "    if rot_y > np.pi:\n",
        "      rot_y -= 2 * np.pi\n",
        "    if rot_y < -np.pi:\n",
        "      rot_y += 2 * np.pi\n",
        "    return rot_y\n",
        "\n",
        "def rot_y2alpha(rot_y, x, cx, fx):\n",
        "    \"\"\"\n",
        "    Get rotation_y by alpha + theta - 180\n",
        "    alpha : Observation angle of object, ranging [-pi..pi]\n",
        "    x : Object center x to the camera center (x-W/2), in pixels\n",
        "    rotation_y : Rotation ry around Y-axis in camera coordinates [-pi..pi]\n",
        "    \"\"\"\n",
        "    alpha = rot_y - np.arctan2(x - cx, fx)\n",
        "    if alpha > np.pi:\n",
        "      alpha -= 2 * np.pi\n",
        "    if alpha < -np.pi:\n",
        "      alpha += 2 * np.pi\n",
        "    return alpha\n",
        "\n",
        "\n",
        "def ddd2locrot(center, alpha, dim, depth, calib):\n",
        "  # single image\n",
        "  locations = unproject_2d_to_3d(center, depth, calib)\n",
        "  locations[1] += dim[0] / 2\n",
        "  rotation_y = alpha2rot_y(alpha, center[0], calib[0, 2], calib[0, 0])\n",
        "  return locations, rotation_y\n",
        "\n",
        "def project_3d_bbox(location, dim, rotation_y, calib):\n",
        "  box_3d = compute_box_3d(dim, location, rotation_y)\n",
        "  box_2d = project_to_image(box_3d, calib)\n",
        "  return box_2d\n",
        "\n",
        "#-----------------------------------------------------\n",
        "def box3d_vol(corners):\n",
        "    ''' corners: (8,3) no assumption on axis direction '''\n",
        "    a = np.sqrt(np.sum((corners[0,:] - corners[1,:])**2))\n",
        "    b = np.sqrt(np.sum((corners[1,:] - corners[2,:])**2))\n",
        "    c = np.sqrt(np.sum((corners[0,:] - corners[4,:])**2))\n",
        "    return a*b*c\n",
        "\n",
        "def poly_area(x,y):\n",
        "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
        "\n",
        "def polygon_clip(subjectPolygon, clipPolygon):\n",
        "   \"\"\" Clip a polygon with another polygon.\n",
        "   Args:\n",
        "     subjectPolygon: a list of (x,y) 2d points, any polygon.\n",
        "     clipPolygon: a list of (x,y) 2d points, has to be *convex*\n",
        "   Note:\n",
        "     **points have to be counter-clockwise ordered**\n",
        "   Return:\n",
        "     a list of (x,y) vertex point for the intersection polygon.\n",
        "   \"\"\"\n",
        "   def inside(p):\n",
        "      return(cp2[0]-cp1[0])*(p[1]-cp1[1]) > (cp2[1]-cp1[1])*(p[0]-cp1[0])\n",
        " \n",
        "   def computeIntersection():\n",
        "      dc = [ cp1[0] - cp2[0], cp1[1] - cp2[1] ]\n",
        "      dp = [ s[0] - e[0], s[1] - e[1] ]\n",
        "      n1 = cp1[0] * cp2[1] - cp1[1] * cp2[0]\n",
        "      n2 = s[0] * e[1] - s[1] * e[0] \n",
        "      n3 = 1.0 / (dc[0] * dp[1] - dc[1] * dp[0])\n",
        "      return [(n1*dp[0] - n2*dc[0]) * n3, (n1*dp[1] - n2*dc[1]) * n3]\n",
        " \n",
        "   outputList = subjectPolygon\n",
        "   cp1 = clipPolygon[-1]\n",
        " \n",
        "   for clipVertex in clipPolygon:\n",
        "      cp2 = clipVertex\n",
        "      inputList = outputList\n",
        "      outputList = []\n",
        "      s = inputList[-1]\n",
        " \n",
        "      for subjectVertex in inputList:\n",
        "         e = subjectVertex\n",
        "         if inside(e):\n",
        "            if not inside(s):\n",
        "               outputList.append(computeIntersection())\n",
        "            outputList.append(e)\n",
        "         elif inside(s):\n",
        "            outputList.append(computeIntersection())\n",
        "         s = e\n",
        "      cp1 = cp2\n",
        "      if len(outputList) == 0:\n",
        "          return None\n",
        "   return(outputList)\n",
        "\n",
        "def convex_hull_intersection(p1, p2):\n",
        "    \"\"\" Compute area of two convex hull's intersection area.\n",
        "        p1,p2 are a list of (x,y) tuples of hull vertices.\n",
        "        return a list of (x,y) for the intersection and its volume\n",
        "    \"\"\"\n",
        "    inter_p = polygon_clip(p1,p2)\n",
        "    if inter_p is not None:\n",
        "        hull_inter = ConvexHull(inter_p)\n",
        "        return inter_p, hull_inter.volume\n",
        "    else:\n",
        "        return None, 0.0\n",
        "\n",
        "def iou3d(corners1, corners2):\n",
        "  ''' Compute 3D bounding box IoU.\n",
        "  Input:\n",
        "      corners1: numpy array (8,3), assume up direction is negative Y\n",
        "      corners2: numpy array (8,3), assume up direction is negative Y\n",
        "  Output:\n",
        "      iou: 3D bounding box IoU\n",
        "      iou_2d: bird's eye view 2D bounding box IoU\n",
        "  '''\n",
        "  # corner points are in counter clockwise order\n",
        "  rect1 = [(corners1[i,0], corners1[i,2]) for i in range(3,-1,-1)]\n",
        "  rect2 = [(corners2[i,0], corners2[i,2]) for i in range(3,-1,-1)] \n",
        "  area1 = poly_area(np.array(rect1)[:,0], np.array(rect1)[:,1])\n",
        "  area2 = poly_area(np.array(rect2)[:,0], np.array(rect2)[:,1])\n",
        "  inter, inter_area = convex_hull_intersection(rect1, rect2)\n",
        "  iou_2d = inter_area/(area1+area2-inter_area)\n",
        "  ymax = min(corners1[0,1], corners2[0,1])\n",
        "  ymin = max(corners1[4,1], corners2[4,1])\n",
        "  inter_vol = inter_area * max(0.0, ymax-ymin)\n",
        "  vol1 = box3d_vol(corners1)\n",
        "  vol2 = box3d_vol(corners2)\n",
        "  iou = inter_vol / (vol1 + vol2 - inter_vol)\n",
        "  return iou, iou_2d\n",
        "\n",
        "\n",
        "def iou3d_global(corners1, corners2):\n",
        "  ''' Compute 3D bounding box IoU.\n",
        "  Input:\n",
        "      corners1: numpy array (8,3), assume up direction is negative Y\n",
        "      corners2: numpy array (8,3), assume up direction is negative Y\n",
        "  Output:\n",
        "      iou: 3D bounding box IoU\n",
        "      iou_2d: bird's eye view 2D bounding box IoU\n",
        "  '''\n",
        "  # corner points are in counter clockwise order\n",
        "  rect1 = corners1[:,[0,3,7,4]].T\n",
        "  rect2 = corners2[:,[0,3,7,4]].T\n",
        "  \n",
        "  rect1 = [(rect1[i,0], rect1[i,1]) for i in range(3,-1,-1)]\n",
        "  rect2 = [(rect2[i,0], rect2[i,1]) for i in range(3,-1,-1)]\n",
        "  \n",
        "  area1 = poly_area(np.array(rect1)[:,0], np.array(rect1)[:,1])\n",
        "  area2 = poly_area(np.array(rect2)[:,0], np.array(rect2)[:,1])\n",
        "  inter, inter_area = convex_hull_intersection(rect1, rect2)\n",
        "\n",
        "  iou_2d = inter_area/(area1+area2-inter_area)\n",
        "  \n",
        "  iou = 0\n",
        "  # ymax = min(corners1[0,2], corners2[0,2])\n",
        "  # ymin = max(corners1[1,2], corners2[1,2])\n",
        "  # inter_vol = inter_area * max(0.0, ymax-ymin)\n",
        "  # vol1 = box3d_vol(corners1)\n",
        "  # vol2 = box3d_vol(corners2)\n",
        "  # iou = inter_vol / (vol1 + vol2 - inter_vol)\n",
        "  return iou, iou_2d\n",
        "\n",
        "\n",
        "def get_pc_hm(pc_hm, pc_dep, dep, bbox, dist_thresh, opt):\n",
        "  if len(dep) > 0:\n",
        "    dep = dep[0]\n",
        "  ct = np.array(\n",
        "    [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)\n",
        "  bbox_int = np.array([np.floor(bbox[0]), \n",
        "                        np.floor(bbox[1]), \n",
        "                        np.ceil(bbox[2]), \n",
        "                        np.ceil(bbox[3])], np.int32)# format: xyxy\n",
        "\n",
        "  roi = pc_dep[:, bbox_int[1]:bbox_int[3]+1, bbox_int[0]:bbox_int[2]+1]\n",
        "  pc_dep = roi[opt.pc_feat_channels['pc_dep']]\n",
        "  pc_vx = roi[opt.pc_feat_channels['pc_vx']]\n",
        "  pc_vz = roi[opt.pc_feat_channels['pc_vz']]\n",
        "\n",
        "  nonzero_inds = np.nonzero(pc_dep)\n",
        "  \n",
        "  if len(nonzero_inds[0]) > 0:\n",
        "    nonzero_pc_dep = np.exp(-pc_dep[nonzero_inds])\n",
        "    nonzero_pc_vx = pc_vx[nonzero_inds]\n",
        "    nonzero_pc_vz = pc_vz[nonzero_inds]\n",
        "\n",
        "    ## Get points within dist threshold\n",
        "    within_thresh = (nonzero_pc_dep < dep+dist_thresh) \\\n",
        "            & (nonzero_pc_dep > max(0, dep-dist_thresh))\n",
        "    pc_dep_match = nonzero_pc_dep[within_thresh]\n",
        "    pc_vx_match = nonzero_pc_vx[within_thresh]\n",
        "    pc_vz_match = nonzero_pc_vz[within_thresh]\n",
        "\n",
        "    if len(pc_dep_match) > 0:\n",
        "      arg_min = np.argmin(pc_dep_match)\n",
        "      dist = pc_dep_match[arg_min]\n",
        "      vx = pc_vx_match[arg_min]\n",
        "      vz = pc_vz_match[arg_min]\n",
        "\n",
        "      w = bbox[2] - bbox[0]\n",
        "      w_interval = opt.hm_to_box_ratio*(w)\n",
        "      w_min = int(ct[0] - w_interval/2.)\n",
        "      w_max = int(ct[0] + w_interval/2.)\n",
        "      \n",
        "      h = bbox[3] - bbox[1]\n",
        "      h_interval = opt.hm_to_box_ratio*(h)\n",
        "      h_min = int(ct[1] - h_interval/2.)\n",
        "      h_max = int(ct[1] + h_interval/2.)\n",
        "\n",
        "      pc_hm[opt.pc_feat_channels['pc_dep'],\n",
        "              h_min:h_max+1, \n",
        "              w_min:w_max+1+1] = dist\n",
        "      pc_hm[opt.pc_feat_channels['pc_vx'],\n",
        "              h_min:h_max+1, \n",
        "              w_min:w_max+1+1] = vx\n",
        "      pc_hm[opt.pc_feat_channels['pc_vz'],\n",
        "              h_min:h_max+1, \n",
        "              w_min:w_max+1+1] = vz\n"
      ],
      "metadata": {
        "id": "cXD3k5TX6qE1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Debugger"
      ],
      "metadata": {
        "id": "jR82R16YKJKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mpl_toolkits.mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from .ddd_utils import compute_box_3d, project_to_image, draw_box_3d\n",
        "\n",
        "\n",
        "class Debugger(object):\n",
        "  def __init__(self, opt, dataset):\n",
        "    self.opt = opt\n",
        "    self.imgs = {}\n",
        "    self.theme = opt.debugger_theme\n",
        "    self.plt = plt\n",
        "    self.with_3d = False\n",
        "    self.names = dataset.class_name\n",
        "    # self.out_size = 384 if opt.dataset == 'kitti' else 512\n",
        "    self.out_size = 384 if opt.dataset == 'kitti' else 1024\n",
        "    self.cnt = 0\n",
        "    colors = [(color_list[i]).astype(np.uint8) for i in range(len(color_list))]\n",
        "    while len(colors) < len(self.names):\n",
        "      colors = colors + colors[:min(len(colors), len(self.names) - len(colors))]\n",
        "    self.colors = np.array(colors, dtype=np.uint8).reshape(len(colors), 1, 1, 3)\n",
        "    if self.theme == 'white':\n",
        "      self.colors = self.colors.reshape(-1)[::-1].reshape(len(colors), 1, 1, 3)\n",
        "      self.colors = np.clip(self.colors, 0., 0.6 * 255).astype(np.uint8)\n",
        "  \n",
        "    self.num_joints = 17\n",
        "    self.edges = [[0, 1], [0, 2], [1, 3], [2, 4], \n",
        "                  [3, 5], [4, 6], [5, 6], \n",
        "                  [5, 7], [7, 9], [6, 8], [8, 10], \n",
        "                  [5, 11], [6, 12], [11, 12], \n",
        "                  [11, 13], [13, 15], [12, 14], [14, 16]]\n",
        "    self.ec = [(255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255), \n",
        "                (255, 0, 0), (0, 0, 255), (255, 0, 255),\n",
        "                (255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255),\n",
        "                (255, 0, 0), (0, 0, 255), (255, 0, 255),\n",
        "                (255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255)]\n",
        "    self.colors_hp = [(128, 0, 128), (128, 0, 0), (0, 0, 128), \n",
        "      (128, 0, 0), (0, 0, 128), (128, 0, 0), (0, 0, 128),\n",
        "      (128, 0, 0), (0, 0, 128), (128, 0, 0), (0, 0, 128),\n",
        "      (128, 0, 0), (0, 0, 128), (128, 0, 0), (0, 0, 128),\n",
        "      (128, 0, 0), (0, 0, 128)]\n",
        "    self.track_color = {}\n",
        "    # print('names', self.names)\n",
        "    self.down_ratio=opt.down_ratio\n",
        "    # for bird view\n",
        "    self.world_size = 64\n",
        "\n",
        "\n",
        "  def add_img(self, img, img_id='default', revert_color=False):\n",
        "    if revert_color:\n",
        "      img = 255 - img\n",
        "    self.imgs[img_id] = img.copy()\n",
        "  \n",
        "  def add_mask(self, mask, bg, imgId = 'default', trans = 0.8):\n",
        "    self.imgs[imgId] = (mask.reshape(\n",
        "      mask.shape[0], mask.shape[1], 1) * 255 * trans + \\\n",
        "      bg * (1 - trans)).astype(np.uint8)\n",
        "  \n",
        "  def show_img(self, pause = False, imgId = 'default'):\n",
        "    cv2.imshow('{}'.format(imgId), self.imgs[imgId])\n",
        "    if pause:\n",
        "      cv2.waitKey()\n",
        "  \n",
        "  def add_blend_img(self, back, fore, img_id='blend', trans=0.7):\n",
        "    if self.theme == 'white':\n",
        "      fore = 255 - fore\n",
        "    if fore.shape[0] != back.shape[0] or fore.shape[0] != back.shape[1]:\n",
        "      fore = cv2.resize(fore, (back.shape[1], back.shape[0]))\n",
        "    if len(fore.shape) == 2:\n",
        "      fore = fore.reshape(fore.shape[0], fore.shape[1], 1)\n",
        "    self.imgs[img_id] = (back * (1. - trans) + fore * trans)\n",
        "    self.imgs[img_id][self.imgs[img_id] > 255] = 255\n",
        "    self.imgs[img_id][self.imgs[img_id] < 0] = 0\n",
        "    self.imgs[img_id] = self.imgs[img_id].astype(np.uint8).copy()\n",
        "  \n",
        "\n",
        "  def add_overlay_img(self, img, fore, img_id=\"overlay\", alpha=0.1):\n",
        "\n",
        "    self.imgs[img_id] = img.copy()\n",
        "    fore = fore.copy()\n",
        "    c, h, w = fore.shape[0], fore.shape[1], fore.shape[2]\n",
        "    output_res = (h * self.down_ratio, w * self.down_ratio)\n",
        "\n",
        "    # Create mask\n",
        "    mask = fore.transpose(1, 2, 0)\n",
        "    mask = cv2.resize(mask, (output_res[1], output_res[0]), fx=0, fy=0, interpolation = cv2.INTER_NEAREST)\n",
        "    mask = (mask == 0).astype(np.uint8)*255\n",
        "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
        "    # mask = cv2.bitwise_and(mask, self.imgs[img_id])\n",
        "    # self.imgs[img_id] = mask\n",
        "    # return\n",
        "\n",
        "    # Create colormap\n",
        "    fore = fore.transpose(1, 2, 0).reshape(h, w, c, 1).astype(np.float32)\n",
        "    colors = np.array([0,255,0], dtype=np.float32).reshape(-1, 3)[:c].reshape(1, 1, c, 3)\n",
        "    color_map = (fore * colors).max(axis=2).astype(np.uint8)\n",
        "    color_map = cv2.resize(color_map, (output_res[1], output_res[0]), fx=0, fy=0, interpolation = cv2.INTER_NEAREST)\n",
        "    \n",
        "    if color_map.shape[0] != img.shape[0] or color_map.shape[0] != img.shape[1]:\n",
        "      color_map = cv2.resize(color_map, (img.shape[1], img.shape[0]))\n",
        "    if len(fore.shape) == 2:\n",
        "      color_map = color_map.reshape(color_map.shape[0], color_map.shape[1], 1)\n",
        "    \n",
        "    ## Overlay colormap on image\n",
        "    back = cv2.bitwise_and(mask, self.imgs[img_id])\n",
        "    self.imgs[img_id] = cv2.add(back, color_map)\n",
        "\n",
        "\n",
        "  def gen_colormap(self, img, output_res=None):\n",
        "    img = img.copy()\n",
        "    # ignore region\n",
        "    img[img == 1] = 0.5\n",
        "    c, h, w = img.shape[0], img.shape[1], img.shape[2]\n",
        "    if output_res is None:\n",
        "      output_res = (h * self.down_ratio, w * self.down_ratio)\n",
        "    img = img.transpose(1, 2, 0).reshape(h, w, c, 1).astype(np.float32)\n",
        "    colors = np.array(\n",
        "      self.colors, dtype=np.float32).reshape(-1, 3)[:c].reshape(1, 1, c, 3)\n",
        "    if self.theme == 'white':\n",
        "      colors = 255 - colors\n",
        "    if self.opt.tango_color:\n",
        "      colors = tango_color_dark[:c].reshape(1, 1, c, 3)\n",
        "    color_map = (img * colors).max(axis=2).astype(np.uint8)\n",
        "    color_map = cv2.resize(color_map, (output_res[1], output_res[0]))\n",
        "    return color_map\n",
        "    \n",
        "  def gen_colormap_hp(self, img, output_res=None):\n",
        "    img = img.copy()\n",
        "    img[img == 1] = 0.5 \n",
        "    c, h, w = img.shape[0], img.shape[1], img.shape[2]\n",
        "    if output_res is None:\n",
        "      output_res = (h * self.down_ratio, w * self.down_ratio)\n",
        "    img = img.transpose(1, 2, 0).reshape(h, w, c, 1).astype(np.float32)\n",
        "    colors = np.array(\n",
        "      self.colors_hp, dtype=np.float32).reshape(-1, 3)[:c].reshape(1, 1, c, 3)\n",
        "    if self.theme == 'white':\n",
        "      colors = 255 - colors\n",
        "    color_map = (img * colors).max(axis=2).astype(np.uint8)\n",
        "    color_map = cv2.resize(color_map, (output_res[0], output_res[1]))\n",
        "    return color_map\n",
        "  \n",
        "  def gen_pointcloud(self, pc, pc_N, img_shape):\n",
        "    h, w, c = img_shape\n",
        "    output_res = (h, w, c)\n",
        "    \n",
        "    if self.theme == 'white':\n",
        "      img = 255 * np.ones(shape=output_res, dtype=np.uint8)\n",
        "    else:\n",
        "      img = np.zeros(shape=output_res, dtype=np.uint8)\n",
        "    for i in range(pc_N):\n",
        "      p = pc[:3,i]\n",
        "      img = cv2.circle(img, (int(p[0]), int(p[1])), 6, (0,0,255), -1)\n",
        "    return img\n",
        "\n",
        "  def add_pointcloud(self, pc, pc_N, img_id='pc'):\n",
        "    self.imgs[img_id] = cv2.resize(self.imgs[img_id], (1600, 900))\n",
        "    for i in range(pc_N):\n",
        "      p = pc[:3,i]\n",
        "      c = int((p[2].tolist()/60.0)*255)\n",
        "      c = (0,c,0)\n",
        "      cv2.circle(self.imgs[img_id], (int(p[0]), int(p[1])), 6, c, -1)\n",
        "\n",
        "  def _get_rand_color(self):\n",
        "    c = ((np.random.random((3)) * 0.6 + 0.2) * 255).astype(np.int32).tolist()\n",
        "    return c\n",
        "\n",
        "  def add_coco_bbox(self, bbox, cat, conf=1, show_txt=True, \n",
        "    no_bbox=False, img_id='default', dist=-1): \n",
        "    bbox = np.array(bbox, dtype=np.int32)\n",
        "    dist = ', {:.1f}m'.format(int(dist)) if dist >= 0 else ''\n",
        "    cat = int(cat)\n",
        "    c = self.colors[cat][0][0].tolist()\n",
        "    if self.theme == 'white':\n",
        "      c = (255 - np.array(c)).tolist()\n",
        "    if self.opt.tango_color:\n",
        "      c = (255 - tango_color_dark[cat][0][0]).tolist()\n",
        "    if conf >= 1:\n",
        "      ID = int(conf) if not self.opt.not_show_number else ''\n",
        "      txt = '{}{}{}'.format(self.names[cat], ID, dist)\n",
        "    else:\n",
        "      txt = '{}{:.1f}{}'.format(self.names[cat], conf, dist)\n",
        "    thickness = 2\n",
        "    fontsize = 0.8 if self.opt.qualitative else 0.5\n",
        "    if self.opt.show_track_color:\n",
        "      track_id = int(conf)\n",
        "      if not (track_id in self.track_color):\n",
        "        self.track_color[track_id] = self._get_rand_color()\n",
        "      c = self.track_color[track_id]\n",
        "      # thickness = 4\n",
        "      # fontsize = 0.8\n",
        "    if not self.opt.not_show_bbox:\n",
        "      font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "      cat_size = cv2.getTextSize(txt, font, fontsize, thickness)[0]\n",
        "      if not no_bbox:\n",
        "        cv2.rectangle(\n",
        "          self.imgs[img_id], (bbox[0], bbox[1]), (bbox[2], bbox[3]), \n",
        "          c, thickness)\n",
        "        \n",
        "      if show_txt:\n",
        "        cv2.rectangle(self.imgs[img_id],\n",
        "                      (bbox[0], bbox[1] - cat_size[1] - thickness),\n",
        "                      (bbox[0] + cat_size[0], bbox[1]), c, -1)\n",
        "        cv2.putText(self.imgs[img_id], txt, (bbox[0], bbox[1] - thickness - 1), \n",
        "                    font, fontsize, (0, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n",
        "\n",
        "  def add_tracking_id(self, ct, tracking_id, img_id='default'):\n",
        "    txt = '{}'.format(tracking_id)\n",
        "    fontsize = 0.5\n",
        "    cv2.putText(self.imgs[img_id], txt, (int(ct[0]), int(ct[1])), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, fontsize, \n",
        "                (255, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
        "\n",
        "\n",
        "  def add_coco_hp(self, points, tracking_id=0, img_id='default'): \n",
        "    points = np.array(points, dtype=np.int32).reshape(self.num_joints, 2)\n",
        "    if not self.opt.show_track_color:\n",
        "      for j in range(self.num_joints):\n",
        "        cv2.circle(self.imgs[img_id],\n",
        "                  (points[j, 0], points[j, 1]), 3, self.colors_hp[j], -1)\n",
        "\n",
        "    h, w = self.imgs[img_id].shape[0], self.imgs[img_id].shape[1]\n",
        "    for j, e in enumerate(self.edges):\n",
        "      if points[e].min() > 0 and points[e, 0].max() < w and \\\n",
        "        points[e, 1].max() < h:\n",
        "        c = self.ec[j] if not self.opt.show_track_color else \\\n",
        "          self.track_color[tracking_id]\n",
        "        cv2.line(self.imgs[img_id], (points[e[0], 0], points[e[0], 1]),\n",
        "                      (points[e[1], 0], points[e[1], 1]), c, 2,\n",
        "                      lineType=cv2.LINE_AA)\n",
        "\n",
        "  def clear(self):\n",
        "    return\n",
        "\n",
        "  def show_all_imgs(self, pause=False, Time=0):\n",
        "    if 1:\n",
        "      for i, v in self.imgs.items():\n",
        "        cv2.imshow('{}'.format(i), v)\n",
        "      if not self.with_3d:\n",
        "        cv2.waitKey(0 if pause else 1)\n",
        "      else:\n",
        "        max_range = np.array([\n",
        "          self.xmax-self.xmin, self.ymax-self.ymin, self.zmax-self.zmin]).max()\n",
        "        Xb = 0.5*max_range*np.mgrid[\n",
        "          -1:2:2,-1:2:2,-1:2:2][0].flatten() + 0.5*(self.xmax+self.xmin)\n",
        "        Yb = 0.5*max_range*np.mgrid[\n",
        "          -1:2:2,-1:2:2,-1:2:2][1].flatten() + 0.5*(self.ymax+self.ymin)\n",
        "        Zb = 0.5*max_range*np.mgrid[\n",
        "          -1:2:2,-1:2:2,-1:2:2][2].flatten() + 0.5*(self.zmax+self.zmin)\n",
        "        for xb, yb, zb in zip(Xb, Yb, Zb):\n",
        "          self.ax.plot([xb], [yb], [zb], 'w')\n",
        "        if self.opt.debug == 9:\n",
        "          self.plt.pause(1e-27)\n",
        "        else:\n",
        "          self.plt.show()\n",
        "    else:\n",
        "      self.ax = None\n",
        "      nImgs = len(self.imgs)\n",
        "      fig=plt.figure(figsize=(nImgs * 10,10))\n",
        "      nCols = nImgs\n",
        "      nRows = nImgs // nCols\n",
        "      for i, (k, v) in enumerate(self.imgs.items()):\n",
        "        fig.add_subplot(1, nImgs, i + 1)\n",
        "        if len(v.shape) == 3:\n",
        "          plt.imshow(cv2.cvtColor(v, cv2.COLOR_BGR2RGB))\n",
        "        else:\n",
        "          plt.imshow(v)\n",
        "      plt.show()\n",
        "\n",
        "  def save_img(self, imgId='default', path='./cache/debug/'):\n",
        "    cv2.imwrite(path + '{}.png'.format(imgId), self.imgs[imgId])\n",
        "    \n",
        "  def save_all_imgs(self, path='./cache/debug/', prefix='', genID=False):\n",
        "    if genID:\n",
        "      try:\n",
        "        idx = int(np.loadtxt(path + '/id.txt'))\n",
        "      except:\n",
        "        idx = 0\n",
        "      prefix=idx\n",
        "      np.savetxt(path + '/id.txt', np.ones(1) * (idx + 1), fmt='%d')\n",
        "    for i, v in self.imgs.items():\n",
        "      if i in self.opt.save_imgs or self.opt.save_imgs == []:\n",
        "        cv2.imwrite(\n",
        "          path + '/{}{}{}.png'.format(prefix, i, self.opt.save_img_suffix), v)\n",
        "\n",
        "\n",
        "  def save_all_imgs_plt(self, path='./cache/debug/', prefix='', genID=False):\n",
        "    if genID:\n",
        "      try:\n",
        "        idx = int(np.loadtxt(path + '/id.txt'))\n",
        "      except:\n",
        "        idx = 0\n",
        "      prefix=idx\n",
        "      np.savetxt(path + '/id.txt', np.ones(1) * (idx + 1), fmt='%d')\n",
        "    for i, v in self.imgs.items():\n",
        "      if i in self.opt.save_imgs or self.opt.save_imgs == []:\n",
        "        plt.imsave(path + '/{}{}{}.pdf'.format(prefix, i, self.opt.save_img_suffix),\n",
        "                  cv2.cvtColor(v, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  def remove_side(self, img_id, img):\n",
        "    if not (img_id in self.imgs):\n",
        "      return\n",
        "    ws = img.sum(axis=2).sum(axis=0)\n",
        "    l = 0\n",
        "    while ws[l] == 0 and l < len(ws):\n",
        "      l+= 1\n",
        "    r = ws.shape[0] - 1\n",
        "    while ws[r] == 0 and r > 0:\n",
        "      r -= 1\n",
        "    hs = img.sum(axis=2).sum(axis=1)\n",
        "    t = 0\n",
        "    while hs[t] == 0 and t < len(hs):\n",
        "      t += 1\n",
        "    b = hs.shape[0] - 1\n",
        "    while hs[b] == 0 and b > 0:\n",
        "      b -= 1\n",
        "    self.imgs[img_id] = self.imgs[img_id][t:b+1, l:r+1].copy()\n",
        "\n",
        "  def project_3d_to_bird(self, pt):\n",
        "    pt[0] += self.world_size / 2\n",
        "    pt[1] = self.world_size - pt[1]\n",
        "    pt = pt * self.out_size / self.world_size\n",
        "    return pt.astype(np.int32)\n",
        "\n",
        "  def add_3d_detection(\n",
        "    self, image_or_path, flipped, dets, calib, show_txt=False, \n",
        "    vis_thresh=0.3, img_id='det'):\n",
        "    if isinstance(image_or_path, np.ndarray):\n",
        "      self.imgs[img_id] = image_or_path.copy()\n",
        "    else: \n",
        "      self.imgs[img_id] = cv2.imread(image_or_path)\n",
        "    # thickness = 1\n",
        "    if self.opt.show_track_color:\n",
        "      # self.imgs[img_id] = (self.imgs[img_id] * 0.5 + \\\n",
        "      #   np.ones_like(self.imgs[img_id]) * 255 * 0.5).astype(np.uint8)\n",
        "        # thickness = 3\n",
        "      pass\n",
        "    if flipped:\n",
        "      self.imgs[img_id] = self.imgs[img_id][:, ::-1].copy()\n",
        "    for item in dets:\n",
        "      if item['score'] > vis_thresh \\\n",
        "        and 'dim' in item and 'loc' in item and 'rot_y' in item:\n",
        "        cl = (self.colors[int(item['class']) - 1, 0, 0]).tolist() \\\n",
        "          if not self.opt.show_track_color else \\\n",
        "          self.track_color[int(item['tracking_id'])]\n",
        "        if self.theme == 'white' and not self.opt.show_track_color:\n",
        "          cl = (255 - np.array(cl)).tolist()\n",
        "        if self.opt.tango_color:\n",
        "          cl = (255 - tango_color_dark[int(item['class']) - 1, 0, 0]).tolist()\n",
        "        dim = item['dim']\n",
        "        loc = item['loc']\n",
        "        rot_y = item['rot_y']\n",
        "        if loc[2] > 1:\n",
        "          box_3d = compute_box_3d(dim, loc, rot_y)\n",
        "          box_2d = project_to_image(box_3d, calib)\n",
        "          self.imgs[img_id] = draw_box_3d(\n",
        "            self.imgs[img_id], box_2d.astype(np.int32), cl, \n",
        "            same_color=self.opt.show_track_color or self.opt.qualitative)\n",
        "          if self.opt.show_track_color or self.opt.qualitative:\n",
        "            bbox = [box_2d[:,0].min(), box_2d[:,1].min(),\n",
        "                    box_2d[:,0].max(), box_2d[:,1].max()]\n",
        "            sc = int(item['tracking_id']) if self.opt.show_track_color else \\\n",
        "              item['score']\n",
        "            self.add_coco_bbox(\n",
        "              bbox, item['class'] - 1, sc, no_bbox=True, img_id=img_id)\n",
        "          if self.opt.show_track_color:\n",
        "            self.add_arrow([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], \n",
        "              item['tracking'], img_id=img_id)\n",
        "\n",
        "    # print('===========================')\n",
        "  def compose_vis_ddd(\n",
        "    self, img_path, flipped, dets, calib,\n",
        "    vis_thresh, pred, bev, img_id='out'):\n",
        "    self.imgs[img_id] = cv2.imread(img_path)\n",
        "    if flipped:\n",
        "      self.imgs[img_id] = self.imgs[img_id][:, ::-1].copy()\n",
        "    h, w = pred.shape[:2]\n",
        "    hs, ws = self.imgs[img_id].shape[0] / h, self.imgs[img_id].shape[1] / w\n",
        "    self.imgs[img_id] = cv2.resize(self.imgs[img_id], (w, h))\n",
        "    self.add_blend_img(self.imgs[img_id], pred, img_id, trans=self.opt.hm_transparency)\n",
        "    for item in dets:\n",
        "      if item['score'] > vis_thresh:\n",
        "        dim = item['dim']\n",
        "        loc = item['loc']\n",
        "        rot_y = item['rot_y']\n",
        "        cl = (self.colors[int(item['class']) - 1, 0, 0]).tolist()\n",
        "        if loc[2] > 1:\n",
        "          box_3d = compute_box_3d(dim, loc, rot_y)\n",
        "          box_2d = project_to_image(box_3d, calib)\n",
        "          box_2d[:, 0] /= hs\n",
        "          box_2d[:, 1] /= ws\n",
        "          self.imgs[img_id] = draw_box_3d(self.imgs[img_id], box_2d, cl)\n",
        "\n",
        "    self.imgs[img_id] = np.concatenate(\n",
        "      [self.imgs[img_id], self.imgs[bev]], axis=1)\n",
        "\n",
        "  def add_bird_view(self, dets, vis_thresh=0.3, img_id='bird', cnt=0):\n",
        "    if self.opt.vis_gt_bev:\n",
        "      bird_view = cv2.imread(\n",
        "        self.opt.vis_gt_bev + '/{}bird_pred_gt.png'.format(cnt))\n",
        "    else:\n",
        "      bird_view = np.ones((self.out_size, self.out_size, 3), dtype=np.uint8) * 230\n",
        "    for item in dets:\n",
        "      cl = (self.colors[int(item['class']) - 1, 0, 0]).tolist()\n",
        "      lc = (250, 152, 12)\n",
        "      if item['score'] > vis_thresh:\n",
        "        dim = item['dim']\n",
        "        loc = item['loc']\n",
        "        rot_y = item['rot_y']\n",
        "        rect = compute_box_3d(dim, loc, rot_y)[:4, [0, 2]]\n",
        "        for k in range(4):\n",
        "          rect[k] = self.project_3d_to_bird(rect[k])\n",
        "        cv2.polylines(\n",
        "            bird_view,[rect.reshape(-1, 1, 2).astype(np.int32)],\n",
        "            True,lc,2,lineType=cv2.LINE_AA)\n",
        "        for e in [[0, 1]]:\n",
        "          t = 4 if e == [0, 1] else 1\n",
        "          cv2.line(bird_view, (rect[e[0]][0], rect[e[0]][1]),\n",
        "                  (rect[e[1]][0], rect[e[1]][1]), lc, t,\n",
        "                  lineType=cv2.LINE_AA)\n",
        "\n",
        "    self.imgs[img_id] = bird_view\n",
        "\n",
        "  def add_bird_views(self, dets_dt, dets_gt, vis_thresh=0.3, img_id='bird', \n",
        "                     pc_3d=None, draw_ego=True, show_velocity=False):\n",
        "    bird_view = np.ones((self.out_size, self.out_size, 3), dtype=np.uint8) * 230\n",
        "    for ii, (dets, lc, cc) in enumerate(\n",
        "      [(dets_gt, (59, 67, 235), (10, 20, 180)), \n",
        "       (dets_dt, (250, 152, 12), (255, 0, 0))]):\n",
        "      for item in dets:\n",
        "        if item['score'] > vis_thresh \\\n",
        "          and 'dim' in item and 'loc' in item and 'rot_y' in item:\n",
        "          dim = item['dim']\n",
        "          loc = item['loc']\n",
        "          rot_y = item['rot_y']\n",
        "          rect = compute_box_3d(dim, loc, rot_y)[:4, [0, 2]]\n",
        "          for k in range(4):\n",
        "            rect[k] = self.project_3d_to_bird(rect[k])\n",
        "          if ii == 0:\n",
        "            cv2.fillPoly(\n",
        "              bird_view,[rect.reshape(-1, 1, 2).astype(np.int32)],\n",
        "              lc,lineType=cv2.LINE_AA)\n",
        "          else:\n",
        "            cv2.polylines(\n",
        "              bird_view,[rect.reshape(-1, 1, 2).astype(np.int32)],\n",
        "              True,lc,2,lineType=cv2.LINE_AA)\n",
        "          # for e in [[0, 1], [1, 2], [2, 3], [3, 0]]:\n",
        "          for e in [[0, 1]]:\n",
        "            t = 4 if e == [0, 1] else 1\n",
        "            cv2.line(bird_view, (rect[e[0]][0], rect[e[0]][1]),\n",
        "                    (rect[e[1]][0], rect[e[1]][1]), lc, t,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "          if show_velocity:\n",
        "            str_pt = (int(rect[e[0]][0]/2. + rect[e[1]][0]/2.), int(rect[e[0]][1]/2. + rect[e[1]][1]/2.))\n",
        "            end_pt = (int(str_pt[0] + 3*item['velocity'][0]), int(str_pt[1] - 3*item['velocity'][2]))\n",
        "            cv2.arrowedLine(bird_view, str_pt, end_pt, cc, \n",
        "              thickness=2, line_type=cv2.LINE_AA, tipLength=0.3)\n",
        "\n",
        "    if pc_3d is not None:\n",
        "      for p_3d in pc_3d.squeeze(0).T:\n",
        "        if (p_3d == 0).all():\n",
        "          break\n",
        "        p_bev = self.project_3d_to_bird(p_3d[[0, 2]])\n",
        "        bird_view = cv2.circle(bird_view, (int(p_bev[0]), int(p_bev[1])), 3, (0,120,0), -1)\n",
        "    if draw_ego:\n",
        "      p_ego = self.project_3d_to_bird(np.array([0.0, 0.0]))\n",
        "      bird_view = cv2.circle(bird_view, (int(p_ego[0]), int(p_ego[1])), 6, (50,50,50), -1)\n",
        "    \n",
        "    self.imgs[img_id] = bird_view\n",
        "\n",
        "\n",
        "  def add_arrow(self, st, ed, img_id, c=(255, 0, 255), w=2):\n",
        "    cv2.arrowedLine(\n",
        "      self.imgs[img_id], (int(st[0]), int(st[1])), \n",
        "      (int(ed[0] + st[0]), int(ed[1] + st[1])), c, 2,\n",
        "      line_type=cv2.LINE_AA, tipLength=0.3)\n",
        "\n",
        "color_list = np.array(\n",
        "        [1.000, 1.000, 1.000,\n",
        "            0.850, 0.325, 0.098,\n",
        "            0.929, 0.694, 0.125,\n",
        "            0.494, 0.184, 0.556,\n",
        "            0.466, 0.674, 0.188,\n",
        "            0.301, 0.745, 0.933,\n",
        "            0.635, 0.078, 0.184,\n",
        "            0.300, 0.300, 0.300,\n",
        "            0.600, 0.600, 0.600,\n",
        "            1.000, 0.000, 0.000,\n",
        "            1.000, 0.500, 0.000,\n",
        "            0.749, 0.749, 0.000,\n",
        "            0.000, 1.000, 0.000,\n",
        "            0.000, 0.000, 1.000,\n",
        "            0.667, 0.000, 1.000,\n",
        "            0.333, 0.333, 0.000,\n",
        "            0.333, 0.667, 0.000,\n",
        "            0.333, 1.000, 0.000,\n",
        "            0.667, 0.333, 0.000,\n",
        "            0.667, 0.667, 0.000,\n",
        "            0.667, 1.000, 0.000,\n",
        "            1.000, 0.333, 0.000,\n",
        "            1.000, 0.667, 0.000,\n",
        "            1.000, 1.000, 0.000,\n",
        "            0.000, 0.333, 0.500,\n",
        "            0.000, 0.667, 0.500,\n",
        "            0.000, 1.000, 0.500,\n",
        "            0.333, 0.000, 0.500,\n",
        "            0.333, 0.333, 0.500,\n",
        "            0.333, 0.667, 0.500,\n",
        "            0.333, 1.000, 0.500,\n",
        "            0.667, 0.000, 0.500,\n",
        "            0.667, 0.333, 0.500,\n",
        "            0.667, 0.667, 0.500,\n",
        "            0.667, 1.000, 0.500,\n",
        "            1.000, 0.000, 0.500,\n",
        "            1.000, 0.333, 0.500,\n",
        "            1.000, 0.667, 0.500,\n",
        "            1.000, 1.000, 0.500,\n",
        "            0.000, 0.333, 1.000,\n",
        "            0.000, 0.667, 1.000,\n",
        "            0.000, 1.000, 1.000,\n",
        "            0.333, 0.000, 1.000,\n",
        "            0.333, 0.333, 1.000,\n",
        "            0.333, 0.667, 1.000,\n",
        "            0.333, 1.000, 1.000,\n",
        "            0.667, 0.000, 1.000,\n",
        "            0.667, 0.333, 1.000,\n",
        "            0.667, 0.667, 1.000,\n",
        "            0.667, 1.000, 1.000,\n",
        "            1.000, 0.000, 1.000,\n",
        "            1.000, 0.333, 1.000,\n",
        "            1.000, 0.667, 1.000,\n",
        "            0.167, 0.000, 0.000,\n",
        "            0.333, 0.000, 0.000,\n",
        "            0.500, 0.000, 0.000,\n",
        "            0.667, 0.000, 0.000,\n",
        "            0.833, 0.000, 0.000,\n",
        "            1.000, 0.000, 0.000,\n",
        "            0.000, 0.167, 0.000,\n",
        "            0.000, 0.333, 0.000,\n",
        "            0.000, 0.500, 0.000,\n",
        "            0.000, 0.667, 0.000,\n",
        "            0.000, 0.833, 0.000,\n",
        "            0.000, 1.000, 0.000,\n",
        "            0.000, 0.000, 0.000,\n",
        "            0.000, 0.000, 0.167,\n",
        "            0.000, 0.000, 0.333,\n",
        "            0.000, 0.000, 0.500,\n",
        "            0.000, 0.000, 0.667,\n",
        "            0.000, 0.000, 0.833,\n",
        "            0.000, 0.000, 1.000,\n",
        "            0.333, 0.000, 0.500,\n",
        "            0.143, 0.143, 0.143,\n",
        "            0.286, 0.286, 0.286,\n",
        "            0.429, 0.429, 0.429,\n",
        "            0.571, 0.571, 0.571,\n",
        "            0.714, 0.714, 0.714,\n",
        "            0.857, 0.857, 0.857,\n",
        "            0.000, 0.447, 0.741,\n",
        "            0.50, 0.5, 0\n",
        "        ]\n",
        "    ).astype(np.float32)\n",
        "color_list = color_list.reshape((-1, 3)) * 255\n",
        "\n",
        "\n",
        "tango_color = [[252, 233,  79], #\tButter 1\n",
        "  [237, 212,   0], #\tButter 2\n",
        "  [196, 160,   0], #\tButter 3\n",
        "  [138, 226,  52], #\tChameleon 1\n",
        "  [115, 210,  22], #\tChameleon 2\n",
        "  [ 78, 154,   6], #\tChameleon 3\n",
        "  [252, 175,  62], #\tOrange 1\n",
        "  [245, 121,   0], #\tOrange 2\n",
        "  [206,  92,   0], #\tOrange 3\n",
        "  [114, 159, 207], #\tSky Blue 1\n",
        "  [ 52, 101, 164], #\tSky Blue 2\n",
        "  [ 32,  74, 135], #\tSky Blue 3\n",
        "  [173, 127, 168], #\tPlum 1\n",
        "  [117,  80, 123], #\tPlum 2\n",
        "  [ 92,  53, 102], #\tPlum 3\n",
        "  [233, 185, 110], #\tChocolate 1\n",
        "  [193, 125,  17], #\tChocolate 2\n",
        "  [143,  89,   2], #\tChocolate 3\n",
        "  [239,  41,  41], #\tScarlet Red 1\n",
        "  [204,   0,   0], #\tScarlet Red 2\n",
        "  [164,   0,   0], #\tScarlet Red 3\n",
        "  [238, 238, 236], #\tAluminium 1\n",
        "  [211, 215, 207], #\tAluminium 2\n",
        "  [186, 189, 182], #\tAluminium 3\n",
        "  [136, 138, 133], #\tAluminium 4\n",
        "  [ 85,  87,  83], #\tAluminium 5\n",
        "  [ 46,  52,  54], #\tAluminium 6\n",
        "]\n",
        "tango_color = np.array(tango_color, np.uint8).reshape((-1, 1, 1, 3))\n",
        "\n",
        "\n",
        "tango_color_dark = [\n",
        "  [114, 159, 207], #\tSky Blue 1\n",
        "  [196, 160,   0], #\tButter 3\n",
        "  [ 78, 154,   6], #\tChameleon 3\n",
        "  [206,  92,   0], #\tOrange 3\n",
        "  [164,   0,   0], #\tScarlet Red 3\n",
        "  [ 32,  74, 135], #\tSky Blue 3\n",
        "  [ 92,  53, 102], #\tPlum 3\n",
        "  [143,  89,   2], #\tChocolate 3\n",
        "  [ 85,  87,  83], #\tAluminium 5\n",
        "  [186, 189, 182], #\tAluminium 3\n",
        "]\n",
        "\n",
        "tango_color_dark = np.array(tango_color_dark, np.uint8).reshape((-1, 1, 1, 3))\n"
      ],
      "metadata": {
        "id": "wVTlmPrxKLxE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model Decode"
      ],
      "metadata": {
        "id": "scN9q_VnKTLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from .utils import _gather_feat, _tranpose_and_gather_feat\n",
        "# from .utils import _nms, _topk, _topk_channel\n",
        "\n",
        "\n",
        "def _update_kps_with_hm(\n",
        "  kps, output, batch, num_joints, K, bboxes=None, scores=None):\n",
        "  if 'hm_hp' in output:\n",
        "    hm_hp = output['hm_hp']\n",
        "    hm_hp = _nms(hm_hp)\n",
        "    thresh = 0.2\n",
        "    kps = kps.view(batch, K, num_joints, 2).permute(\n",
        "        0, 2, 1, 3).contiguous() # b x J x K x 2\n",
        "    reg_kps = kps.unsqueeze(3).expand(batch, num_joints, K, K, 2)\n",
        "    hm_score, hm_inds, hm_ys, hm_xs = _topk_channel(hm_hp, K=K) # b x J x K\n",
        "    if 'hp_offset' in output or 'reg' in output:\n",
        "        hp_offset = output['hp_offset'] if 'hp_offset' in output \\\n",
        "                    else output['reg']\n",
        "        hp_offset = _tranpose_and_gather_feat(\n",
        "            hp_offset, hm_inds.view(batch, -1))\n",
        "        hp_offset = hp_offset.view(batch, num_joints, K, 2)\n",
        "        hm_xs = hm_xs + hp_offset[:, :, :, 0]\n",
        "        hm_ys = hm_ys + hp_offset[:, :, :, 1]\n",
        "    else:\n",
        "        hm_xs = hm_xs + 0.5\n",
        "        hm_ys = hm_ys + 0.5\n",
        "    \n",
        "    mask = (hm_score > thresh).float()\n",
        "    hm_score = (1 - mask) * -1 + mask * hm_score\n",
        "    hm_ys = (1 - mask) * (-10000) + mask * hm_ys\n",
        "    hm_xs = (1 - mask) * (-10000) + mask * hm_xs\n",
        "    hm_kps = torch.stack([hm_xs, hm_ys], dim=-1).unsqueeze(\n",
        "        2).expand(batch, num_joints, K, K, 2)\n",
        "    dist = (((reg_kps - hm_kps) ** 2).sum(dim=4) ** 0.5)\n",
        "    min_dist, min_ind = dist.min(dim=3) # b x J x K\n",
        "    hm_score = hm_score.gather(2, min_ind).unsqueeze(-1) # b x J x K x 1\n",
        "    min_dist = min_dist.unsqueeze(-1)\n",
        "    min_ind = min_ind.view(batch, num_joints, K, 1, 1).expand(\n",
        "        batch, num_joints, K, 1, 2)\n",
        "    hm_kps = hm_kps.gather(3, min_ind)\n",
        "    hm_kps = hm_kps.view(batch, num_joints, K, 2)        \n",
        "    mask = (hm_score < thresh)\n",
        "    \n",
        "    if bboxes is not None:\n",
        "      l = bboxes[:, :, 0].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      t = bboxes[:, :, 1].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      r = bboxes[:, :, 2].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      b = bboxes[:, :, 3].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      mask = (hm_kps[..., 0:1] < l) + (hm_kps[..., 0:1] > r) + \\\n",
        "              (hm_kps[..., 1:2] < t) + (hm_kps[..., 1:2] > b) + mask\n",
        "    else:\n",
        "      l = kps[:, :, :, 0:1].min(dim=1, keepdim=True)[0]\n",
        "      t = kps[:, :, :, 1:2].min(dim=1, keepdim=True)[0]\n",
        "      r = kps[:, :, :, 0:1].max(dim=1, keepdim=True)[0]\n",
        "      b = kps[:, :, :, 1:2].max(dim=1, keepdim=True)[0]\n",
        "      margin = 0.25\n",
        "      l = l - (r - l) * margin\n",
        "      r = r + (r - l) * margin\n",
        "      t = t - (b - t) * margin\n",
        "      b = b + (b - t) * margin\n",
        "      mask = (hm_kps[..., 0:1] < l) + (hm_kps[..., 0:1] > r) + \\\n",
        "              (hm_kps[..., 1:2] < t) + (hm_kps[..., 1:2] > b) + mask\n",
        "      # sc = (kps[:, :, :, :].max(dim=1, keepdim=True) - kps[:, :, :, :].min(dim=1))\n",
        "    # mask = mask + (min_dist > 10)\n",
        "    mask = (mask > 0).float()\n",
        "    kps_score = (1 - mask) * hm_score + mask * \\\n",
        "      scores.unsqueeze(-1).expand(batch, num_joints, K, 1) # bJK1\n",
        "    kps_score = scores * kps_score.mean(dim=1).view(batch, K)\n",
        "    # kps_score[scores < 0.1] = 0\n",
        "    mask = mask.expand(batch, num_joints, K, 2)\n",
        "    kps = (1 - mask) * hm_kps + mask * kps\n",
        "    kps = kps.permute(0, 2, 1, 3).contiguous().view(\n",
        "        batch, K, num_joints * 2)\n",
        "    return kps, kps_score\n",
        "  else:\n",
        "    return kps, kps\n",
        "\n",
        "\n",
        "\n",
        "## Decoder with Radar point cloud fusion support\n",
        "def fusion_decode(output, K=100, opt=None):\n",
        "  if not ('hm' in output):\n",
        "    return {}\n",
        "\n",
        "  if opt.zero_tracking:\n",
        "    output['tracking'] *= 0\n",
        "  \n",
        "  heat = output['hm']\n",
        "  batch, cat, height, width = heat.size()\n",
        "\n",
        "  heat = _nms(heat)\n",
        "  scores, inds, clses, ys0, xs0 = _topk(heat, K=K)\n",
        "\n",
        "  clses  = clses.view(batch, K)\n",
        "  scores = scores.view(batch, K)\n",
        "  bboxes = None\n",
        "  cts = torch.cat([xs0.unsqueeze(2), ys0.unsqueeze(2)], dim=2)\n",
        "  ret = {'scores': scores, 'clses': clses.float(), \n",
        "         'xs': xs0, 'ys': ys0, 'cts': cts}\n",
        "  if 'reg' in output:\n",
        "    reg = output['reg']\n",
        "    reg = _tranpose_and_gather_feat(reg, inds)\n",
        "    reg = reg.view(batch, K, 2)\n",
        "    xs = xs0.view(batch, K, 1) + reg[:, :, 0:1]\n",
        "    ys = ys0.view(batch, K, 1) + reg[:, :, 1:2]\n",
        "  else:\n",
        "    xs = xs0.view(batch, K, 1) + 0.5\n",
        "    ys = ys0.view(batch, K, 1) + 0.5\n",
        "\n",
        "  if 'wh' in output:\n",
        "    wh = output['wh']\n",
        "    wh = _tranpose_and_gather_feat(wh, inds) # B x K x (F)\n",
        "    # wh = wh.view(batch, K, -1)\n",
        "    wh = wh.view(batch, K, 2)\n",
        "    wh[wh < 0] = 0\n",
        "    if wh.size(2) == 2 * cat: # cat spec\n",
        "      wh = wh.view(batch, K, -1, 2)\n",
        "      cats = clses.view(batch, K, 1, 1).expand(batch, K, 1, 2)\n",
        "      wh = wh.gather(2, cats.long()).squeeze(2) # B x K x 2\n",
        "    else:\n",
        "      pass\n",
        "    bboxes = torch.cat([xs - wh[..., 0:1] / 2, \n",
        "                        ys - wh[..., 1:2] / 2,\n",
        "                        xs + wh[..., 0:1] / 2, \n",
        "                        ys + wh[..., 1:2] / 2], dim=2)\n",
        "    ret['bboxes'] = bboxes\n",
        " \n",
        "  if 'ltrb' in output:\n",
        "    ltrb = output['ltrb']\n",
        "    ltrb = _tranpose_and_gather_feat(ltrb, inds) # B x K x 4\n",
        "    ltrb = ltrb.view(batch, K, 4)\n",
        "    bboxes = torch.cat([xs0.view(batch, K, 1) + ltrb[..., 0:1], \n",
        "                        ys0.view(batch, K, 1) + ltrb[..., 1:2],\n",
        "                        xs0.view(batch, K, 1) + ltrb[..., 2:3], \n",
        "                        ys0.view(batch, K, 1) + ltrb[..., 3:4]], dim=2)\n",
        "    ret['bboxes'] = bboxes\n",
        "\n",
        "  ## Decode depth with depth residual support\n",
        "  if 'dep' in output:\n",
        "    dep = output['dep']\n",
        "    dep = _tranpose_and_gather_feat(dep, inds) # B x K x (C)\n",
        "    # dep = dep.view(batch, K, -1)\n",
        "    # dep[dep < 0] = 0\n",
        "    cats = clses.view(batch, K, 1, 1)\n",
        "    if dep.size(2) == cat: # cat spec\n",
        "      dep = dep.view(batch, K, -1, 1) # B x K x C x 1\n",
        "      dep = dep.gather(2, cats.long()).squeeze(2) # B x K x 1\n",
        "    \n",
        "    # add depth residuals to estimated depth values\n",
        "    if 'dep_sec' in output:\n",
        "      dep_sec = output['dep_sec']\n",
        "      dep_sec = _tranpose_and_gather_feat(dep_sec, inds) # B x K x [C]\n",
        "      if dep_sec.size(2) == cat: # cat spec\n",
        "        dep_sec = dep_sec.view(batch, K, -1, 1) # B x K x C x 1\n",
        "        dep_sec = dep_sec.gather(2, cats.long()).squeeze(2) # B x K x 1\n",
        "        dep_sec_mask = torch.tensor(dep_sec_mask, device=dep_sec.device).unsqueeze(0).unsqueeze(0).unsqueeze(3)\n",
        "      dep = dep_sec\n",
        "    \n",
        "    ret['dep'] = dep\n",
        "  \n",
        "\n",
        "  regression_heads = ['tracking', 'rot', 'dim', 'amodel_offset',\n",
        "    'nuscenes_att', 'velocity', 'rot_sec']\n",
        "\n",
        "  for head in regression_heads:\n",
        "    if head in output:\n",
        "      ret[head] = _tranpose_and_gather_feat(\n",
        "        output[head], inds).view(batch, K, -1)\n",
        "  \n",
        "  if 'rot_sec' in output:\n",
        "    ret['rot'] = ret['rot_sec']\n",
        "\n",
        "  if 'ltrb_amodal' in output:\n",
        "    ltrb_amodal = output['ltrb_amodal']\n",
        "    ltrb_amodal = _tranpose_and_gather_feat(ltrb_amodal, inds) # B x K x 4\n",
        "    ltrb_amodal = ltrb_amodal.view(batch, K, 4)\n",
        "    bboxes_amodal = torch.cat([xs0.view(batch, K, 1) + ltrb_amodal[..., 0:1], \n",
        "                          ys0.view(batch, K, 1) + ltrb_amodal[..., 1:2],\n",
        "                          xs0.view(batch, K, 1) + ltrb_amodal[..., 2:3], \n",
        "                          ys0.view(batch, K, 1) + ltrb_amodal[..., 3:4]], dim=2)\n",
        "    ret['bboxes_amodal'] = bboxes_amodal\n",
        "    ret['bboxes'] = bboxes_amodal\n",
        "\n",
        "  if 'hps' in output:\n",
        "    kps = output['hps']\n",
        "    num_joints = kps.shape[1] // 2\n",
        "    kps = _tranpose_and_gather_feat(kps, inds)\n",
        "    kps = kps.view(batch, K, num_joints * 2)\n",
        "    kps[..., ::2] += xs0.view(batch, K, 1).expand(batch, K, num_joints)\n",
        "    kps[..., 1::2] += ys0.view(batch, K, 1).expand(batch, K, num_joints)\n",
        "    kps, kps_score = _update_kps_with_hm(\n",
        "      kps, output, batch, num_joints, K, bboxes, scores)\n",
        "    ret['hps'] = kps\n",
        "    ret['kps_score'] = kps_score\n",
        "\n",
        "  if 'pre_inds' in output and output['pre_inds'] is not None:\n",
        "    pre_inds = output['pre_inds'] # B x pre_K\n",
        "    pre_K = pre_inds.shape[1]\n",
        "    pre_ys = (pre_inds / width).int().float()\n",
        "    pre_xs = (pre_inds % width).int().float()\n",
        "\n",
        "    ret['pre_cts'] = torch.cat(\n",
        "      [pre_xs.unsqueeze(2), pre_ys.unsqueeze(2)], dim=2)\n",
        "  \n",
        "  return ret\n"
      ],
      "metadata": {
        "id": "u9AgT8BAKWXi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Post Process\n"
      ],
      "metadata": {
        "id": "4fCJAe0WKY_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from .image import transform_preds_with_trans, get_affine_transform\n",
        "# from .ddd_utils import ddd2locrot, comput_corners_3d\n",
        "# from .ddd_utils import project_to_image, rot_y2alpha\n",
        "import numba\n",
        "import math\n",
        "\n",
        "def get_alpha(rot):\n",
        "  # output: (B, 8) [bin1_cls[0], bin1_cls[1], bin1_sin, bin1_cos, \n",
        "  #                 bin2_cls[0], bin2_cls[1], bin2_sin, bin2_cos]\n",
        "  # return rot[:, 0]\n",
        "  idx = rot[:, 1] > rot[:, 5]\n",
        "  alpha1 = np.arctan2(rot[:, 2], rot[:, 3]) + (-0.5 * np.pi)\n",
        "  alpha2 = np.arctan2(rot[:, 6], rot[:, 7]) + ( 0.5 * np.pi)\n",
        "  return alpha1 * idx + alpha2 * (1 - idx)\n",
        "\n",
        "def points_in_bbox(bbox, pc):\n",
        "  pass\n",
        "\n",
        "def generic_post_process(\n",
        "  opt, dets, c, s, h, w, num_classes, calibs=None, height=-1, width=-1, is_gt=False):\n",
        "  if not ('scores' in dets):\n",
        "    return [{}], [{}]\n",
        "  ret = []\n",
        "\n",
        "  for i in range(len(dets['scores'])):\n",
        "    preds = []\n",
        "    trans = get_affine_transform(\n",
        "      c[i], s[i], 0, (w, h), inv=1).astype(np.float32)\n",
        "    for j in range(len(dets['scores'][i])):\n",
        "      if dets['scores'][i][j] < opt.out_thresh:\n",
        "        break\n",
        "      item = {}\n",
        "      item['score'] = dets['scores'][i][j]\n",
        "      item['class'] = int(dets['clses'][i][j]) + 1\n",
        "      item['ct'] = transform_preds_with_trans(\n",
        "        (dets['cts'][i][j]).reshape(1, 2), trans).reshape(2)\n",
        "\n",
        "      if 'tracking' in dets:\n",
        "        tracking = transform_preds_with_trans(\n",
        "          (dets['tracking'][i][j] + dets['cts'][i][j]).reshape(1, 2), \n",
        "          trans).reshape(2)\n",
        "        item['tracking'] = tracking - item['ct']\n",
        "\n",
        "      if 'bboxes' in dets:\n",
        "        bbox = transform_preds_with_trans(\n",
        "          dets['bboxes'][i][j].reshape(2, 2), trans).reshape(4)\n",
        "        item['bbox'] = bbox\n",
        "\n",
        "      if 'hps' in dets:\n",
        "        pts = transform_preds_with_trans(\n",
        "          dets['hps'][i][j].reshape(-1, 2), trans).reshape(-1)\n",
        "        item['hps'] = pts\n",
        "\n",
        "      if 'dep' in dets and len(dets['dep'][i]) > j:\n",
        "        # if 'dep_res' in dets:\n",
        "        #   item['dep'] = dets['dep'][i][j] + dets['dep_res'][i][j]\n",
        "        # else:\n",
        "        item['dep'] = dets['dep'][i][j]\n",
        "        if len(item['dep'])>1:\n",
        "          item['dep'] = item['dep'][0]\n",
        "      \n",
        "      if 'dim' in dets and len(dets['dim'][i]) > j:\n",
        "        item['dim'] = dets['dim'][i][j]\n",
        "\n",
        "      if 'rot' in dets and len(dets['rot'][i]) > j:\n",
        "        item['alpha'] = get_alpha(dets['rot'][i][j:j+1])[0]\n",
        "      \n",
        "      if 'rot' in dets and 'dep' in dets and 'dim' in dets \\\n",
        "        and len(dets['dep'][i]) > j:\n",
        "        if 'amodel_offset' in dets and len(dets['amodel_offset'][i]) > j:\n",
        "          ct_output = dets['bboxes'][i][j].reshape(2, 2).mean(axis=0)\n",
        "          amodel_ct_output = ct_output + dets['amodel_offset'][i][j]\n",
        "          ct = transform_preds_with_trans(\n",
        "            amodel_ct_output.reshape(1, 2), trans).reshape(2).tolist()\n",
        "        else:\n",
        "          bbox = item['bbox']\n",
        "          ct = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n",
        "        item['ct'] = ct\n",
        "        item['loc'], item['rot_y'] = ddd2locrot(\n",
        "          ct, item['alpha'], item['dim'], item['dep'], calibs[i])\n",
        "      \n",
        "      preds.append(item)\n",
        "\n",
        "    if 'nuscenes_att' in dets:\n",
        "      for j in range(len(preds)):\n",
        "        preds[j]['nuscenes_att'] = dets['nuscenes_att'][i][j]\n",
        "\n",
        "    if 'velocity' in dets:\n",
        "      for j in range(len(preds)):\n",
        "        vel = dets['velocity'][i][j]\n",
        "        if opt.pointcloud and not is_gt:\n",
        "          ## put velocity in the same direction as box orientation\n",
        "          V = math.sqrt(vel[0]**2 + vel[2]**2)\n",
        "          vel[0] = np.cos(preds[j]['rot_y']) * V\n",
        "          vel[2] = -np.sin(preds[j]['rot_y']) * V\n",
        "        preds[j]['velocity'] = vel[:3]\n",
        "    \n",
        "    ret.append(preds)\n",
        "  \n",
        "  return ret"
      ],
      "metadata": {
        "id": "3Lo0cafsKgkI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "fEHSAMrPLAna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Preparation (from the official repo of centerfusion) \n",
        "1.   Download the nuScenes dataset from nuScenes website.\n",
        "2.   Extract the downloaded files in the ${CF_ROOT}\\data\\nuscenes directory. You should have the following directory structure after extraction:\n",
        "\n",
        "```\n",
        "${CF_ROOT}\n",
        "`-- data\n",
        "    `-- nuscenes\n",
        "        |-- maps\n",
        "        |-- samples\n",
        "        |   |-- CAM_BACK\n",
        "        |   |   | -- xxx.jpg\n",
        "        |   |   ` -- ...\n",
        "        |   |-- CAM_BACK_LEFT\n",
        "        |   |-- CAM_BACK_RIGHT\n",
        "        |   |-- CAM_FRONT\n",
        "        |   |-- CAM_FRONT_LEFT\n",
        "        |   |-- CAM_FRONT_RIGHT\n",
        "        |   |-- RADAR_BACK_LEFT\n",
        "        |   |   | -- xxx.pcd\n",
        "        |   |   ` -- ...\n",
        "        |   |-- RADAR_BACK_RIGHT\n",
        "        |   |-- RADAR_FRON\n",
        "        |   |-- RADAR_FRONT_LEFT\n",
        "        |   `-- RADAR_FRONT_RIGHT\n",
        "        |-- sweeps\n",
        "        |-- v1.0-mini\n",
        "        |-- v1.0-test\n",
        "        `-- v1.0-trainval\n",
        "```\n",
        "3. Run the convert_nuScenes.py script to convet the nuScenes dataset to COCO format:\n",
        "```\n",
        "cd $CF_ROOT/src/tools\n",
        "python convert_nuScenes.py\n",
        "```\n",
        "4. Upload the dataset to google drive to be accessed by google colab\n",
        "5. Please check the official repo of [centerfusion](https://github.com/mrnabati/CenterFusion) to download the pretrained models"
      ],
      "metadata": {
        "id": "34XhHfK6LZ9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Processing"
      ],
      "metadata": {
        "id": "T16D30JO0NKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### generic dataset class"
      ],
      "metadata": {
        "id": "GzsWOP8K8JRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "import cv2\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "import pycocotools.coco as coco\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "# from utils.image import flip, color_aug\n",
        "# from utils.image import get_affine_transform, affine_transform\n",
        "# from utils.image import gaussian_radius, draw_umich_gaussian, gaussian2D\n",
        "# from utils.pointcloud import map_pointcloud_to_image, pc_dep_to_hm\n",
        "import copy\n",
        "from nuscenes.utils.data_classes import Box\n",
        "from pyquaternion import Quaternion\n",
        "from nuscenes.utils.geometry_utils import view_points\n",
        "# from utils.ddd_utils import compute_box_3d, project_to_image, draw_box_3d\n",
        "# from utils.ddd_utils import comput_corners_3d, alpha2rot_y, get_pc_hm\n",
        "\n",
        "\n",
        "def get_dist_thresh(calib, ct, dim, alpha):\n",
        "    rotation_y = alpha2rot_y(alpha, ct[0], calib[0, 2], calib[0, 0])\n",
        "    corners_3d = comput_corners_3d(dim, rotation_y)\n",
        "    dist_thresh = max(corners_3d[:,2]) - min(corners_3d[:,2]) / 2.0\n",
        "    return dist_thresh\n",
        "\n",
        "\n",
        "class GenericDataset(data.Dataset):\n",
        "  default_resolution = None\n",
        "  num_categories = None\n",
        "  class_name = None\n",
        "  # cat_ids: map from 'category_id' in the annotation files to 1..num_categories\n",
        "  # Not using 0 because 0 is used for don't care region and ignore loss.\n",
        "  cat_ids = None\n",
        "  max_objs = None\n",
        "  rest_focal_length = 1200\n",
        "  num_joints = 17\n",
        "  flip_idx = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], \n",
        "              [11, 12], [13, 14], [15, 16]]\n",
        "  edges = [[0, 1], [0, 2], [1, 3], [2, 4], \n",
        "           [4, 6], [3, 5], [5, 6], \n",
        "           [5, 7], [7, 9], [6, 8], [8, 10], \n",
        "           [6, 12], [5, 11], [11, 12], \n",
        "           [12, 14], [14, 16], [11, 13], [13, 15]]\n",
        "  mean = np.array([0.40789654, 0.44719302, 0.47026115],\n",
        "                   dtype=np.float32).reshape(1, 1, 3)\n",
        "  std  = np.array([0.28863828, 0.27408164, 0.27809835],\n",
        "                   dtype=np.float32).reshape(1, 1, 3)\n",
        "  _eig_val = np.array([0.2141788, 0.01817699, 0.00341571],\n",
        "                      dtype=np.float32)\n",
        "  _eig_vec = np.array([\n",
        "        [-0.58752847, -0.69563484, 0.41340352],\n",
        "        [-0.5832747, 0.00994535, -0.81221408],\n",
        "        [-0.56089297, 0.71832671, 0.41158938]\n",
        "    ], dtype=np.float32)\n",
        "  ignore_val = 1\n",
        "  nuscenes_att_range = {0: [0, 1], 1: [0, 1], 2: [2, 3, 4], 3: [2, 3, 4], \n",
        "    4: [2, 3, 4], 5: [5, 6, 7], 6: [5, 6, 7], 7: [5, 6, 7]}\n",
        "  \n",
        "  ## change these vectors to actual mean and std to normalize\n",
        "  pc_mean = np.zeros((18,1))\n",
        "  pc_std = np.ones((18,1))\n",
        "  img_ind = 0\n",
        "\n",
        "\n",
        "  def __init__(self, opt=None, split=None, ann_path=None, img_dir=None):\n",
        "    super(GenericDataset, self).__init__()\n",
        "    if opt is not None and split is not None:\n",
        "      self.split = split\n",
        "      self.opt = opt\n",
        "      self._data_rng = np.random.RandomState(123)\n",
        "      self.enable_meta = True if (opt.run_dataset_eval and split in [\"val\", \"mini_val\", \"test\"]) or opt.eval else False\n",
        "    \n",
        "    if ann_path is not None and img_dir is not None:\n",
        "      print('==> initializing {} data from {}, \\n images from {} ...'.format(\n",
        "        split, ann_path, img_dir))\n",
        "      self.coco = coco.COCO(ann_path)\n",
        "      self.images = self.coco.getImgIds()\n",
        "\n",
        "      if opt.tracking:\n",
        "        if not ('videos' in self.coco.dataset):\n",
        "          self.fake_video_data()\n",
        "        print('Creating video index!')\n",
        "        self.video_to_images = defaultdict(list)\n",
        "        for image in self.coco.dataset['images']:\n",
        "          self.video_to_images[image['video_id']].append(image)\n",
        "      \n",
        "      self.img_dir = img_dir\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    opt = self.opt\n",
        "    img, anns, img_info, img_path = self._load_data(index)\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "\n",
        "    ## sort annotations based on depth form far to near\n",
        "    new_anns = sorted(anns, key=lambda k: k['depth'], reverse=True)\n",
        "\n",
        "    ## Get center and scale from image\n",
        "    c = np.array([img.shape[1] / 2., img.shape[0] / 2.], dtype=np.float32)\n",
        "    s = max(img.shape[0], img.shape[1]) * 1.0 if not self.opt.not_max_crop \\\n",
        "      else np.array([img.shape[1], img.shape[0]], np.float32)\n",
        "    aug_s, rot, flipped = 1, 0, 0\n",
        "\n",
        "    ## data augmentation for training set\n",
        "    if 'train' in self.split:\n",
        "      c, aug_s, rot = self._get_aug_param(c, s, width, height)\n",
        "      s = s * aug_s\n",
        "      if np.random.random() < opt.flip:\n",
        "        flipped = 1\n",
        "        img = img[:, ::-1, :]\n",
        "        anns = self._flip_anns(anns, width)\n",
        "\n",
        "    trans_input = get_affine_transform(\n",
        "      c, s, rot, [opt.input_w, opt.input_h])\n",
        "    trans_output = get_affine_transform(\n",
        "      c, s, rot, [opt.output_w, opt.output_h])\n",
        "    inp = self._get_input(img, trans_input)\n",
        "    ret = {'image': inp}\n",
        "    gt_det = {'bboxes': [], 'scores': [], 'clses': [], 'cts': []}\n",
        "\n",
        "    #  load point cloud data\n",
        "    if opt.pointcloud:\n",
        "      pc_2d, pc_N, pc_dep, pc_3d = self._load_pc_data(img, img_info, \n",
        "        trans_input, trans_output, flipped)\n",
        "      ret.update({ 'pc_2d': pc_2d,\n",
        "                   'pc_3d': pc_3d,\n",
        "                   'pc_N': pc_N,\n",
        "                   'pc_dep': pc_dep })\n",
        "\n",
        "    pre_cts, track_ids = None, None\n",
        "    if opt.tracking:\n",
        "      pre_image, pre_anns, frame_dist, pre_img_info = self._load_pre_data(\n",
        "        img_info['video_id'], img_info['frame_id'], \n",
        "        img_info['sensor_id'] if 'sensor_id' in img_info else 1)\n",
        "      if flipped:\n",
        "        pre_image = pre_image[:, ::-1, :].copy()\n",
        "        pre_anns = self._flip_anns(pre_anns, width)\n",
        "        if pc_2d is not None:\n",
        "          pc_2d = self._flip_pc(pc_2d,  width)\n",
        "      if opt.same_aug_pre and frame_dist != 0:\n",
        "        trans_input_pre = trans_input \n",
        "        trans_output_pre = trans_output\n",
        "      else:\n",
        "        c_pre, aug_s_pre, _ = self._get_aug_param(\n",
        "          c, s, width, height, disturb=True)\n",
        "        s_pre = s * aug_s_pre\n",
        "        trans_input_pre = get_affine_transform(\n",
        "          c_pre, s_pre, rot, [opt.input_w, opt.input_h])\n",
        "        trans_output_pre = get_affine_transform(\n",
        "          c_pre, s_pre, rot, [opt.output_w, opt.output_h])\n",
        "      pre_img = self._get_input(pre_image, trans_input_pre)\n",
        "      pre_hm, pre_cts, track_ids = self._get_pre_dets(\n",
        "        pre_anns, trans_input_pre, trans_output_pre)\n",
        "      ret['pre_img'] = pre_img\n",
        "      if opt.pre_hm:\n",
        "        ret['pre_hm'] = pre_hm\n",
        "      if opt.pointcloud:\n",
        "        pre_pc_2d, pre_pc_N, pre_pc_hm, pre_pc_3d = self._load_pc_data(pre_img, pre_img_info, \n",
        "            trans_input_pre, trans_output_pre, flipped)\n",
        "        ret['pre_pc_2d'] = pre_pc_2d\n",
        "        ret['pre_pc_3d'] = pre_pc_3d\n",
        "        ret['pre_pc_N'] = pre_pc_N\n",
        "        ret['pre_pc_hm'] = pre_pc_hm\n",
        "\n",
        "    ### init samples\n",
        "    self._init_ret(ret, gt_det)\n",
        "    calib = self._get_calib(img_info, width, height)\n",
        "\n",
        "    # get velocity transformation matrix\n",
        "    if \"velocity_trans_matrix\" in img_info:\n",
        "      velocity_mat = np.array(img_info['velocity_trans_matrix'], dtype=np.float32)\n",
        "    else:\n",
        "      velocity_mat = np.eye(4)\n",
        "    \n",
        "    num_objs = min(len(anns), self.max_objs)\n",
        "    for k in range(num_objs):\n",
        "      ann = anns[k]\n",
        "      cls_id = int(self.cat_ids[ann['category_id']])\n",
        "      if cls_id > self.opt.num_classes or cls_id <= -999:\n",
        "        continue\n",
        "      bbox, bbox_amodal = self._get_bbox_output(\n",
        "        ann['bbox'], trans_output, height, width)\n",
        "      if cls_id <= 0 or ('iscrowd' in ann and ann['iscrowd'] > 0):\n",
        "        self._mask_ignore_or_crowd(ret, cls_id, bbox)\n",
        "        continue\n",
        "      self._add_instance(\n",
        "        ret, gt_det, k, cls_id, bbox, bbox_amodal, ann, trans_output, aug_s, \n",
        "        calib, pre_cts, track_ids)\n",
        "\n",
        "    if self.opt.debug > 0 or self.enable_meta:\n",
        "      gt_det = self._format_gt_det(gt_det)\n",
        "      meta = {'c': c, 's': s, 'gt_det': gt_det, 'img_id': img_info['id'],\n",
        "              'img_path': img_path, 'calib': calib,\n",
        "              'img_width': img_info['width'], 'img_height': img_info['height'],\n",
        "              'flipped': flipped, 'velocity_mat':velocity_mat}\n",
        "      ret['meta'] = meta\n",
        "    ret['calib'] = calib\n",
        "    return ret\n",
        "\n",
        "\n",
        "  def get_default_calib(self, width, height):\n",
        "    calib = np.array([[self.rest_focal_length, 0, width / 2, 0], \n",
        "                        [0, self.rest_focal_length, height / 2, 0], \n",
        "                        [0, 0, 1, 0]])\n",
        "    return calib\n",
        "\n",
        "  def _load_image_anns(self, img_id, coco, img_dir):\n",
        "    img_info = coco.loadImgs(ids=[img_id])[0]\n",
        "    file_name = img_info['file_name']\n",
        "    img_path = os.path.join(img_dir, file_name)\n",
        "    ann_ids = coco.getAnnIds(imgIds=[img_id])\n",
        "    anns = copy.deepcopy(coco.loadAnns(ids=ann_ids))\n",
        "    img = cv2.imread(img_path)\n",
        "    return img, anns, img_info, img_path\n",
        "\n",
        "  def _load_data(self, index):\n",
        "    coco = self.coco\n",
        "    img_dir = self.img_dir\n",
        "    img_id = self.images[index]\n",
        "    img, anns, img_info, img_path = self._load_image_anns(img_id, coco, img_dir)\n",
        "\n",
        "    return img, anns, img_info, img_path\n",
        "\n",
        "\n",
        "  def _load_pre_data(self, video_id, frame_id, sensor_id=1):\n",
        "    img_infos = self.video_to_images[video_id]\n",
        "    # If training, random sample nearby frames as the \"previous\" frame\n",
        "    # If testing, get the exact prevous frame\n",
        "    if 'train' in self.split:\n",
        "      img_ids = [(img_info['id'], img_info['frame_id']) \\\n",
        "          for img_info in img_infos \\\n",
        "          if abs(img_info['frame_id'] - frame_id) < self.opt.max_frame_dist and \\\n",
        "          (not ('sensor_id' in img_info) or img_info['sensor_id'] == sensor_id)]\n",
        "    else:\n",
        "      img_ids = [(img_info['id'], img_info['frame_id']) \\\n",
        "          for img_info in img_infos \\\n",
        "            if (img_info['frame_id'] - frame_id) == -1 and \\\n",
        "            (not ('sensor_id' in img_info) or img_info['sensor_id'] == sensor_id)]\n",
        "      if len(img_ids) == 0:\n",
        "        img_ids = [(img_info['id'], img_info['frame_id']) \\\n",
        "            for img_info in img_infos \\\n",
        "            if (img_info['frame_id'] - frame_id) == 0 and \\\n",
        "            (not ('sensor_id' in img_info) or img_info['sensor_id'] == sensor_id)]\n",
        "    rand_id = np.random.choice(len(img_ids))\n",
        "    img_id, pre_frame_id = img_ids[rand_id]\n",
        "    frame_dist = abs(frame_id - pre_frame_id)\n",
        "    img, anns, img_info, _ = self._load_image_anns(img_id, self.coco, self.img_dir)\n",
        "    return img, anns, frame_dist, img_info\n",
        "\n",
        "\n",
        "  def _get_pre_dets(self, anns, trans_input, trans_output):\n",
        "    hm_h, hm_w = self.opt.input_h, self.opt.input_w\n",
        "    down_ratio = self.opt.down_ratio\n",
        "    trans = trans_input\n",
        "    reutrn_hm = self.opt.pre_hm\n",
        "    pre_hm = np.zeros((1, hm_h, hm_w), dtype=np.float32) if reutrn_hm else None\n",
        "    pre_cts, track_ids = [], []\n",
        "    for ann in anns:\n",
        "      cls_id = int(self.cat_ids[ann['category_id']])\n",
        "      if cls_id > self.opt.num_classes or cls_id <= -99 or \\\n",
        "         ('iscrowd' in ann and ann['iscrowd'] > 0):\n",
        "        continue\n",
        "      bbox = self._coco_box_to_bbox(ann['bbox'])\n",
        "      bbox[:2] = affine_transform(bbox[:2], trans)\n",
        "      bbox[2:] = affine_transform(bbox[2:], trans)\n",
        "      bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, hm_w - 1)\n",
        "      bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, hm_h - 1)\n",
        "      h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]\n",
        "      max_rad = 1\n",
        "      if (h > 0 and w > 0):\n",
        "        radius = gaussian_radius((math.ceil(h), math.ceil(w)))\n",
        "        radius = max(0, int(radius)) \n",
        "        max_rad = max(max_rad, radius)\n",
        "        ct = np.array(\n",
        "          [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)\n",
        "        ct0 = ct.copy()\n",
        "        conf = 1\n",
        "\n",
        "        ct[0] = ct[0] + np.random.randn() * self.opt.hm_disturb * w\n",
        "        ct[1] = ct[1] + np.random.randn() * self.opt.hm_disturb * h\n",
        "        conf = 1 if np.random.random() > self.opt.lost_disturb else 0\n",
        "        \n",
        "        ct_int = ct.astype(np.int32)\n",
        "        if conf == 0:\n",
        "          pre_cts.append(ct / down_ratio)\n",
        "        else:\n",
        "          pre_cts.append(ct0 / down_ratio)\n",
        "\n",
        "        track_ids.append(ann['track_id'] if 'track_id' in ann else -1)\n",
        "        if reutrn_hm:\n",
        "          draw_umich_gaussian(pre_hm[0], ct_int, radius, k=conf)\n",
        "\n",
        "        if np.random.random() < self.opt.fp_disturb and reutrn_hm:\n",
        "          ct2 = ct0.copy()\n",
        "          # Hard code heatmap disturb ratio, haven't tried other numbers.\n",
        "          ct2[0] = ct2[0] + np.random.randn() * 0.05 * w\n",
        "          ct2[1] = ct2[1] + np.random.randn() * 0.05 * h \n",
        "          ct2_int = ct2.astype(np.int32)\n",
        "          draw_umich_gaussian(pre_hm[0], ct2_int, radius, k=conf)\n",
        "\n",
        "    return pre_hm, pre_cts, track_ids\n",
        "\n",
        "  def _get_border(self, border, size):\n",
        "    i = 1\n",
        "    while size - border // i <= border // i:\n",
        "        i *= 2\n",
        "    return border // i\n",
        "\n",
        "\n",
        "  def _get_aug_param(self, c, s, width, height, disturb=False):\n",
        "    if (not self.opt.not_rand_crop) and not disturb:\n",
        "      aug_s = np.random.choice(np.arange(0.6, 1.4, 0.1))\n",
        "      w_border = self._get_border(128, width)\n",
        "      h_border = self._get_border(128, height)\n",
        "      c[0] = np.random.randint(low=w_border, high=width - w_border)\n",
        "      c[1] = np.random.randint(low=h_border, high=height - h_border)\n",
        "    else:\n",
        "      sf = self.opt.scale\n",
        "      cf = self.opt.shift\n",
        "      # if type(s) == float:\n",
        "      #   s = [s, s]\n",
        "      temp = np.random.randn()*cf\n",
        "      c[0] += s * np.clip(temp, -2*cf, 2*cf)\n",
        "      c[1] += s * np.clip(np.random.randn()*cf, -2*cf, 2*cf)\n",
        "      aug_s = np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)\n",
        "    \n",
        "    if np.random.random() < self.opt.aug_rot:\n",
        "      rf = self.opt.rotate\n",
        "      rot = np.clip(np.random.randn()*rf, -rf*2, rf*2)\n",
        "    else:\n",
        "      rot = 0\n",
        "    \n",
        "    return c, aug_s, rot\n",
        "\n",
        "\n",
        "  def _flip_anns(self, anns, width):\n",
        "    for k in range(len(anns)):\n",
        "      bbox = anns[k]['bbox']\n",
        "      anns[k]['bbox'] = [\n",
        "        width - bbox[0] - 1 - bbox[2], bbox[1], bbox[2], bbox[3]]\n",
        "      \n",
        "      if 'hps' in self.opt.heads and 'keypoints' in anns[k]:\n",
        "        keypoints = np.array(anns[k]['keypoints'], dtype=np.float32).reshape(\n",
        "          self.num_joints, 3)\n",
        "        keypoints[:, 0] = width - keypoints[:, 0] - 1\n",
        "        for e in self.flip_idx:\n",
        "          keypoints[e[0]], keypoints[e[1]] = \\\n",
        "            keypoints[e[1]].copy(), keypoints[e[0]].copy()\n",
        "        anns[k]['keypoints'] = keypoints.reshape(-1).tolist()\n",
        "\n",
        "      if 'rot' in self.opt.heads and 'alpha' in anns[k]:\n",
        "        anns[k]['alpha'] = np.pi - anns[k]['alpha'] if anns[k]['alpha'] > 0 \\\n",
        "                           else - np.pi - anns[k]['alpha']\n",
        "\n",
        "      if 'amodel_offset' in self.opt.heads and 'amodel_center' in anns[k]:\n",
        "        anns[k]['amodel_center'][0] = width - anns[k]['amodel_center'][0] - 1\n",
        "\n",
        "      if self.opt.velocity and 'velocity' in anns[k]:\n",
        "        # anns[k]['velocity'] = [-10000, -10000, -10000]\n",
        "        anns[k]['velocity'][0] *= -1\n",
        "\n",
        "    return anns\n",
        "\n",
        "\n",
        "\n",
        "  ## Load the Radar point cloud data\n",
        "  def _load_pc_data(self, img, img_info, inp_trans, out_trans, flipped=0):\n",
        "    img_height, img_width = img.shape[0], img.shape[1]\n",
        "    radar_pc = np.array(img_info.get('radar_pc', None))\n",
        "    if radar_pc is None:\n",
        "      return None, None, None, None\n",
        "\n",
        "    # calculate distance to points\n",
        "    depth = radar_pc[2,:]\n",
        "    \n",
        "    # filter points by distance\n",
        "    if self.opt.max_pc_dist > 0:\n",
        "      mask = (depth <= self.opt.max_pc_dist)\n",
        "      radar_pc = radar_pc[:,mask]\n",
        "      depth = depth[mask]\n",
        "\n",
        "    # add z offset to radar points\n",
        "    if self.opt.pc_z_offset != 0:\n",
        "      radar_pc[1,:] -= self.opt.pc_z_offset\n",
        "    \n",
        "    # map points to the image and filter ones outside\n",
        "    pc_2d, mask = map_pointcloud_to_image(radar_pc, np.array(img_info['camera_intrinsic']), \n",
        "                              img_shape=(img_info['width'],img_info['height']))\n",
        "    pc_3d = radar_pc[:,mask]\n",
        "\n",
        "    # sort points by distance\n",
        "    ind = np.argsort(pc_2d[2,:])\n",
        "    pc_2d = pc_2d[:,ind]\n",
        "    pc_3d = pc_3d[:,ind]\n",
        "\n",
        "    # flip points if image is flipped\n",
        "    if flipped:\n",
        "      pc_2d = self._flip_pc(pc_2d,  img_width)\n",
        "      pc_3d[0,:] *= -1  # flipping the x dimension\n",
        "      pc_3d[8,:] *= -1  # flipping x velocity (x is right, z is front)\n",
        "\n",
        "    pc_2d, pc_3d, pc_dep = self._process_pc(pc_2d, pc_3d, img, inp_trans, out_trans, img_info)\n",
        "    pc_N = np.array(pc_2d.shape[1])\n",
        "\n",
        "    # pad point clouds with zero to avoid size mismatch error in dataloader\n",
        "    n_points = min(self.opt.max_pc, pc_2d.shape[1])\n",
        "    pc_z = np.zeros((pc_2d.shape[0], self.opt.max_pc))\n",
        "    pc_z[:, :n_points] = pc_2d[:, :n_points]\n",
        "    pc_3dz = np.zeros((pc_3d.shape[0], self.opt.max_pc))\n",
        "    pc_3dz[:, :n_points] = pc_3d[:, :n_points]\n",
        "\n",
        "    return pc_z, pc_N, pc_dep, pc_3dz\n",
        "\n",
        "\n",
        "\n",
        "  def _process_pc(self, pc_2d, pc_3d, img, inp_trans, out_trans, img_info):    \n",
        "    img_height, img_width = img.shape[0], img.shape[1]\n",
        "\n",
        "    # transform points\n",
        "    mask = None\n",
        "    if len(self.opt.pc_feat_lvl) > 0:\n",
        "      pc_feat, mask = self._transform_pc(pc_2d, out_trans, self.opt.output_w, self.opt.output_h)\n",
        "      pc_hm_feat = np.zeros((len(self.opt.pc_feat_lvl), self.opt.output_h, self.opt.output_w), np.float32)\n",
        "    \n",
        "    if mask is not None:\n",
        "      pc_N = np.array(sum(mask))\n",
        "      pc_2d = pc_2d[:,mask]\n",
        "      pc_3d = pc_3d[:,mask]\n",
        "    else:\n",
        "      pc_N = pc_2d.shape[1]\n",
        "\n",
        "    # create point cloud pillars\n",
        "    if self.opt.pc_roi_method == \"pillars\":\n",
        "      pillar_wh = self.create_pc_pillars(img, img_info, pc_2d, pc_3d, inp_trans, out_trans)    \n",
        "\n",
        "    # generate point cloud channels\n",
        "    for i in range(pc_N-1, -1, -1):\n",
        "      for feat in self.opt.pc_feat_lvl:\n",
        "        point = pc_feat[:,i]\n",
        "        depth = point[2]\n",
        "        ct = np.array([point[0], point[1]])\n",
        "        ct_int = ct.astype(np.int32)\n",
        "\n",
        "        if self.opt.pc_roi_method == \"pillars\":\n",
        "          wh = pillar_wh[:,i]\n",
        "          b = [max(ct[1]-wh[1], 0), \n",
        "              ct[1], \n",
        "              max(ct[0]-wh[0]/2, 0), \n",
        "              min(ct[0]+wh[0]/2, self.opt.output_w)]\n",
        "          b = np.round(b).astype(np.int32)\n",
        "        \n",
        "        elif self.opt.pc_roi_method == \"hm\":\n",
        "          radius = (1.0 / depth) * self.opt.r_a + self.opt.r_b\n",
        "          radius = gaussian_radius((radius, radius))\n",
        "          radius = max(0, int(radius))\n",
        "          x, y = ct_int[0], ct_int[1]\n",
        "          height, width = pc_hm_feat.shape[1:3]\n",
        "          left, right = min(x, radius), min(width - x, radius + 1)\n",
        "          top, bottom = min(y, radius), min(height - y, radius + 1)\n",
        "          b = np.array([y - top, y + bottom, x - left, x + right])\n",
        "          b = np.round(b).astype(np.int32)\n",
        "        \n",
        "        if feat == 'pc_dep':\n",
        "          channel = self.opt.pc_feat_channels['pc_dep']\n",
        "          pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = depth\n",
        "        \n",
        "        if feat == 'pc_vx':\n",
        "          vx = pc_3d[8,i]\n",
        "          channel = self.opt.pc_feat_channels['pc_vx']\n",
        "          pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = vx\n",
        "        \n",
        "        if feat == 'pc_vz':\n",
        "          vz = pc_3d[9,i]\n",
        "          channel = self.opt.pc_feat_channels['pc_vz']\n",
        "          pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = vz\n",
        "\n",
        "    return pc_2d, pc_3d, pc_hm_feat\n",
        "\n",
        "\n",
        "  def create_pc_pillars(self, img, img_info, pc_2d, pc_3d, inp_trans, out_trans):\n",
        "    pillar_wh = np.zeros((2, pc_3d.shape[1]))\n",
        "    boxes_2d = np.zeros((0,8,2))\n",
        "    pillar_dim = self.opt.pillar_dims\n",
        "    v = np.dot(np.eye(3), np.array([1,0,0]))\n",
        "    ry = -np.arctan2(v[2], v[0])\n",
        "\n",
        "    for i, center in enumerate(pc_3d[:3,:].T):\n",
        "      # Create a 3D pillar at pc location for the full-size image\n",
        "      box_3d = compute_box_3d(dim=pillar_dim, location=center, rotation_y=ry)\n",
        "      box_2d = project_to_image(box_3d, img_info['calib']).T  # [2x8]        \n",
        "      \n",
        "      ## save the box for debug plots\n",
        "      if self.opt.debug:\n",
        "        box_2d_img, m = self._transform_pc(box_2d, inp_trans, self.opt.input_w, \n",
        "                                            self.opt.input_h, filter_out=False)\n",
        "        boxes_2d = np.concatenate((boxes_2d, np.expand_dims(box_2d_img.T,0)),0)\n",
        "\n",
        "      # transform points\n",
        "      box_2d_t, m = self._transform_pc(box_2d, out_trans, self.opt.output_w, self.opt.output_h)\n",
        "      \n",
        "      if box_2d_t.shape[1] <= 1:\n",
        "        continue\n",
        "\n",
        "      # get the bounding box in [xyxy] format\n",
        "      bbox = [np.min(box_2d_t[0,:]), \n",
        "              np.min(box_2d_t[1,:]), \n",
        "              np.max(box_2d_t[0,:]), \n",
        "              np.max(box_2d_t[1,:])] # format: xyxy\n",
        "\n",
        "      # store height and width of the 2D box\n",
        "      pillar_wh[0,i] = bbox[2] - bbox[0]\n",
        "      pillar_wh[1,i] = bbox[3] - bbox[1]\n",
        "\n",
        "    ## DEBUG #################################################################\n",
        "    if self.opt.debug:\n",
        "      img_2d = copy.deepcopy(img)\n",
        "      # img_3d = copy.deepcopy(img)\n",
        "      img_2d_inp = cv2.warpAffine(img, inp_trans, \n",
        "                        (self.opt.input_w, self.opt.input_h),\n",
        "                        flags=cv2.INTER_LINEAR)\n",
        "      img_2d_out = cv2.warpAffine(img, out_trans, \n",
        "                        (self.opt.output_w, self.opt.output_h),\n",
        "                        flags=cv2.INTER_LINEAR)\n",
        "      img_3d = cv2.warpAffine(img, inp_trans, \n",
        "                        (self.opt.input_w, self.opt.input_h),\n",
        "                        flags=cv2.INTER_LINEAR)\n",
        "      blank_image = 255*np.ones((self.opt.input_h,self.opt.input_w,3), np.uint8)\n",
        "      overlay = img_2d_inp.copy()\n",
        "      output = img_2d_inp.copy()\n",
        "\n",
        "      pc_inp, _= self._transform_pc(pc_2d, inp_trans, self.opt.input_w, self.opt.input_h)\n",
        "      pc_out, _= self._transform_pc(pc_2d, out_trans, self.opt.output_w, self.opt.output_h)\n",
        "\n",
        "      pill_wh_inp = pillar_wh * (self.opt.input_w/self.opt.output_w)\n",
        "      pill_wh_out = pillar_wh\n",
        "      pill_wh_ori = pill_wh_inp * 2\n",
        "      \n",
        "      for i, p in enumerate(pc_inp[:3,:].T):\n",
        "        color = int((p[2].tolist()/60.0)*255)\n",
        "        color = (0,color,0)\n",
        "        \n",
        "        rect_tl = (np.min(int(p[0]-pill_wh_inp[0,i]/2), 0), np.min(int(p[1]-pill_wh_inp[1,i]),0))\n",
        "        rect_br = (np.min(int(p[0]+pill_wh_inp[0,i]/2), 0), int(p[1]))\n",
        "        cv2.rectangle(img_2d_inp, rect_tl, rect_br, (0, 0, 255), 1, lineType=cv2.LINE_AA)\n",
        "        img_2d_inp = cv2.circle(img_2d_inp, (int(p[0]), int(p[1])), 3, color, -1)\n",
        "\n",
        "        ## On original-sized image\n",
        "        rect_tl_ori = (np.min(int(pc_2d[0,i]-pill_wh_ori[0,i]/2), 0), np.min(int(pc_2d[1,i]-pill_wh_ori[1,i]),0))\n",
        "        rect_br_ori = (np.min(int(pc_2d[0,i]+pill_wh_ori[0,i]/2), 0), int(pc_2d[1,i]))\n",
        "        cv2.rectangle(img_2d, rect_tl_ori, rect_br_ori, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
        "        img_2d = cv2.circle(img_2d, (int(pc_2d[0,i]), int(pc_2d[1,i])), 6, color, -1)\n",
        "        \n",
        "        p2 = pc_out[:3,i].T\n",
        "        rect_tl2 = (np.min(int(p2[0]-pill_wh_out[0,i]/2), 0), np.min(int(p2[1]-pill_wh_out[1,i]),0))\n",
        "        rect_br2 = (np.min(int(p2[0]+pill_wh_out[0,i]/2), 0), int(p2[1]))\n",
        "        cv2.rectangle(img_2d_out, rect_tl2, rect_br2, (0, 0, 255), 1, lineType=cv2.LINE_AA)\n",
        "        img_2d_out = cv2.circle(img_2d_out, (int(p[0]), int(p[1])), 3, (255,0,0), -1)\n",
        "        \n",
        "        # on blank image\n",
        "        cv2.rectangle(blank_image, rect_tl, rect_br, color, -1, lineType=cv2.LINE_AA)\n",
        "        \n",
        "        # overlay\n",
        "        alpha = 0.1\n",
        "        cv2.rectangle(overlay, rect_tl, rect_br, color, -1, lineType=cv2.LINE_AA)\n",
        "        cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n",
        "\n",
        "        # plot 3d pillars\n",
        "        img_3d = draw_box_3d(img_3d, boxes_2d[i].astype(np.int32), [114, 159, 207], \n",
        "                    same_color=False)\n",
        "\n",
        "      cv2.imwrite((self.opt.debug_dir+ '/{}pc_pillar_2d_inp.' + self.opt.img_format)\\\n",
        "        .format(self.img_ind), img_2d_inp)\n",
        "      cv2.imwrite((self.opt.debug_dir+ '/{}pc_pillar_2d_ori.' + self.opt.img_format)\\\n",
        "        .format(self.img_ind), img_2d)\n",
        "      cv2.imwrite((self.opt.debug_dir+ '/{}pc_pillar_2d_out.' + self.opt.img_format)\\\n",
        "        .format(self.img_ind), img_2d_out)\n",
        "      cv2.imwrite((self.opt.debug_dir+'/{}pc_pillar_2d_blank.'+ self.opt.img_format)\\\n",
        "        .format(self.img_ind), blank_image)\n",
        "      cv2.imwrite((self.opt.debug_dir+'/{}pc_pillar_2d_overlay.'+ self.opt.img_format)\\\n",
        "        .format(self.img_ind), output)\n",
        "      cv2.imwrite((self.opt.debug_dir+'/{}pc_pillar_3d.'+ self.opt.img_format)\\\n",
        "        .format(self.img_ind), img_3d)\n",
        "      self.img_ind += 1\n",
        "    ## DEBUG #################################################################\n",
        "    return pillar_wh\n",
        "\n",
        "\n",
        "  def _flip_pc(self, pc_2d, width):\n",
        "    pc_2d[0,:] = width - 1 - pc_2d[0,:]\n",
        "    return pc_2d\n",
        "  \n",
        "\n",
        "    # Transform points to image or feature space with augmentation\n",
        "    #  Inputs:\n",
        "    # pc_2d: [3xN]\n",
        "  def _transform_pc(self, pc_2d, trans, img_width, img_height, filter_out=True):\n",
        "\n",
        "    if pc_2d.shape[1] == 0:\n",
        "      return pc_2d, []\n",
        "\n",
        "    pc_t = np.expand_dims(pc_2d[:2,:].T, 0)   # [3,N] -> [1,N,2]\n",
        "    t_points = cv2.transform(pc_t, trans)\n",
        "    t_points = np.squeeze(t_points,0).T       # [1,N,2] -> [2,N]\n",
        "    \n",
        "    # remove points outside image\n",
        "    if filter_out:\n",
        "      mask = (t_points[0,:]<img_width) \\\n",
        "              & (t_points[1,:]<img_height) \\\n",
        "              & (0<t_points[0,:]) \\\n",
        "              & (0<t_points[1,:])\n",
        "      out = np.concatenate((t_points[:,mask], pc_2d[2:,mask]), axis=0)\n",
        "    else:\n",
        "      mask = None\n",
        "      out = np.concatenate((t_points, pc_2d[2:,:]), axis=0)\n",
        "\n",
        "    return out, mask\n",
        "\n",
        "\n",
        "  ## Augment, resize and normalize the image\n",
        "  def _get_input(self, img, trans_input):\n",
        "    inp = cv2.warpAffine(img, trans_input, \n",
        "                        (self.opt.input_w, self.opt.input_h),\n",
        "                        flags=cv2.INTER_LINEAR)\n",
        "    \n",
        "    inp = (inp.astype(np.float32) / 255.)\n",
        "    if 'train' in self.split and not self.opt.no_color_aug:\n",
        "      color_aug(self._data_rng, inp, self._eig_val, self._eig_vec)\n",
        "    inp = (inp - self.mean) / self.std\n",
        "    inp = inp.transpose(2, 0, 1)\n",
        "    return inp\n",
        "\n",
        "\n",
        "  def _init_ret(self, ret, gt_det):\n",
        "    max_objs = self.max_objs * self.opt.dense_reg\n",
        "    ret['hm'] = np.zeros(\n",
        "      (self.opt.num_classes, self.opt.output_h, self.opt.output_w), \n",
        "      np.float32)\n",
        "    ret['ind'] = np.zeros((max_objs), dtype=np.int64)\n",
        "    ret['cat'] = np.zeros((max_objs), dtype=np.int64)\n",
        "    ret['mask'] = np.zeros((max_objs), dtype=np.float32)\n",
        "    \n",
        "    if self.opt.pointcloud:\n",
        "      ret['pc_hm'] = np.zeros(\n",
        "        (len(self.opt.pc_feat_lvl), self.opt.output_h, self.opt.output_w), \n",
        "        np.float32)\n",
        "\n",
        "    regression_head_dims = {\n",
        "      'reg': 2, 'wh': 2, 'tracking': 2, 'ltrb': 4, 'ltrb_amodal': 4, \n",
        "      'nuscenes_att': 8, 'velocity': 3, 'hps': self.num_joints * 2, \n",
        "      'dep': 1, 'dim': 3, 'amodel_offset': 2 }\n",
        "\n",
        "    for head in regression_head_dims:\n",
        "      if head in self.opt.heads:\n",
        "        ret[head] = np.zeros(\n",
        "          (max_objs, regression_head_dims[head]), dtype=np.float32)\n",
        "        ret[head + '_mask'] = np.zeros(\n",
        "          (max_objs, regression_head_dims[head]), dtype=np.float32)\n",
        "        gt_det[head] = []\n",
        "    # if self.opt.pointcloud:\n",
        "    #     ret['pc_dep_mask'] = np.zeros((max_objs, 1), dtype=np.float32)\n",
        "    #     ret['pc_dep'] = np.zeros((max_objs, 1), dtype=np.float32)\n",
        "    #     gt_det['pc_dep'] = []\n",
        "\n",
        "    if 'hm_hp' in self.opt.heads:\n",
        "      num_joints = self.num_joints\n",
        "      ret['hm_hp'] = np.zeros(\n",
        "        (num_joints, self.opt.output_h, self.opt.output_w), dtype=np.float32)\n",
        "      ret['hm_hp_mask'] = np.zeros(\n",
        "        (max_objs * num_joints), dtype=np.float32)\n",
        "      ret['hp_offset'] = np.zeros(\n",
        "        (max_objs * num_joints, 2), dtype=np.float32)\n",
        "      ret['hp_ind'] = np.zeros((max_objs * num_joints), dtype=np.int64)\n",
        "      ret['hp_offset_mask'] = np.zeros(\n",
        "        (max_objs * num_joints, 2), dtype=np.float32)\n",
        "      ret['joint'] = np.zeros((max_objs * num_joints), dtype=np.int64)\n",
        "    \n",
        "    if 'rot' in self.opt.heads:\n",
        "      ret['rotbin'] = np.zeros((max_objs, 2), dtype=np.int64)\n",
        "      ret['rotres'] = np.zeros((max_objs, 2), dtype=np.float32)\n",
        "      ret['rot_mask'] = np.zeros((max_objs), dtype=np.float32)\n",
        "      gt_det.update({'rot': []})\n",
        "\n",
        "\n",
        "  def _get_calib(self, img_info, width, height):\n",
        "    if 'calib' in img_info:\n",
        "      calib = np.array(img_info['calib'], dtype=np.float32)\n",
        "    else:\n",
        "      calib = np.array([[self.rest_focal_length, 0, width / 2, 0], \n",
        "                        [0, self.rest_focal_length, height / 2, 0], \n",
        "                        [0, 0, 1, 0]])\n",
        "    return calib\n",
        "\n",
        "\n",
        "  def _ignore_region(self, region, ignore_val=1):\n",
        "    np.maximum(region, ignore_val, out=region)\n",
        "\n",
        "\n",
        "  def _mask_ignore_or_crowd(self, ret, cls_id, bbox):\n",
        "    # mask out crowd region, only rectangular mask is supported\n",
        "    if cls_id == 0: # ignore all classes\n",
        "      self._ignore_region(ret['hm'][:, int(bbox[1]): int(bbox[3]) + 1, \n",
        "                                        int(bbox[0]): int(bbox[2]) + 1])\n",
        "    else:\n",
        "      # mask out one specific class\n",
        "      self._ignore_region(ret['hm'][abs(cls_id) - 1, \n",
        "                                    int(bbox[1]): int(bbox[3]) + 1, \n",
        "                                    int(bbox[0]): int(bbox[2]) + 1])\n",
        "    if ('hm_hp' in ret) and cls_id <= 1:\n",
        "      self._ignore_region(ret['hm_hp'][:, int(bbox[1]): int(bbox[3]) + 1, \n",
        "                                          int(bbox[0]): int(bbox[2]) + 1])\n",
        "\n",
        "\n",
        "  def _coco_box_to_bbox(self, box):\n",
        "    bbox = np.array([box[0], box[1], box[0] + box[2], box[1] + box[3]],\n",
        "                    dtype=np.float32)\n",
        "    return bbox\n",
        "\n",
        "\n",
        "  def _get_bbox_output(self, bbox, trans_output, height, width):\n",
        "    bbox = self._coco_box_to_bbox(bbox).copy()\n",
        "\n",
        "    rect = np.array([[bbox[0], bbox[1]], [bbox[0], bbox[3]],\n",
        "                    [bbox[2], bbox[3]], [bbox[2], bbox[1]]], dtype=np.float32)\n",
        "    for t in range(4):\n",
        "      rect[t] =  affine_transform(rect[t], trans_output)\n",
        "    bbox[:2] = rect[:, 0].min(), rect[:, 1].min()\n",
        "    bbox[2:] = rect[:, 0].max(), rect[:, 1].max()\n",
        "\n",
        "    bbox_amodal = copy.deepcopy(bbox)\n",
        "    bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, self.opt.output_w - 1)\n",
        "    bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, self.opt.output_h - 1)\n",
        "    h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]\n",
        "    return bbox, bbox_amodal\n",
        "\n",
        "\n",
        "  def _add_instance(\n",
        "    self, ret, gt_det, k, cls_id, bbox, bbox_amodal, ann, trans_output,\n",
        "    aug_s, calib, pre_cts=None, track_ids=None):\n",
        "    h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]\n",
        "    if h <= 0 or w <= 0:\n",
        "      return\n",
        "    radius = gaussian_radius((math.ceil(h), math.ceil(w)))\n",
        "    radius = max(0, int(radius)) \n",
        "    ct = np.array(\n",
        "      [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)\n",
        "    ct_int = ct.astype(np.int32)\n",
        "    ret['cat'][k] = cls_id - 1\n",
        "    ret['mask'][k] = 1\n",
        "    if 'wh' in ret:\n",
        "      ret['wh'][k] = 1. * w, 1. * h\n",
        "      ret['wh_mask'][k] = 1\n",
        "    ret['ind'][k] = ct_int[1] * self.opt.output_w + ct_int[0]\n",
        "    ret['reg'][k] = ct - ct_int\n",
        "    ret['reg_mask'][k] = 1\n",
        "    draw_umich_gaussian(ret['hm'][cls_id - 1], ct_int, radius)\n",
        "\n",
        "    gt_det['bboxes'].append(\n",
        "      np.array([ct[0] - w / 2, ct[1] - h / 2,\n",
        "                ct[0] + w / 2, ct[1] + h / 2], dtype=np.float32))\n",
        "    gt_det['scores'].append(1)\n",
        "    gt_det['clses'].append(cls_id - 1)\n",
        "    gt_det['cts'].append(ct)\n",
        "\n",
        "    if 'tracking' in self.opt.heads:\n",
        "      if ann['track_id'] in track_ids:\n",
        "        pre_ct = pre_cts[track_ids.index(ann['track_id'])]\n",
        "        ret['tracking_mask'][k] = 1\n",
        "        ret['tracking'][k] = pre_ct - ct_int\n",
        "        gt_det['tracking'].append(ret['tracking'][k])\n",
        "      else:\n",
        "        gt_det['tracking'].append(np.zeros(2, np.float32))\n",
        "\n",
        "    if 'ltrb' in self.opt.heads:\n",
        "      ret['ltrb'][k] = bbox[0] - ct_int[0], bbox[1] - ct_int[1], \\\n",
        "        bbox[2] - ct_int[0], bbox[3] - ct_int[1]\n",
        "      ret['ltrb_mask'][k] = 1\n",
        "\n",
        "    ## ltrb_amodal is to use the left, top, right, bottom bounding box representation \n",
        "    # to enable detecting out-of-image bounding box (important for MOT datasets)\n",
        "    if 'ltrb_amodal' in self.opt.heads:\n",
        "      ret['ltrb_amodal'][k] = \\\n",
        "        bbox_amodal[0] - ct_int[0], bbox_amodal[1] - ct_int[1], \\\n",
        "        bbox_amodal[2] - ct_int[0], bbox_amodal[3] - ct_int[1]\n",
        "      ret['ltrb_amodal_mask'][k] = 1\n",
        "      gt_det['ltrb_amodal'].append(bbox_amodal)\n",
        "\n",
        "    if 'nuscenes_att' in self.opt.heads:\n",
        "      if ('attributes' in ann) and ann['attributes'] > 0:\n",
        "        att = int(ann['attributes'] - 1)\n",
        "        ret['nuscenes_att'][k][att] = 1\n",
        "        ret['nuscenes_att_mask'][k][self.nuscenes_att_range[att]] = 1\n",
        "      gt_det['nuscenes_att'].append(ret['nuscenes_att'][k])\n",
        "\n",
        "    if 'velocity' in self.opt.heads:\n",
        "      if ('velocity_cam' in ann) and min(ann['velocity_cam']) > -1000:\n",
        "        ret['velocity'][k] = np.array(ann['velocity_cam'], np.float32)[:3]\n",
        "        ret['velocity_mask'][k] = 1\n",
        "      gt_det['velocity'].append(ret['velocity'][k])\n",
        "\n",
        "    if 'hps' in self.opt.heads:\n",
        "      self._add_hps(ret, k, ann, gt_det, trans_output, ct_int, bbox, h, w)\n",
        "\n",
        "    if 'rot' in self.opt.heads:\n",
        "      self._add_rot(ret, ann, k, gt_det)\n",
        "\n",
        "    if 'dep' in self.opt.heads:\n",
        "      if 'depth' in ann:\n",
        "        ret['dep_mask'][k] = 1\n",
        "        ret['dep'][k] = ann['depth'] * aug_s\n",
        "        gt_det['dep'].append(ret['dep'][k])\n",
        "      else:\n",
        "        gt_det['dep'].append(2)\n",
        "\n",
        "    if 'dim' in self.opt.heads:\n",
        "      if 'dim' in ann:\n",
        "        ret['dim_mask'][k] = 1\n",
        "        ret['dim'][k] = ann['dim']\n",
        "        gt_det['dim'].append(ret['dim'][k])\n",
        "      else:\n",
        "        gt_det['dim'].append([1,1,1])\n",
        "    \n",
        "    if 'amodel_offset' in self.opt.heads:\n",
        "      if 'amodel_center' in ann:\n",
        "        amodel_center = affine_transform(ann['amodel_center'], trans_output)\n",
        "        ret['amodel_offset_mask'][k] = 1\n",
        "        ret['amodel_offset'][k] = amodel_center - ct_int\n",
        "        gt_det['amodel_offset'].append(ret['amodel_offset'][k])\n",
        "      else:\n",
        "        gt_det['amodel_offset'].append([0, 0])\n",
        "    \n",
        "    if self.opt.pointcloud:\n",
        "      ## get pointcloud heatmap\n",
        "      if self.opt.disable_frustum:\n",
        "        ret['pc_hm'] = ret['pc_dep']\n",
        "        if opt.normalize_depth:\n",
        "          ret['pc_hm'][self.opt.pc_feat_channels['pc_dep']] /= opt.max_pc_dist\n",
        "      else:\n",
        "        dist_thresh = get_dist_thresh(calib, ct, ann['dim'], ann['alpha'])\n",
        "        pc_dep_to_hm(ret['pc_hm'], ret['pc_dep'], ann['depth'], bbox, dist_thresh, self.opt)\n",
        "    \n",
        "    \n",
        "\n",
        "  def _add_hps(self, ret, k, ann, gt_det, trans_output, ct_int, bbox, h, w):\n",
        "    num_joints = self.num_joints\n",
        "    pts = np.array(ann['keypoints'], np.float32).reshape(num_joints, 3) \\\n",
        "        if 'keypoints' in ann else np.zeros((self.num_joints, 3), np.float32)\n",
        "    if self.opt.simple_radius > 0:\n",
        "      hp_radius = int(simple_radius(h, w, min_overlap=self.opt.simple_radius))\n",
        "    else:\n",
        "      hp_radius = gaussian_radius((math.ceil(h), math.ceil(w)))\n",
        "      hp_radius = max(0, int(hp_radius))\n",
        "\n",
        "    for j in range(num_joints):\n",
        "      pts[j, :2] = affine_transform(pts[j, :2], trans_output)\n",
        "      if pts[j, 2] > 0:\n",
        "        if pts[j, 0] >= 0 and pts[j, 0] < self.opt.output_w and \\\n",
        "          pts[j, 1] >= 0 and pts[j, 1] < self.opt.output_h:\n",
        "          ret['hps'][k, j * 2: j * 2 + 2] = pts[j, :2] - ct_int\n",
        "          ret['hps_mask'][k, j * 2: j * 2 + 2] = 1\n",
        "          pt_int = pts[j, :2].astype(np.int32)\n",
        "          ret['hp_offset'][k * num_joints + j] = pts[j, :2] - pt_int\n",
        "          ret['hp_ind'][k * num_joints + j] = \\\n",
        "            pt_int[1] * self.opt.output_w + pt_int[0]\n",
        "          ret['hp_offset_mask'][k * num_joints + j] = 1\n",
        "          ret['hm_hp_mask'][k * num_joints + j] = 1\n",
        "          ret['joint'][k * num_joints + j] = j\n",
        "          draw_umich_gaussian(\n",
        "            ret['hm_hp'][j], pt_int, hp_radius)\n",
        "          if pts[j, 2] == 1:\n",
        "            ret['hm_hp'][j, pt_int[1], pt_int[0]] = self.ignore_val\n",
        "            ret['hp_offset_mask'][k * num_joints + j] = 0\n",
        "            ret['hm_hp_mask'][k * num_joints + j] = 0\n",
        "        else:\n",
        "          pts[j, :2] *= 0\n",
        "      else:\n",
        "        pts[j, :2] *= 0\n",
        "        self._ignore_region(\n",
        "          ret['hm_hp'][j, int(bbox[1]): int(bbox[3]) + 1, \n",
        "                          int(bbox[0]): int(bbox[2]) + 1])\n",
        "    gt_det['hps'].append(pts[:, :2].reshape(num_joints * 2))\n",
        "\n",
        "  def _add_rot(self, ret, ann, k, gt_det):\n",
        "    if 'alpha' in ann:\n",
        "      ret['rot_mask'][k] = 1\n",
        "      alpha = ann['alpha']\n",
        "      if alpha < np.pi / 6. or alpha > 5 * np.pi / 6.:\n",
        "        ret['rotbin'][k, 0] = 1\n",
        "        ret['rotres'][k, 0] = alpha - (-0.5 * np.pi)    \n",
        "      if alpha > -np.pi / 6. or alpha < -5 * np.pi / 6.:\n",
        "        ret['rotbin'][k, 1] = 1\n",
        "        ret['rotres'][k, 1] = alpha - (0.5 * np.pi)\n",
        "      gt_det['rot'].append(self._alpha_to_8(ann['alpha']))\n",
        "    else:\n",
        "      gt_det['rot'].append(self._alpha_to_8(0))\n",
        "    \n",
        "  def _alpha_to_8(self, alpha):\n",
        "    ret = [0, 0, 0, 1, 0, 0, 0, 1]\n",
        "    if alpha < np.pi / 6. or alpha > 5 * np.pi / 6.:\n",
        "      r = alpha - (-0.5 * np.pi)\n",
        "      ret[1] = 1\n",
        "      ret[2], ret[3] = np.sin(r), np.cos(r)\n",
        "    if alpha > -np.pi / 6. or alpha < -5 * np.pi / 6.:\n",
        "      r = alpha - (0.5 * np.pi)\n",
        "      ret[5] = 1\n",
        "      ret[6], ret[7] = np.sin(r), np.cos(r)\n",
        "    return ret\n",
        "  \n",
        "  def _format_gt_det(self, gt_det):\n",
        "    if (len(gt_det['scores']) == 0):\n",
        "      gt_det = {'bboxes': np.array([[0,0,1,1]], dtype=np.float32), \n",
        "                'scores': np.array([1], dtype=np.float32), \n",
        "                'clses': np.array([0], dtype=np.float32),\n",
        "                'cts': np.array([[0, 0]], dtype=np.float32),\n",
        "                'pre_cts': np.array([[0, 0]], dtype=np.float32),\n",
        "                'tracking': np.array([[0, 0]], dtype=np.float32),\n",
        "                'bboxes_amodal': np.array([[0, 0]], dtype=np.float32),\n",
        "                'hps': np.zeros((1, 17, 2), dtype=np.float32),}\n",
        "    gt_det = {k: np.array(gt_det[k], dtype=np.float32) for k in gt_det}\n",
        "    return gt_det\n",
        "\n",
        "  def fake_video_data(self):\n",
        "    self.coco.dataset['videos'] = []\n",
        "    for i in range(len(self.coco.dataset['images'])):\n",
        "      img_id = self.coco.dataset['images'][i]['id']\n",
        "      self.coco.dataset['images'][i]['video_id'] = img_id\n",
        "      self.coco.dataset['images'][i]['frame_id'] = 1\n",
        "      self.coco.dataset['videos'].append({'id': img_id})\n",
        "    \n",
        "    if not ('annotations' in self.coco.dataset):\n",
        "      return\n",
        "\n",
        "    for i in range(len(self.coco.dataset['annotations'])):\n",
        "      self.coco.dataset['annotations'][i]['track_id'] = i + 1\n"
      ],
      "metadata": {
        "id": "ZwD-8f4F0p2n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### NuScenes"
      ],
      "metadata": {
        "id": "i5Rn-1_w7i5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Xingyi Zhou. All Rights Reserved\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import pycocotools.coco as coco\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pyquaternion import Quaternion\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from ..generic_dataset import GenericDataset\n",
        "# from utils.ddd_utils import compute_box_3d, project_to_image, iou3d_global\n",
        "from nuscenes.utils.geometry_utils import view_points\n",
        "from nuscenes.utils.data_classes import Box\n",
        "from itertools import compress\n",
        "\n",
        "class nuScenes(GenericDataset):\n",
        "  default_resolution = [448, 800]\n",
        "  num_categories = 10\n",
        "  class_name = [\n",
        "    'car', 'truck', 'bus', 'trailer', \n",
        "    'construction_vehicle', 'pedestrian', 'motorcycle', 'bicycle',\n",
        "    'traffic_cone', 'barrier']\n",
        "  cat_ids = {i + 1: i + 1 for i in range(num_categories)}\n",
        "  focal_length = 1200\n",
        "  max_objs = 128\n",
        "  _tracking_ignored_class = ['construction_vehicle', 'traffic_cone', 'barrier']\n",
        "  _vehicles = ['car', 'truck', 'bus', 'trailer', 'construction_vehicle']\n",
        "  _cycles = ['motorcycle', 'bicycle']\n",
        "  _pedestrians = ['pedestrian']\n",
        "  attribute_to_id = {\n",
        "    '': 0, 'cycle.with_rider' : 1, 'cycle.without_rider' : 2,\n",
        "    'pedestrian.moving': 3, 'pedestrian.standing': 4, \n",
        "    'pedestrian.sitting_lying_down': 5,\n",
        "    'vehicle.moving': 6, 'vehicle.parked': 7, \n",
        "    'vehicle.stopped': 8}\n",
        "  id_to_attribute = {v: k for k, v in attribute_to_id.items()}\n",
        "\n",
        "\n",
        "  def __init__(self, opt, split):\n",
        "    split_names = {\n",
        "        'mini_train':'mini_train', \n",
        "        'mini_val':'mini_val',\n",
        "        'train': 'train', \n",
        "        'train_detect': 'train_detect',\n",
        "        'train_track':'train_track', \n",
        "        'val': 'val',\n",
        "        'test': 'test',\n",
        "        'mini_train_2': 'mini_train_2',\n",
        "        'trainval': 'trainval',\n",
        "    }\n",
        "    \n",
        "    split_name = split_names[split] \n",
        "    data_dir = os.path.join(opt.data_dir, 'nuscenes')\n",
        "    print('Dataset version', opt.dataset_version)\n",
        "    \n",
        "    anns_dir = 'annotations'\n",
        "    if opt.radar_sweeps > 1:\n",
        "      anns_dir += '_{}sweeps'.format(opt.radar_sweeps)\n",
        "\n",
        "    if opt.dataset_version == 'test':\n",
        "      ann_path = os.path.join(data_dir, anns_dir, 'test.json')\n",
        "    else:\n",
        "      ann_path = os.path.join(data_dir, anns_dir, '{}.json').format(split_name)\n",
        "\n",
        "    self.images = None\n",
        "    super(nuScenes, self).__init__(opt, split, ann_path, data_dir)\n",
        "\n",
        "    self.alpha_in_degree = False    \n",
        "    self.num_samples = len(self.images)\n",
        "\n",
        "    print('Loaded {} {} samples'.format(split, self.num_samples))\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_samples\n",
        "\n",
        "\n",
        "  def _to_float(self, x):\n",
        "    return float(\"{:.2f}\".format(x))\n",
        "\n",
        "\n",
        "  def convert_coco_format(self, all_bboxes):\n",
        "    detections = []\n",
        "    for image_id in all_bboxes:\n",
        "      if type(all_bboxes[image_id]) != type({}):\n",
        "        # newest format\n",
        "        for j in range(len(all_bboxes[image_id])):\n",
        "          item = all_bboxes[image_id][j]   \n",
        "          category_id = citem['class']\n",
        "          bbox = item['bbox']\n",
        "          bbox[2] -= bbox[0]\n",
        "          bbox[3] -= bbox[1]\n",
        "          bbox_out  = list(map(self._to_float, bbox[0:4]))\n",
        "          detection = {\n",
        "              \"image_id\": int(image_id),\n",
        "              \"category_id\": int(category_id),\n",
        "              \"bbox\": bbox_out,\n",
        "              \"score\": float(\"{:.2f}\".format(item['score']))\n",
        "          }\n",
        "          detections.append(detection)\n",
        "    return detections\n",
        "\n",
        "\n",
        "  def convert_eval_format(self, results):\n",
        "    ret = {'meta': {'use_camera': True, 'use_lidar': False, \n",
        "      'use_radar': self.opt.pointcloud, \n",
        "      'use_map': False, 'use_external': False}, 'results': {}}\n",
        "    print('Converting nuscenes format...')\n",
        "    for image_id in self.images:\n",
        "      if not (image_id in results):\n",
        "        continue\n",
        "      image_info = self.coco.loadImgs(ids=[image_id])[0]\n",
        "      sample_token = image_info['sample_token']\n",
        "      trans_matrix = np.array(image_info['trans_matrix'], np.float32)\n",
        "      velocity_mat = np.array(image_info['velocity_trans_matrix'], np.float32)\n",
        "      sensor_id = image_info['sensor_id']\n",
        "      sample_results = []\n",
        "      for item in results[image_id]:\n",
        "        class_name = self.class_name[int(item['class'] - 1)] \\\n",
        "            if not ('detection_name' in item) else item['detection_name']\n",
        "        if self.opt.tracking and class_name in self._tracking_ignored_class:\n",
        "          continue\n",
        "        score = float(item['score']) \\\n",
        "            if not ('detection_score' in item) else item['detection_score']\n",
        "        if 'size' in item:\n",
        "          size = item['size']\n",
        "        else:\n",
        "          size = [float(item['dim'][1]), float(item['dim'][2]), \\\n",
        "            float(item['dim'][0])]\n",
        "        if 'translation' in item:\n",
        "          translation = item['translation']\n",
        "        else:\n",
        "          translation = np.dot(trans_matrix, np.array(\n",
        "            [item['loc'][0], item['loc'][1] - size[2], item['loc'][2], 1], \n",
        "            np.float32))\n",
        "\n",
        "        det_id = item['det_id'] if 'det_id' in item else -1\n",
        "        tracking_id = item['tracking_id'] if 'tracking_id' in item else 1\n",
        "        \n",
        "        if not ('rotation' in item):\n",
        "          rot_cam = Quaternion(\n",
        "            axis=[0, 1, 0], angle=item['rot_y'])\n",
        "          loc = np.array(\n",
        "            [item['loc'][0], item['loc'][1], item['loc'][2]], np.float32)\n",
        "          box = Box(loc, size, rot_cam, name='2', token='1')\n",
        "          box.translate(np.array([0, - box.wlh[2] / 2, 0]))\n",
        "          box.rotate(Quaternion(image_info['cs_record_rot']))\n",
        "          box.translate(np.array(image_info['cs_record_trans']))\n",
        "          box.rotate(Quaternion(image_info['pose_record_rot']))\n",
        "          box.translate(np.array(image_info['pose_record_trans']))\n",
        "          rotation = box.orientation\n",
        "          rotation = [float(rotation.w), float(rotation.x), \\\n",
        "            float(rotation.y), float(rotation.z)]\n",
        "        else:\n",
        "           rotation = item['rotation']\n",
        "        \n",
        "        nuscenes_att = np.array(item['nuscenes_att'], np.float32) \\\n",
        "          if 'nuscenes_att' in item else np.zeros(8, np.float32)\n",
        "        att = ''\n",
        "        if class_name in self._cycles:\n",
        "          att = self.id_to_attribute[np.argmax(nuscenes_att[0:2]) + 1]\n",
        "        elif class_name in self._pedestrians:\n",
        "          att = self.id_to_attribute[np.argmax(nuscenes_att[2:5]) + 3]\n",
        "        elif class_name in self._vehicles:\n",
        "          att = self.id_to_attribute[np.argmax(nuscenes_att[5:8]) + 6]\n",
        "        if 'velocity' in item and len(item['velocity']) == 2:\n",
        "          velocity = item['velocity']\n",
        "        else:\n",
        "          velocity = item['velocity'] if 'velocity' in item else [0, 0, 0]\n",
        "          \n",
        "          velocity = np.dot(velocity_mat, np.array(\n",
        "            [velocity[0], velocity[1], velocity[2], 0], np.float32))\n",
        "          # velocity = np.dot(trans_matrix, np.array(\n",
        "          #   [velocity[0], velocity[1], velocity[2], 0], np.float32))\n",
        "          velocity = [float(velocity[0]), float(velocity[1])]\n",
        "\n",
        "        result = {\n",
        "          'sample_token': sample_token, \n",
        "          'translation': [float(translation[0]), float(translation[1]), \\\n",
        "            float(translation[2])],\n",
        "          'size': size,\n",
        "          'rotation': rotation,\n",
        "          'velocity': velocity,\n",
        "          'detection_name': class_name,\n",
        "          'attribute_name': att \\\n",
        "            if not ('attribute_name' in item) else item['attribute_name'],\n",
        "          'detection_score': score,\n",
        "          'tracking_name': class_name,\n",
        "          'tracking_score': score,\n",
        "          'tracking_id': tracking_id,\n",
        "          'sensor_id': sensor_id,\n",
        "          'det_id': det_id}\n",
        "\n",
        "        sample_results.append(result)\n",
        "      if sample_token in ret['results']:\n",
        "        ret['results'][sample_token] = ret['results'][sample_token] + \\\n",
        "          sample_results\n",
        "      else:\n",
        "        ret['results'][sample_token] = sample_results\n",
        "\n",
        "    for sample_token in ret['results'].keys():\n",
        "      confs = sorted([(-d['detection_score'], ind) \\\n",
        "        for ind, d in enumerate(ret['results'][sample_token])])\n",
        "      ret['results'][sample_token] = [ret['results'][sample_token][ind] \\\n",
        "        for _, ind in confs[:min(500, len(confs))]]\n",
        "    \n",
        "    if self.opt.iou_thresh > 0:\n",
        "      print(\"Applying BEV NMS...\")\n",
        "      n_removed = 0\n",
        "      for sample_token, dets in tqdm(ret['results'].items()):\n",
        "        ret['results'][sample_token], n = self.apply_bev_nms(dets, self.opt.iou_thresh, \n",
        "                                                          dist_thresh=2)\n",
        "        n_removed += n\n",
        "      print(\"Removed {} detections with IOU > {}\".format(n_removed, self.opt.iou_thresh))\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "  def apply_bev_nms(self, dets, iou_thresh, dist_thresh=2):\n",
        "    \"\"\"\n",
        "    Filter detection results in every sample based on BEV IOU of bounding boxes.\n",
        "    results in each sample must be sorted by score\n",
        "\n",
        "    Ouput:\n",
        "      ious: list of ious\n",
        "      n: number of remove detections\n",
        "    \"\"\"\n",
        "    N = len(dets)\n",
        "    ious = []\n",
        "    for i in range(N):\n",
        "\n",
        "      try:\n",
        "        ious = self.bev_iou(dets[i], dets[i+1:])\n",
        "      except (ValueError, IndexError) as e:\n",
        "        break\n",
        "      \n",
        "      iou_mask = (np.array(ious) < iou_thresh)\n",
        "      dets = dets[:i+1] + list(compress(dets[i+1:], iou_mask))\n",
        "\n",
        "    return dets, N-len(dets)\n",
        "  \n",
        "\n",
        "  def bev_iou(self, det1, det2, dist_thresh = 2):\n",
        "    ious = []\n",
        "    for det in det2:\n",
        "      dist = np.linalg.norm(np.array(det1['translation'][:2]) - np.array(det['translation'][:2]))\n",
        "      if dist > dist_thresh:\n",
        "        ious.append(0)\n",
        "        continue\n",
        "\n",
        "      box1 = Box(det1['translation'], det1['size'], Quaternion(det1['rotation']))\n",
        "      box2 = Box(det['translation'], det['size'], Quaternion(det['rotation']))\n",
        "      iou, iou_2d = iou3d_global(box1.corners(), box2.corners())\n",
        "      ious.append(iou_2d)\n",
        "    \n",
        "    return ious\n",
        "\n",
        "\n",
        "  def save_results(self, results, save_dir, task, split):\n",
        "    json.dump(self.convert_eval_format(results), \n",
        "                open('{}/results_nuscenes_{}_{}.json'.format(save_dir, task, split), 'w'))\n",
        "\n",
        "\n",
        "  def run_eval(self, results, save_dir, n_plots=10, render_curves=False):\n",
        "    task = 'tracking' if self.opt.tracking else 'det'\n",
        "    split = self.opt.val_split\n",
        "    version = 'v1.0-mini' if 'mini' in split else 'v1.0-trainval'\n",
        "    self.save_results(results, save_dir, task, split)\n",
        "    render_curves = 1 if render_curves else 0\n",
        "    \n",
        "    if task == 'det':\n",
        "      output_dir = '{}/nuscenes_eval_det_output_{}/'.format(save_dir, split)\n",
        "      os.system('python ' + \\\n",
        "        'tools/nuscenes-devkit/python-sdk/nuscenes/eval/detection/evaluate.py ' + \\\n",
        "        '{}/results_nuscenes_{}_{}.json '.format(save_dir, task, split) + \\\n",
        "        '--output_dir {} '.format(output_dir) + \\\n",
        "        '--eval_set {} '.format(split) + \\\n",
        "        '--dataroot ../data/nuscenes/ ' + \\\n",
        "        '--version {} '.format(version) + \\\n",
        "        '--plot_examples {} '.format(n_plots) + \\\n",
        "        '--render_curves {} '.format(render_curves))\n",
        "    else:\n",
        "      output_dir = '{}/nuscenes_evaltracl__output/'.format(save_dir)\n",
        "      os.system('python ' + \\\n",
        "        'tools/nuscenes-devkit/python-sdk/nuscenes/eval/tracking/evaluate.py ' + \\\n",
        "        '{}/results_nuscenes_{}_{}.json '.format(save_dir, task, split) + \\\n",
        "        '--output_dir {} '.format(output_dir) + \\\n",
        "        '--dataroot ../data/nuscenes/')\n",
        "      os.system('python ' + \\\n",
        "        'tools/nuscenes-devkit/python-sdk-alpha02/nuscenes/eval/tracking/evaluate.py ' + \\\n",
        "        '{}/results_nuscenes_{}_{}.json '.format(save_dir, task, split) + \\\n",
        "        '--output_dir {} '.format(output_dir) + \\\n",
        "        '--dataroot ../data/nuscenes/')\n",
        "    \n",
        "    return output_dir\n"
      ],
      "metadata": {
        "id": "aROCSNYHpsqV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset factory (can add other datasets if needed)\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import pycocotools.coco as coco\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "# from .datasets.coco import COCO\n",
        "# from .datasets.kitti import KITTI\n",
        "# from .datasets.coco_hp import COCOHP\n",
        "# from .datasets.mot import MOT\n",
        "# from .datasets.nuscenes import nuScenes\n",
        "# from .datasets.crowdhuman import CrowdHuman\n",
        "# from .datasets.kitti_tracking import KITTITracking\n",
        "# from .datasets.custom_dataset import CustomDataset\n",
        "\n",
        "dataset_factory = {\n",
        "  # 'custom': CustomDataset,\n",
        "  # 'coco': COCO,\n",
        "  # 'kitti': KITTI,\n",
        "  # 'coco_hp': COCOHP,\n",
        "  # 'mot': MOT,\n",
        "  'nuscenes': nuScenes,\n",
        "  # 'crowdhuman': CrowdHuman,\n",
        "  # 'kitti_tracking': KITTITracking,\n",
        "}\n",
        "\n",
        "\n",
        "def get_dataset(dataset):\n",
        "  return dataset_factory[dataset]"
      ],
      "metadata": {
        "id": "ZllzFoYanle_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "kqhhuADy89kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base Model"
      ],
      "metadata": {
        "id": "IrLG-zTl9sAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "# from ..utils import _topk, _tranpose_and_gather_feat\n",
        "# from utils.ddd_utils import get_pc_hm\n",
        "# from utils.pointcloud import generate_pc_hm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def fill_fc_weights(layers):\n",
        "    for m in layers.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, heads, head_convs, num_stacks, last_channel, opt=None):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.opt = opt\n",
        "        if opt is not None and opt.head_kernel != 3:\n",
        "          print('Using head kernel:', opt.head_kernel)\n",
        "          head_kernel = opt.head_kernel\n",
        "        else:\n",
        "          head_kernel = 3\n",
        "        \n",
        "        self.num_stacks = num_stacks\n",
        "        self.heads = heads\n",
        "        self.secondary_heads = opt.secondary_heads\n",
        "        \n",
        "        last_channels = {head: last_channel for head in heads}\n",
        "        for head in self.secondary_heads:\n",
        "          last_channels[head] = last_channel+len(opt.pc_feat_lvl)\n",
        "        \n",
        "        for head in self.heads:\n",
        "          classes = self.heads[head]\n",
        "          head_conv = head_convs[head]\n",
        "          if len(head_conv) > 0:\n",
        "            out = nn.Conv2d(head_conv[-1], classes, \n",
        "                  kernel_size=1, stride=1, padding=0, bias=True)\n",
        "            conv = nn.Conv2d(last_channels[head], head_conv[0],\n",
        "                              kernel_size=head_kernel, \n",
        "                              padding=head_kernel // 2, bias=True)\n",
        "            convs = [conv]\n",
        "            for k in range(1, len(head_conv)):\n",
        "                convs.append(nn.Conv2d(head_conv[k - 1], head_conv[k], \n",
        "                              kernel_size=1, bias=True))\n",
        "            if len(convs) == 1:\n",
        "              fc = nn.Sequential(conv, nn.ReLU(inplace=True), out)\n",
        "            elif len(convs) == 2:\n",
        "              fc = nn.Sequential(\n",
        "                convs[0], nn.ReLU(inplace=True), \n",
        "                convs[1], nn.ReLU(inplace=True), out)\n",
        "            elif len(convs) == 3:\n",
        "              fc = nn.Sequential(\n",
        "                  convs[0], nn.ReLU(inplace=True), \n",
        "                  convs[1], nn.ReLU(inplace=True), \n",
        "                  convs[2], nn.ReLU(inplace=True), out)\n",
        "            elif len(convs) == 4:\n",
        "              fc = nn.Sequential(\n",
        "                  convs[0], nn.ReLU(inplace=True), \n",
        "                  convs[1], nn.ReLU(inplace=True), \n",
        "                  convs[2], nn.ReLU(inplace=True), \n",
        "                  convs[3], nn.ReLU(inplace=True), out)\n",
        "            if 'hm' in head:\n",
        "              fc[-1].bias.data.fill_(opt.prior_bias)\n",
        "            else:\n",
        "              fill_fc_weights(fc)\n",
        "          else:\n",
        "            fc = nn.Conv2d(last_channels[head], classes, \n",
        "                kernel_size=1, stride=1, padding=0, bias=True)\n",
        "            if 'hm' in head:\n",
        "              fc.bias.data.fill_(opt.prior_bias)\n",
        "            else:\n",
        "              fill_fc_weights(fc)\n",
        "\n",
        "          self.__setattr__(head, fc)\n",
        "\n",
        "\n",
        "    def img2feats(self, x):\n",
        "      raise NotImplementedError\n",
        "    \n",
        "\n",
        "    def imgpre2feats(self, x, pre_img=None, pre_hm=None):\n",
        "      raise NotImplementedError\n",
        "\n",
        "\n",
        "    def forward(self, x, pc_hm=None, pc_dep=None, calib=None):\n",
        "      ## extract features from image\n",
        "      feats = self.img2feats(x)\n",
        "      out = []\n",
        "      \n",
        "      for s in range(self.num_stacks):\n",
        "        z = {}\n",
        "\n",
        "        ## Run the first stage heads\n",
        "        for head in self.heads:\n",
        "          if head not in self.secondary_heads:\n",
        "            z[head] = self.__getattr__(head)(feats[s])\n",
        "\n",
        "        if self.opt.pointcloud:\n",
        "          ## get pointcloud heatmap\n",
        "          if not self.training:\n",
        "            if self.opt.disable_frustum:\n",
        "              pc_hm = pc_dep\n",
        "              if self.opt.normalize_depth:\n",
        "                pc_hm[self.opt.pc_feat_channels['pc_dep']] /= self.opt.max_pc_dist\n",
        "            else:\n",
        "              pc_hm = generate_pc_hm(z, pc_dep, calib, self.opt)\n",
        "          ind = self.opt.pc_feat_channels['pc_dep']\n",
        "          z['pc_hm'] = pc_hm[:,ind,:,:].unsqueeze(1)\n",
        "\n",
        "          ## Run the second stage heads  \n",
        "          sec_feats = [feats[s], pc_hm]\n",
        "          sec_feats = torch.cat(sec_feats, 1)\n",
        "          for head in self.secondary_heads:\n",
        "            z[head] = self.__getattr__(head)(sec_feats)\n",
        "        \n",
        "        out.append(z)\n",
        "\n",
        "      return out\n",
        "\n"
      ],
      "metadata": {
        "id": "DVF0PFh09vc4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DLA Models"
      ],
      "metadata": {
        "id": "rhrQFmAZ-F7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "# from .base_model import BaseModel\n",
        "\n",
        "try:\n",
        "    from .DCNv2.dcn_v2 import DCN\n",
        "except:\n",
        "    print('import DCN failed')\n",
        "    DCN = None\n",
        "\n",
        "\n",
        "BN_MOMENTUM = 0.1\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_model_url(data='imagenet', name='dla34', hash='ba72cf86'):\n",
        "    return join('http://dl.yf.io/dla/models', data, '{}-{}.pth'.format(name, hash))\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        expansion = Bottleneck.expansion\n",
        "        bottle_planes = planes // expansion\n",
        "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckX(nn.Module):\n",
        "    expansion = 2\n",
        "    cardinality = 32\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(BottleneckX, self).__init__()\n",
        "        cardinality = BottleneckX.cardinality\n",
        "        # dim = int(math.floor(planes * (BottleneckV5.expansion / 64.0)))\n",
        "        # bottle_planes = dim * cardinality\n",
        "        bottle_planes = planes * cardinality // 32\n",
        "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation, bias=False,\n",
        "                               dilation=dilation, groups=cardinality)\n",
        "        self.bn2 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Root(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, residual):\n",
        "        super(Root, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, out_channels, 1,\n",
        "            stride=1, bias=False, padding=(kernel_size - 1) // 2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, *x):\n",
        "        children = x\n",
        "        x = self.conv(torch.cat(x, 1))\n",
        "        x = self.bn(x)\n",
        "        if self.residual:\n",
        "            x += children[0]\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Tree(nn.Module):\n",
        "    def __init__(self, levels, block, in_channels, out_channels, stride=1,\n",
        "                 level_root=False, root_dim=0, root_kernel_size=1,\n",
        "                 dilation=1, root_residual=False):\n",
        "        super(Tree, self).__init__()\n",
        "        if root_dim == 0:\n",
        "            root_dim = 2 * out_channels\n",
        "        if level_root:\n",
        "            root_dim += in_channels\n",
        "        if levels == 1:\n",
        "            self.tree1 = block(in_channels, out_channels, stride,\n",
        "                               dilation=dilation)\n",
        "            self.tree2 = block(out_channels, out_channels, 1,\n",
        "                               dilation=dilation)\n",
        "        else:\n",
        "            self.tree1 = Tree(levels - 1, block, in_channels, out_channels,\n",
        "                              stride, root_dim=0,\n",
        "                              root_kernel_size=root_kernel_size,\n",
        "                              dilation=dilation, root_residual=root_residual)\n",
        "            self.tree2 = Tree(levels - 1, block, out_channels, out_channels,\n",
        "                              root_dim=root_dim + out_channels,\n",
        "                              root_kernel_size=root_kernel_size,\n",
        "                              dilation=dilation, root_residual=root_residual)\n",
        "        if levels == 1:\n",
        "            self.root = Root(root_dim, out_channels, root_kernel_size,\n",
        "                             root_residual)\n",
        "        self.level_root = level_root\n",
        "        self.root_dim = root_dim\n",
        "        self.downsample = None\n",
        "        self.project = None\n",
        "        self.levels = levels\n",
        "        if stride > 1:\n",
        "            self.downsample = nn.MaxPool2d(stride, stride=stride)\n",
        "        if in_channels != out_channels:\n",
        "            self.project = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, residual=None, children=None):\n",
        "        children = [] if children is None else children\n",
        "        bottom = self.downsample(x) if self.downsample else x\n",
        "        residual = self.project(bottom) if self.project else bottom\n",
        "        if self.level_root:\n",
        "            children.append(bottom)\n",
        "        x1 = self.tree1(x, residual)\n",
        "        if self.levels == 1:\n",
        "            x2 = self.tree2(x1)\n",
        "            x = self.root(x2, x1, *children)\n",
        "        else:\n",
        "            children.append(x1)\n",
        "            x = self.tree2(x1, children=children)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DLA(nn.Module):\n",
        "    def __init__(self, levels, channels, num_classes=1000,\n",
        "                 block=BasicBlock, residual_root=False, linear_root=False,\n",
        "                 opt=None):\n",
        "        super(DLA, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.num_classes = num_classes\n",
        "        self.base_layer = nn.Sequential(\n",
        "            nn.Conv2d(opt.num_img_channels, channels[0], kernel_size=7, stride=1,\n",
        "                      padding=3, bias=False),\n",
        "            nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "        self.level0 = self._make_conv_level(\n",
        "            channels[0], channels[0], levels[0])\n",
        "        self.level1 = self._make_conv_level(\n",
        "            channels[0], channels[1], levels[1], stride=2)\n",
        "        self.level2 = Tree(levels[2], block, channels[1], channels[2], 2,\n",
        "                           level_root=False,\n",
        "                           root_residual=residual_root)\n",
        "        self.level3 = Tree(levels[3], block, channels[2], channels[3], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        self.level4 = Tree(levels[4], block, channels[3], channels[4], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        self.level5 = Tree(levels[5], block, channels[4], channels[5], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        if opt.pre_img:\n",
        "            self.pre_img_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n",
        "                      padding=3, bias=False),\n",
        "            nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "        if opt.pre_hm:\n",
        "            self.pre_hm_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, channels[0], kernel_size=7, stride=1,\n",
        "                    padding=3, bias=False),\n",
        "            nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "        # for m in self.modules():\n",
        "        #     if isinstance(m, nn.Conv2d):\n",
        "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        #     elif isinstance(m, nn.BatchNorm2d):\n",
        "        #         m.weight.data.fill_(1)\n",
        "        #         m.bias.data.zero_()\n",
        "\n",
        "    def _make_level(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.MaxPool2d(stride, stride=stride),\n",
        "                nn.Conv2d(inplanes, planes,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample=downsample))\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n",
        "        modules = []\n",
        "        for i in range(convs):\n",
        "            modules.extend([\n",
        "                nn.Conv2d(inplanes, planes, kernel_size=3,\n",
        "                          stride=stride if i == 0 else 1,\n",
        "                          padding=dilation, bias=False, dilation=dilation),\n",
        "                nn.BatchNorm2d(planes, momentum=BN_MOMENTUM),\n",
        "                nn.ReLU(inplace=True)])\n",
        "            inplanes = planes\n",
        "        return nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x, pre_img=None, pre_hm=None):\n",
        "        y = []\n",
        "        x = self.base_layer(x)\n",
        "        if pre_img is not None:\n",
        "            x = x + self.pre_img_layer(pre_img)\n",
        "        if pre_hm is not None:\n",
        "            x = x + self.pre_hm_layer(pre_hm)\n",
        "        for i in range(6):\n",
        "            x = getattr(self, 'level{}'.format(i))(x)\n",
        "            y.append(x)\n",
        "        \n",
        "        return y\n",
        "\n",
        "    def load_pretrained_model(self, data='imagenet', name='dla34', hash='ba72cf86'):\n",
        "        # fc = self.fc\n",
        "        if name.endswith('.pth'):\n",
        "            model_weights = torch.load(data + name)\n",
        "        else:\n",
        "            model_url = get_model_url(data, name, hash)\n",
        "            model_weights = model_zoo.load_url(model_url)\n",
        "        num_classes = len(model_weights[list(model_weights.keys())[-1]])\n",
        "        self.fc = nn.Conv2d(\n",
        "            self.channels[-1], num_classes,\n",
        "            kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.load_state_dict(model_weights, strict=False)\n",
        "        # self.fc = fc\n",
        "\n",
        "\n",
        "def dla34(pretrained=True, **kwargs):  # DLA-34\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 128, 256, 512],\n",
        "                block=BasicBlock, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla34', hash='ba72cf86')\n",
        "    else:\n",
        "        print('Warning: No ImageNet pretrain!!')\n",
        "    return model\n",
        "\n",
        "def dla102(pretrained=None, **kwargs):  # DLA-102\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, residual_root=True, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla102', hash='d94d9790')\n",
        "    return model\n",
        "\n",
        "def dla46_c(pretrained=None, **kwargs):  # DLA-46-C\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=Bottleneck, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla46_c', hash='2bfd52c3')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla46x_c(pretrained=None, **kwargs):  # DLA-X-46-C\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla46x_c', hash='d761bae7')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60x_c(pretrained=None, **kwargs):  # DLA-X-60-C\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla60x_c', hash='b870c45c')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60(pretrained=None, **kwargs):  # DLA-60\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla60', hash='24839fc4')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60x(pretrained=None, **kwargs):  # DLA-X-60\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla60x', hash='d15cacda')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla102x(pretrained=None, **kwargs):  # DLA-X-102\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla102x', hash='ad62be81')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla102x2(pretrained=None, **kwargs):  # DLA-X-102 64\n",
        "    BottleneckX.cardinality = 64\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla102x2', hash='262837b6')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla169(pretrained=None, **kwargs):  # DLA-169\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 2, 3, 5, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla169', hash='0914e092')\n",
        "    return model\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "def fill_fc_weights(layers):\n",
        "    for m in layers.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def fill_up_weights(up):\n",
        "    w = up.weight.data\n",
        "    f = math.ceil(w.size(2) / 2)\n",
        "    c = (2 * f - 1 - f % 2) / (2. * f)\n",
        "    for i in range(w.size(2)):\n",
        "        for j in range(w.size(3)):\n",
        "            w[0, 0, i, j] = \\\n",
        "                (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n",
        "    for c in range(1, w.size(0)):\n",
        "        w[c, 0, :, :] = w[0, 0, :, :]\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, chi, cho):\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(chi, cho, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(cho, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class GlobalConv(nn.Module):\n",
        "    def __init__(self, chi, cho, k=7, d=1):\n",
        "        super(GlobalConv, self).__init__()\n",
        "        gcl = nn.Sequential(\n",
        "            nn.Conv2d(chi, cho, kernel_size=(k, 1), stride=1, bias=False, \n",
        "                                dilation=d, padding=(d * (k // 2), 0)),\n",
        "            nn.Conv2d(cho, cho, kernel_size=(1, k), stride=1, bias=False, \n",
        "                                dilation=d, padding=(0, d * (k // 2))))\n",
        "        gcr = nn.Sequential(\n",
        "            nn.Conv2d(chi, cho, kernel_size=(1, k), stride=1, bias=False, \n",
        "                                dilation=d, padding=(0, d * (k // 2))),\n",
        "            nn.Conv2d(cho, cho, kernel_size=(k, 1), stride=1, bias=False, \n",
        "                                dilation=d, padding=(d * (k // 2), 0)))\n",
        "        fill_fc_weights(gcl)\n",
        "        fill_fc_weights(gcr)\n",
        "        self.gcl = gcl\n",
        "        self.gcr = gcr\n",
        "        self.act = nn.Sequential(\n",
        "            nn.BatchNorm2d(cho, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gcl(x) + self.gcr(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeformConv(nn.Module):\n",
        "    def __init__(self, chi, cho):\n",
        "        super(DeformConv, self).__init__()\n",
        "        self.actf = nn.Sequential(\n",
        "            nn.BatchNorm2d(cho, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv = DCN(chi, cho, kernel_size=(3,3), stride=1, padding=1, dilation=1, deformable_groups=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.actf(x)\n",
        "        return x\n",
        "\n",
        "class IDAUp(nn.Module):\n",
        "    def __init__(self, o, channels, up_f, node_type=(DeformConv, DeformConv)):\n",
        "        super(IDAUp, self).__init__()\n",
        "        for i in range(1, len(channels)):\n",
        "            c = channels[i]\n",
        "            f = int(up_f[i])  \n",
        "            proj = node_type[0](c, o)\n",
        "            node = node_type[1](o, o)\n",
        "     \n",
        "            up = nn.ConvTranspose2d(o, o, f * 2, stride=f, \n",
        "                                    padding=f // 2, output_padding=0,\n",
        "                                    groups=o, bias=False)\n",
        "            fill_up_weights(up)\n",
        "\n",
        "            setattr(self, 'proj_' + str(i), proj)\n",
        "            setattr(self, 'up_' + str(i), up)\n",
        "            setattr(self, 'node_' + str(i), node)\n",
        "                 \n",
        "        \n",
        "    def forward(self, layers, startp, endp):\n",
        "        for i in range(startp + 1, endp):\n",
        "            upsample = getattr(self, 'up_' + str(i - startp))\n",
        "            project = getattr(self, 'proj_' + str(i - startp))\n",
        "            layers[i] = upsample(project(layers[i]))\n",
        "            node = getattr(self, 'node_' + str(i - startp))\n",
        "            layers[i] = node(layers[i] + layers[i - 1])\n",
        "\n",
        "\n",
        "\n",
        "class DLAUp(nn.Module):\n",
        "    def __init__(self, startp, channels, scales, in_channels=None, \n",
        "                 node_type=DeformConv):\n",
        "        super(DLAUp, self).__init__()\n",
        "        self.startp = startp\n",
        "        if in_channels is None:\n",
        "            in_channels = channels\n",
        "        self.channels = channels\n",
        "        channels = list(channels)\n",
        "        scales = np.array(scales, dtype=int)\n",
        "        for i in range(len(channels) - 1):\n",
        "            j = -i - 2\n",
        "            setattr(self, 'ida_{}'.format(i),\n",
        "                    IDAUp(channels[j], in_channels[j:],\n",
        "                          scales[j:] // scales[j],\n",
        "                          node_type=node_type))\n",
        "            scales[j + 1:] = scales[j]\n",
        "            in_channels[j + 1:] = [channels[j] for _ in channels[j + 1:]]\n",
        "\n",
        "    def forward(self, layers):\n",
        "        out = [layers[-1]] # start with 32\n",
        "        for i in range(len(layers) - self.startp - 1):\n",
        "            ida = getattr(self, 'ida_{}'.format(i))\n",
        "            ida(layers, len(layers) -i - 2, len(layers))\n",
        "            out.insert(0, layers[-1])\n",
        "        return out\n",
        "\n",
        "\n",
        "class Interpolate(nn.Module):\n",
        "    def __init__(self, scale, mode):\n",
        "        super(Interpolate, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mode = mode\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=self.scale, mode=self.mode, align_corners=False)\n",
        "        return x\n",
        "\n",
        "\n",
        "DLA_NODE = {\n",
        "    'dcn': (DeformConv, DeformConv),\n",
        "    'gcn': (Conv, GlobalConv),\n",
        "    'conv': (Conv, Conv),\n",
        "}\n",
        "\n",
        "class DLASeg(BaseModel):\n",
        "    def __init__(self, num_layers, heads, head_convs, opt):\n",
        "        super(DLASeg, self).__init__(\n",
        "            heads, head_convs, 1, 64 if num_layers == 34 else 128, opt=opt)\n",
        "        down_ratio=4\n",
        "        self.opt = opt\n",
        "        self.node_type = DLA_NODE[opt.dla_node]\n",
        "        print('Using node type:', self.node_type)\n",
        "        self.first_level = int(np.log2(down_ratio))\n",
        "        self.last_level = 5\n",
        "        self.base = globals()['dla{}'.format(num_layers)](\n",
        "            pretrained=(opt.load_model == ''), opt=opt)\n",
        "\n",
        "        channels = self.base.channels\n",
        "        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n",
        "        self.dla_up = DLAUp(\n",
        "            self.first_level, channels[self.first_level:], scales,\n",
        "            node_type=self.node_type)\n",
        "        out_channel = channels[self.first_level]\n",
        "\n",
        "        self.ida_up = IDAUp(\n",
        "            out_channel, channels[self.first_level:self.last_level], \n",
        "            [2 ** i for i in range(self.last_level - self.first_level)],\n",
        "            node_type=self.node_type)\n",
        "        \n",
        "\n",
        "    def img2feats(self, x):\n",
        "        x = self.base(x)\n",
        "        x = self.dla_up(x)\n",
        "\n",
        "        y = []\n",
        "        for i in range(self.last_level - self.first_level):\n",
        "            y.append(x[i].clone())\n",
        "        self.ida_up(y, 0, len(y))\n",
        "\n",
        "        return [y[-1]]\n",
        "\n",
        "    def imgpre2feats(self, x, pre_img=None, pre_hm=None):\n",
        "        x = self.base(x, pre_img, pre_hm)\n",
        "        x = self.dla_up(x)\n",
        "\n",
        "        y = []\n",
        "        for i in range(self.last_level - self.first_level):\n",
        "            y.append(x[i].clone())\n",
        "        self.ida_up(y, 0, len(y))\n",
        "\n",
        "        return [y[-1]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4CC4UVG90Lo",
        "outputId": "8ded339a-000f-4550-ddac-ffa495c36fcd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import DCN failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet"
      ],
      "metadata": {
        "id": "uVmj6--P-Zw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Copyright (c) Microsoft\n",
        "# Licensed under the MIT License.\n",
        "# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n",
        "# Modified by Xingyi Zhou\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "# from .base_model import BaseModel\n",
        "\n",
        "BN_MOMENTUM = 0.1\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n",
        "                                  momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "resnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),\n",
        "               34: (BasicBlock, [3, 4, 6, 3]),\n",
        "               50: (Bottleneck, [3, 4, 6, 3]),\n",
        "               101: (Bottleneck, [3, 4, 23, 3]),\n",
        "               152: (Bottleneck, [3, 8, 36, 3])}\n",
        "\n",
        "class PoseResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, heads, head_convs, _):\n",
        "        super(PoseResNet, self).__init__(heads, head_convs, 1, 64)\n",
        "        block, layers = resnet_spec[num_layers]\n",
        "        self.inplanes = 64\n",
        "        self.deconv_with_bias = False\n",
        "        self.heads = heads\n",
        "\n",
        "        super(PoseResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # used for deconv layers\n",
        "        self.deconv_layers = self._make_deconv_layer(\n",
        "            3,\n",
        "            [256, 256, 256],\n",
        "            [4, 4, 4],\n",
        "        )\n",
        "\n",
        "        self.init_weights(num_layers, pretrained=True)\n",
        "\n",
        "    def img2feats(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.deconv_layers(x)\n",
        "        return [x]\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _get_deconv_cfg(self, deconv_kernel, index):\n",
        "        if deconv_kernel == 4:\n",
        "            padding = 1\n",
        "            output_padding = 0\n",
        "        elif deconv_kernel == 3:\n",
        "            padding = 1\n",
        "            output_padding = 1\n",
        "        elif deconv_kernel == 2:\n",
        "            padding = 0\n",
        "            output_padding = 0\n",
        "\n",
        "        return deconv_kernel, padding, output_padding\n",
        "\n",
        "    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n",
        "        assert num_layers == len(num_filters), \\\n",
        "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
        "        assert num_layers == len(num_kernels), \\\n",
        "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            kernel, padding, output_padding = \\\n",
        "                self._get_deconv_cfg(num_kernels[i], i)\n",
        "\n",
        "            planes = num_filters[i]\n",
        "            layers.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=self.inplanes,\n",
        "                    out_channels=planes,\n",
        "                    kernel_size=kernel,\n",
        "                    stride=2,\n",
        "                    padding=padding,\n",
        "                    output_padding=output_padding,\n",
        "                    bias=self.deconv_with_bias))\n",
        "            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            self.inplanes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def init_weights(self, num_layers, pretrained=True):\n",
        "        if pretrained:\n",
        "            # print('=> init resnet deconv weights from normal distribution')\n",
        "            for _, m in self.deconv_layers.named_modules():\n",
        "                if isinstance(m, nn.ConvTranspose2d):\n",
        "                    # print('=> init {}.weight as normal(0, 0.001)'.format(name))\n",
        "                    # print('=> init {}.bias as 0'.format(name))\n",
        "                    nn.init.normal_(m.weight, std=0.001)\n",
        "                    if self.deconv_with_bias:\n",
        "                        nn.init.constant_(m.bias, 0)\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    # print('=> init {}.weight as 1'.format(name))\n",
        "                    # print('=> init {}.bias as 0'.format(name))\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            # print('=> init final conv weights from normal distribution')\n",
        "            for head in self.heads:\n",
        "              final_layer = self.__getattr__(head)\n",
        "              for i, m in enumerate(final_layer.modules()):\n",
        "                  if isinstance(m, nn.Conv2d):\n",
        "                      # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                      # print('=> init {}.weight as normal(0, 0.001)'.format(name))\n",
        "                      # print('=> init {}.bias as 0'.format(name))\n",
        "                      if m.weight.shape[0] == self.heads[head]:\n",
        "                          if 'hm' in head:\n",
        "                              nn.init.constant_(m.bias, -2.19)\n",
        "                          else:\n",
        "                              nn.init.normal_(m.weight, std=0.001)\n",
        "                              nn.init.constant_(m.bias, 0)\n",
        "            #pretrained_state_dict = torch.load(pretrained)\n",
        "            url = model_urls['resnet{}'.format(num_layers)]\n",
        "            pretrained_state_dict = model_zoo.load_url(url)\n",
        "            print('=> loading pretrained model {}'.format(url))\n",
        "            self.load_state_dict(pretrained_state_dict, strict=False)\n",
        "        else:\n",
        "            print('=> imagenet pretrained model dose not exist')\n",
        "            print('=> please download it first')\n",
        "            raise ValueError('imagenet pretrained model does not exist')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9xRO-xmO-zh6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ResDCN"
      ],
      "metadata": {
        "id": "QipVfwPr-gM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Copyright (c) Microsoft\n",
        "# Licensed under the MIT License.\n",
        "# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n",
        "# Modified by Dequan Wang and Xingyi Zhou\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import math\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "try:\n",
        "  from .DCNv2.dcn_v2 import DCN\n",
        "except:\n",
        "  print('Import DCN failed')\n",
        "  DCN = None\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "# from .base_model import BaseModel\n",
        "\n",
        "\n",
        "BN_MOMENTUM = 0.1\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n",
        "                                  momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def fill_up_weights(up):\n",
        "    w = up.weight.data\n",
        "    f = math.ceil(w.size(2) / 2)\n",
        "    c = (2 * f - 1 - f % 2) / (2. * f)\n",
        "    for i in range(w.size(2)):\n",
        "        for j in range(w.size(3)):\n",
        "            w[0, 0, i, j] = \\\n",
        "                (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n",
        "    for c in range(1, w.size(0)):\n",
        "        w[c, 0, :, :] = w[0, 0, :, :] \n",
        "\n",
        "def fill_fc_weights(layers):\n",
        "    for m in layers.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.normal_(m.weight, std=0.001)\n",
        "            # torch.nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
        "            # torch.nn.init.xavier_normal_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "resnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),\n",
        "               34: (BasicBlock, [3, 4, 6, 3]),\n",
        "               50: (Bottleneck, [3, 4, 6, 3]),\n",
        "               101: (Bottleneck, [3, 4, 23, 3]),\n",
        "               152: (Bottleneck, [3, 8, 36, 3])}\n",
        "\n",
        "class PoseResDCN(BaseModel):\n",
        "    # def __init__(self, block, layers, heads, head_conv):\n",
        "    def __init__(self, num_layers, heads, head_convs, _):\n",
        "        assert head_convs['hm'][0] in [64, 256]\n",
        "        super(PoseResDCN, self).__init__(\n",
        "            heads, head_convs, 1, head_convs['hm'][0], opt=_)\n",
        "        block, layers = resnet_spec[num_layers]\n",
        "        self.inplanes = 64\n",
        "        self.deconv_with_bias = False\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # used for deconv layers\n",
        "        if head_convs['hm'][0] == 64:\n",
        "            print('Using slimed resnet: 256 128 64 up channels.')\n",
        "            self.deconv_layers = self._make_deconv_layer(\n",
        "                3,\n",
        "                [256, 128, 64],\n",
        "                [4, 4, 4],\n",
        "            )\n",
        "        else:\n",
        "            print('Using original resnet: 256 256 256 up channels.')\n",
        "            print('Using 256 deconvs')\n",
        "            self.deconv_layers = self._make_deconv_layer(\n",
        "                3,\n",
        "                [256, 256, 256],\n",
        "                [4, 4, 4],\n",
        "            )\n",
        "\n",
        "        self.init_weights(num_layers, _.rgb)\n",
        "        \n",
        "\n",
        "    def img2feats(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.deconv_layers(x)\n",
        "        return [x]\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _get_deconv_cfg(self, deconv_kernel, index):\n",
        "        if deconv_kernel == 4:\n",
        "            padding = 1\n",
        "            output_padding = 0\n",
        "        elif deconv_kernel == 3:\n",
        "            padding = 1\n",
        "            output_padding = 1\n",
        "        elif deconv_kernel == 2:\n",
        "            padding = 0\n",
        "            output_padding = 0\n",
        "\n",
        "        return deconv_kernel, padding, output_padding\n",
        "\n",
        "    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n",
        "        assert num_layers == len(num_filters), \\\n",
        "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
        "        assert num_layers == len(num_kernels), \\\n",
        "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            kernel, padding, output_padding = \\\n",
        "                self._get_deconv_cfg(num_kernels[i], i)\n",
        "\n",
        "            planes = num_filters[i]\n",
        "            fc = DCN(self.inplanes, planes, \n",
        "                    kernel_size=(3,3), stride=1,\n",
        "                    padding=1, dilation=1, deformable_groups=1)\n",
        "            # fc = nn.Conv2d(self.inplanes, planes,\n",
        "            #         kernel_size=3, stride=1, \n",
        "            #         padding=1, dilation=1, bias=False)\n",
        "            # fill_fc_weights(fc)\n",
        "            up = nn.ConvTranspose2d(\n",
        "                    in_channels=planes,\n",
        "                    out_channels=planes,\n",
        "                    kernel_size=kernel,\n",
        "                    stride=2,\n",
        "                    padding=padding,\n",
        "                    output_padding=output_padding,\n",
        "                    bias=self.deconv_with_bias)\n",
        "            fill_up_weights(up)\n",
        "\n",
        "            layers.append(fc)\n",
        "            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            layers.append(up)\n",
        "            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            self.inplanes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def init_weights(self, num_layers, rgb=False):\n",
        "        url = model_urls['resnet{}'.format(num_layers)]\n",
        "        pretrained_state_dict = model_zoo.load_url(url)\n",
        "        print('=> loading pretrained model {}'.format(url))\n",
        "        self.load_state_dict(pretrained_state_dict, strict=False)\n",
        "        if rgb:\n",
        "            print('shuffle ImageNet pretrained model from RGB to BGR')\n",
        "            self.base.base_layer[0].weight.data[:, 0], \\\n",
        "            self.base.base_layer[0].weight.data[:, 2] =  \\\n",
        "                self.base.base_layer[0].weight.data[:, 2].clone(), \\\n",
        "                self.base.base_layer[0].weight.data[:, 0].clone()\n",
        "        print('=> init deconv weights from normal distribution')\n",
        "        for name, m in self.deconv_layers.named_modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcBMQjNs_LXp",
        "outputId": "3604215c-3401-4352-82c3-94f237a14fe5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import DCN failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DLAv0"
      ],
      "metadata": {
        "id": "UlK1fR3h-tyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "# from .base_model import BaseModel\n",
        "import numpy as np\n",
        "\n",
        "BatchNorm = nn.BatchNorm2d\n",
        "\n",
        "def get_model_url(data='imagenet', name='dla34', hash='ba72cf86'):\n",
        "    return join('http://dl.yf.io/dla/models', data, '{}-{}.pth'.format(name, hash))\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn1 = BatchNorm(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn2 = BatchNorm(planes)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        expansion = Bottleneck.expansion\n",
        "        bottle_planes = planes // expansion\n",
        "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm(bottle_planes)\n",
        "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn2 = BatchNorm(bottle_planes)\n",
        "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = BatchNorm(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckX(nn.Module):\n",
        "    expansion = 2\n",
        "    cardinality = 32\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(BottleneckX, self).__init__()\n",
        "        cardinality = BottleneckX.cardinality\n",
        "        # dim = int(math.floor(planes * (BottleneckV5.expansion / 64.0)))\n",
        "        # bottle_planes = dim * cardinality\n",
        "        bottle_planes = planes * cardinality // 32\n",
        "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm(bottle_planes)\n",
        "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation, bias=False,\n",
        "                               dilation=dilation, groups=cardinality)\n",
        "        self.bn2 = BatchNorm(bottle_planes)\n",
        "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = BatchNorm(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Root(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, residual):\n",
        "        super(Root, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, out_channels, 1,\n",
        "            stride=1, bias=False, padding=(kernel_size - 1) // 2)\n",
        "        self.bn = BatchNorm(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, *x):\n",
        "        children = x\n",
        "        x = self.conv(torch.cat(x, 1))\n",
        "        x = self.bn(x)\n",
        "        if self.residual:\n",
        "            x += children[0]\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Tree(nn.Module):\n",
        "    def __init__(self, levels, block, in_channels, out_channels, stride=1,\n",
        "                 level_root=False, root_dim=0, root_kernel_size=1,\n",
        "                 dilation=1, root_residual=False):\n",
        "        super(Tree, self).__init__()\n",
        "        if root_dim == 0:\n",
        "            root_dim = 2 * out_channels\n",
        "        if level_root:\n",
        "            root_dim += in_channels\n",
        "        if levels == 1:\n",
        "            self.tree1 = block(in_channels, out_channels, stride,\n",
        "                               dilation=dilation)\n",
        "            self.tree2 = block(out_channels, out_channels, 1,\n",
        "                               dilation=dilation)\n",
        "        else:\n",
        "            self.tree1 = Tree(levels - 1, block, in_channels, out_channels,\n",
        "                              stride, root_dim=0,\n",
        "                              root_kernel_size=root_kernel_size,\n",
        "                              dilation=dilation, root_residual=root_residual)\n",
        "            self.tree2 = Tree(levels - 1, block, out_channels, out_channels,\n",
        "                              root_dim=root_dim + out_channels,\n",
        "                              root_kernel_size=root_kernel_size,\n",
        "                              dilation=dilation, root_residual=root_residual)\n",
        "        if levels == 1:\n",
        "            self.root = Root(root_dim, out_channels, root_kernel_size,\n",
        "                             root_residual)\n",
        "        self.level_root = level_root\n",
        "        self.root_dim = root_dim\n",
        "        self.downsample = None\n",
        "        self.project = None\n",
        "        self.levels = levels\n",
        "        if stride > 1:\n",
        "            self.downsample = nn.MaxPool2d(stride, stride=stride)\n",
        "        if in_channels != out_channels:\n",
        "            self.project = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                BatchNorm(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, residual=None, children=None):\n",
        "        children = [] if children is None else children\n",
        "        bottom = self.downsample(x) if self.downsample else x\n",
        "        residual = self.project(bottom) if self.project else bottom\n",
        "        if self.level_root:\n",
        "            children.append(bottom)\n",
        "        x1 = self.tree1(x, residual)\n",
        "        if self.levels == 1:\n",
        "            x2 = self.tree2(x1)\n",
        "            x = self.root(x2, x1, *children)\n",
        "        else:\n",
        "            children.append(x1)\n",
        "            x = self.tree2(x1, children=children)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DLA(nn.Module):\n",
        "    def __init__(self, levels, channels, opt, num_classes=1000,\n",
        "                 block=BasicBlock, residual_root=False, return_levels=True,\n",
        "                 pool_size=7, linear_root=False):\n",
        "        super(DLA, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.return_levels = True\n",
        "        self.num_classes = num_classes\n",
        "        self.base_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n",
        "                      padding=3, bias=False),\n",
        "            BatchNorm(channels[0]),\n",
        "            nn.ReLU(inplace=True))\n",
        "        self.level0 = self._make_conv_level(\n",
        "            channels[0], channels[0], levels[0])\n",
        "        self.level1 = self._make_conv_level(\n",
        "            channels[0], channels[1], levels[1], stride=2)\n",
        "        self.level2 = Tree(levels[2], block, channels[1], channels[2], 2,\n",
        "                           level_root=False,\n",
        "                           root_residual=residual_root)\n",
        "        self.level3 = Tree(levels[3], block, channels[2], channels[3], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        self.level4 = Tree(levels[4], block, channels[3], channels[4], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        self.level5 = Tree(levels[5], block, channels[4], channels[5], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        if opt.pre_img:\n",
        "            self.pre_img_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n",
        "                      padding=3, bias=False),\n",
        "            BatchNorm(channels[0]),\n",
        "            nn.ReLU(inplace=True))\n",
        "        if opt.pre_hm:\n",
        "            self.pre_hm_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, channels[0], kernel_size=7, stride=1,\n",
        "                    padding=3, bias=False),\n",
        "            BatchNorm(channels[0]),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, BatchNorm):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_level(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.MaxPool2d(stride, stride=stride),\n",
        "                nn.Conv2d(inplanes, planes,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                BatchNorm(planes),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample=downsample))\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n",
        "        modules = []\n",
        "        for i in range(convs):\n",
        "            modules.extend([\n",
        "                nn.Conv2d(inplanes, planes, kernel_size=3,\n",
        "                          stride=stride if i == 0 else 1,\n",
        "                          padding=dilation, bias=False, dilation=dilation),\n",
        "                BatchNorm(planes),\n",
        "                nn.ReLU(inplace=True)])\n",
        "            inplanes = planes\n",
        "        return nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x, pre_img=None, pre_hm=None):\n",
        "        y = []\n",
        "        x = self.base_layer(x)\n",
        "        if pre_img is not None:\n",
        "            x = x + self.pre_img_layer(pre_img)\n",
        "        if pre_hm is not None:\n",
        "            x = x + self.pre_hm_layer(pre_hm)\n",
        "        for i in range(6):\n",
        "            x = getattr(self, 'level{}'.format(i))(x)\n",
        "            y.append(x)\n",
        "        \n",
        "        return y\n",
        "\n",
        "\n",
        "    def load_pretrained_model(self,  data='imagenet', name='dla34', hash='ba72cf86'):\n",
        "        if name.endswith('.pth'):\n",
        "            model_weights = torch.load(data + name)\n",
        "        else:\n",
        "            model_url = get_model_url(data, name, hash)\n",
        "            model_weights = model_zoo.load_url(model_url)\n",
        "\n",
        "\n",
        "def dla34(pretrained, **kwargs):  # DLA-34\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 128, 256, 512],\n",
        "                block=BasicBlock, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_pretrained_model(data='imagenet', name='dla34', hash='ba72cf86')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla46_c(pretrained=None, **kwargs):  # DLA-46-C\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=Bottleneck, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla46_c')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla46x_c(pretrained=None, **kwargs):  # DLA-X-46-C\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla46x_c')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60x_c(pretrained, **kwargs):  # DLA-X-60-C\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_pretrained_model(data='imagenet', name='dla60x_c', hash='b870c45c')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60(pretrained=None, **kwargs):  # DLA-60\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla60')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60x(pretrained=None, **kwargs):  # DLA-X-60\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla60x')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla102(pretrained=None, **kwargs):  # DLA-102\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla102')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla102x(pretrained=None, **kwargs):  # DLA-X-102\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla102x')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla102x2(pretrained=None, **kwargs):  # DLA-X-102 64\n",
        "    BottleneckX.cardinality = 64\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla102x2')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla169(pretrained=None, **kwargs):  # DLA-169\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 2, 3, 5, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(pretrained, 'dla169')\n",
        "    return model\n",
        "\n",
        "\n",
        "def set_bn(bn):\n",
        "    global BatchNorm\n",
        "    BatchNorm = bn\n",
        "    dla.BatchNorm = bn\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "def fill_up_weights(up):\n",
        "    w = up.weight.data\n",
        "    f = math.ceil(w.size(2) / 2)\n",
        "    c = (2 * f - 1 - f % 2) / (2. * f)\n",
        "    for i in range(w.size(2)):\n",
        "        for j in range(w.size(3)):\n",
        "            w[0, 0, i, j] = \\\n",
        "                (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n",
        "    for c in range(1, w.size(0)):\n",
        "        w[c, 0, :, :] = w[0, 0, :, :]\n",
        "\n",
        "\n",
        "class IDAUp(nn.Module):\n",
        "    def __init__(self, node_kernel, out_dim, channels, up_factors):\n",
        "        super(IDAUp, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.out_dim = out_dim\n",
        "        for i, c in enumerate(channels):\n",
        "            if c == out_dim:\n",
        "                proj = Identity()\n",
        "            else:\n",
        "                proj = nn.Sequential(\n",
        "                    nn.Conv2d(c, out_dim,\n",
        "                              kernel_size=1, stride=1, bias=False),\n",
        "                    BatchNorm(out_dim),\n",
        "                    nn.ReLU(inplace=True))\n",
        "            f = int(up_factors[i])\n",
        "            if f == 1:\n",
        "                up = Identity()\n",
        "            else:\n",
        "                up = nn.ConvTranspose2d(\n",
        "                    out_dim, out_dim, f * 2, stride=f, padding=f // 2,\n",
        "                    output_padding=0, groups=out_dim, bias=False)\n",
        "                fill_up_weights(up)\n",
        "            setattr(self, 'proj_' + str(i), proj)\n",
        "            setattr(self, 'up_' + str(i), up)\n",
        "\n",
        "        for i in range(1, len(channels)):\n",
        "            node = nn.Sequential(\n",
        "                nn.Conv2d(out_dim * 2, out_dim,\n",
        "                          kernel_size=node_kernel, stride=1,\n",
        "                          padding=node_kernel // 2, bias=False),\n",
        "                BatchNorm(out_dim),\n",
        "                nn.ReLU(inplace=True))\n",
        "            setattr(self, 'node_' + str(i), node)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, BatchNorm):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, layers):\n",
        "        assert len(self.channels) == len(layers), \\\n",
        "            '{} vs {} layers'.format(len(self.channels), len(layers))\n",
        "        layers = list(layers)\n",
        "        for i, l in enumerate(layers):\n",
        "            upsample = getattr(self, 'up_' + str(i))\n",
        "            project = getattr(self, 'proj_' + str(i))\n",
        "            layers[i] = upsample(project(l))\n",
        "        x = layers[0]\n",
        "        y = []\n",
        "        for i in range(1, len(layers)):\n",
        "            node = getattr(self, 'node_' + str(i))\n",
        "            x = node(torch.cat([x, layers[i]], 1))\n",
        "            y.append(x)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "class DLAUp(nn.Module):\n",
        "    def __init__(self, channels, scales=(1, 2, 4, 8, 16), in_channels=None):\n",
        "        super(DLAUp, self).__init__()\n",
        "        if in_channels is None:\n",
        "            in_channels = channels\n",
        "        self.channels = channels\n",
        "        channels = list(channels)\n",
        "        scales = np.array(scales, dtype=int)\n",
        "        for i in range(len(channels) - 1):\n",
        "            j = -i - 2\n",
        "            setattr(self, 'ida_{}'.format(i),\n",
        "                    IDAUp(3, channels[j], in_channels[j:],\n",
        "                          scales[j:] // scales[j]))\n",
        "            scales[j + 1:] = scales[j]\n",
        "            in_channels[j + 1:] = [channels[j] for _ in channels[j + 1:]]\n",
        "\n",
        "    def forward(self, layers):\n",
        "        layers = list(layers)\n",
        "        assert len(layers) > 1\n",
        "        for i in range(len(layers) - 1):\n",
        "            ida = getattr(self, 'ida_{}'.format(i))\n",
        "            x, y = ida(layers[-i - 2:])\n",
        "            layers[-i - 1:] = y\n",
        "        return x\n",
        "\n",
        "def fill_fc_weights(layers):\n",
        "    for m in layers.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.normal_(m.weight, std=0.001)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "class DLASegv0(BaseModel):\n",
        "    def __init__(self, num_layers, heads, head_convs, opt):\n",
        "        super(DLASegv0, self).__init__(heads, head_convs, 1, 64, opt=opt)\n",
        "        down_ratio=4\n",
        "        self.opt = opt\n",
        "        self.heads = heads\n",
        "        self.first_level = int(np.log2(down_ratio))\n",
        "        self.base = globals()['dla{}'.format(num_layers)](\n",
        "          pretrained=True, opt=opt)\n",
        "\n",
        "        channels = self.base.channels\n",
        "        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n",
        "        self.dla_up = DLAUp(channels[self.first_level:], scales=scales)\n",
        "\n",
        "    def img2feats(self, x):\n",
        "        x = self.base(x)\n",
        "        x = self.dla_up(x[self.first_level:])\n",
        "\n",
        "        return [x]\n",
        "\n",
        "    def imgpre2feats(self, x, pre_img=None, pre_hm=None):\n",
        "        x = self.base(x, pre_img, pre_hm)\n",
        "        x = self.dla_up(x[self.first_level:])\n",
        "\n",
        "        return [x]\n"
      ],
      "metadata": {
        "id": "8kzObRmw-1Ml"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generic Network"
      ],
      "metadata": {
        "id": "E1Hs1Yiq-l-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### backbones (dla)"
      ],
      "metadata": {
        "id": "Vyd4rt2W_lli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "BN_MOMENTUM = 0.1\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_model_url(data='imagenet', name='dla34', hash='ba72cf86'):\n",
        "    return join('http://dl.yf.io/dla/models', data, '{}-{}.pth'.format(name, hash))\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        expansion = Bottleneck.expansion\n",
        "        bottle_planes = planes // expansion\n",
        "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation,\n",
        "                               bias=False, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckX(nn.Module):\n",
        "    expansion = 2\n",
        "    cardinality = 32\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
        "        super(BottleneckX, self).__init__()\n",
        "        cardinality = BottleneckX.cardinality\n",
        "        # dim = int(math.floor(planes * (BottleneckV5.expansion / 64.0)))\n",
        "        # bottle_planes = dim * cardinality\n",
        "        bottle_planes = planes * cardinality // 32\n",
        "        self.conv1 = nn.Conv2d(inplanes, bottle_planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv2 = nn.Conv2d(bottle_planes, bottle_planes, kernel_size=3,\n",
        "                               stride=stride, padding=dilation, bias=False,\n",
        "                               dilation=dilation, groups=cardinality)\n",
        "        self.bn2 = nn.BatchNorm2d(bottle_planes, momentum=BN_MOMENTUM)\n",
        "        self.conv3 = nn.Conv2d(bottle_planes, planes,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x, residual=None):\n",
        "        if residual is None:\n",
        "            residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Root(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, residual):\n",
        "        super(Root, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, out_channels, 1,\n",
        "            stride=1, bias=False, padding=(kernel_size - 1) // 2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, *x):\n",
        "        children = x\n",
        "        x = self.conv(torch.cat(x, 1))\n",
        "        x = self.bn(x)\n",
        "        if self.residual:\n",
        "            x += children[0]\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Tree(nn.Module):\n",
        "    def __init__(self, levels, block, in_channels, out_channels, stride=1,\n",
        "                 level_root=False, root_dim=0, root_kernel_size=1,\n",
        "                 dilation=1, root_residual=False):\n",
        "        super(Tree, self).__init__()\n",
        "        if root_dim == 0:\n",
        "            root_dim = 2 * out_channels\n",
        "        if level_root:\n",
        "            root_dim += in_channels\n",
        "        if levels == 1:\n",
        "            self.tree1 = block(in_channels, out_channels, stride,\n",
        "                               dilation=dilation)\n",
        "            self.tree2 = block(out_channels, out_channels, 1,\n",
        "                               dilation=dilation)\n",
        "        else:\n",
        "            self.tree1 = Tree(levels - 1, block, in_channels, out_channels,\n",
        "                              stride, root_dim=0,\n",
        "                              root_kernel_size=root_kernel_size,\n",
        "                              dilation=dilation, root_residual=root_residual)\n",
        "            self.tree2 = Tree(levels - 1, block, out_channels, out_channels,\n",
        "                              root_dim=root_dim + out_channels,\n",
        "                              root_kernel_size=root_kernel_size,\n",
        "                              dilation=dilation, root_residual=root_residual)\n",
        "        if levels == 1:\n",
        "            self.root = Root(root_dim, out_channels, root_kernel_size,\n",
        "                             root_residual)\n",
        "        self.level_root = level_root\n",
        "        self.root_dim = root_dim\n",
        "        self.downsample = None\n",
        "        self.project = None\n",
        "        self.levels = levels\n",
        "        if stride > 1:\n",
        "            self.downsample = nn.MaxPool2d(stride, stride=stride)\n",
        "        if in_channels != out_channels:\n",
        "            self.project = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, residual=None, children=None):\n",
        "        children = [] if children is None else children\n",
        "        bottom = self.downsample(x) if self.downsample else x\n",
        "        residual = self.project(bottom) if self.project else bottom\n",
        "        if self.level_root:\n",
        "            children.append(bottom)\n",
        "        x1 = self.tree1(x, residual)\n",
        "        if self.levels == 1:\n",
        "            x2 = self.tree2(x1)\n",
        "            x = self.root(x2, x1, *children)\n",
        "        else:\n",
        "            children.append(x1)\n",
        "            x = self.tree2(x1, children=children)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DLA(nn.Module):\n",
        "    def __init__(self, levels, channels, \n",
        "                 block=BasicBlock, residual_root=False, \n",
        "                 opt=None):\n",
        "        super(DLA, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.base_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n",
        "                      padding=3, bias=False),\n",
        "            nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "        self.level0 = self._make_conv_level(\n",
        "            channels[0], channels[0], levels[0])\n",
        "        self.level1 = self._make_conv_level(\n",
        "            channels[0], channels[1], levels[1], stride=2)\n",
        "        self.level2 = Tree(levels[2], block, channels[1], channels[2], 2,\n",
        "                           level_root=False,\n",
        "                           root_residual=residual_root)\n",
        "        self.level3 = Tree(levels[3], block, channels[2], channels[3], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        self.level4 = Tree(levels[4], block, channels[3], channels[4], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        self.level5 = Tree(levels[5], block, channels[4], channels[5], 2,\n",
        "                           level_root=True, root_residual=residual_root)\n",
        "        # for m in self.modules():\n",
        "        #     if isinstance(m, nn.Conv2d):\n",
        "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        #     elif isinstance(m, nn.BatchNorm2d):\n",
        "        #         m.weight.data.fill_(1)\n",
        "        #         m.bias.data.zero_()\n",
        "        if opt.pre_img:\n",
        "            print('adding pre_img layer...')\n",
        "            self.pre_img_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n",
        "                      padding=3, bias=False),\n",
        "            nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "        if opt.pre_hm:\n",
        "            print('adding pre_hm layer...')\n",
        "            self.pre_hm_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, channels[0], kernel_size=7, stride=1,\n",
        "                    padding=3, bias=False),\n",
        "            nn.BatchNorm2d(channels[0], momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def _make_level(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.MaxPool2d(stride, stride=stride),\n",
        "                nn.Conv2d(inplanes, planes,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes, momentum=BN_MOMENTUM),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample=downsample))\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n",
        "        modules = []\n",
        "        for i in range(convs):\n",
        "            modules.extend([\n",
        "                nn.Conv2d(inplanes, planes, kernel_size=3,\n",
        "                          stride=stride if i == 0 else 1,\n",
        "                          padding=dilation, bias=False, dilation=dilation),\n",
        "                nn.BatchNorm2d(planes, momentum=BN_MOMENTUM),\n",
        "                nn.ReLU(inplace=True)])\n",
        "            inplanes = planes\n",
        "        return nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x, pre_img=None, pre_hm=None):\n",
        "        y = []\n",
        "        x = self.base_layer(x)\n",
        "        if pre_img is not None:\n",
        "            x = x + self.pre_img_layer(pre_img)\n",
        "        if pre_hm is not None:\n",
        "            x = x + self.pre_hm_layer(pre_hm)\n",
        "        for i in range(6):\n",
        "            x = getattr(self, 'level{}'.format(i))(x)\n",
        "            y.append(x)\n",
        "        \n",
        "        return y\n",
        "\n",
        "    def load_pretrained_model(self, data='imagenet', name='dla34', hash='ba72cf86'):\n",
        "        if name.endswith('.pth'):\n",
        "            model_weights = torch.load(data + name)\n",
        "        else:\n",
        "            model_url = get_model_url(data, name, hash)\n",
        "            model_weights = model_zoo.load_url(model_url)\n",
        "        num_classes = len(model_weights[list(model_weights.keys())[-1]])\n",
        "        self.fc = nn.Conv2d(\n",
        "            self.channels[-1], num_classes,\n",
        "            kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.load_state_dict(model_weights, strict=False)\n",
        "\n",
        "model_dict = {\n",
        "    'dla34': (\n",
        "        [1, 1, 1, 2, 2, 1], \n",
        "        [16, 32, 64, 128, 256, 512],\n",
        "        'ba72cf86'),\n",
        "    'dla102': (\n",
        "        [1, 1, 1, 3, 4, 1],\n",
        "        [16, 32, 128, 256, 512, 1024],\n",
        "        'd94d9790'),\n",
        "    'dla46_c': (\n",
        "        [1, 1, 1, 2, 2, 1],\n",
        "        [16, 32, 64, 64, 128, 256],\n",
        "        '2bfd52c3'),\n",
        "    'dla46x_c': (\n",
        "        [1, 1, 1, 2, 2, 1],\n",
        "        [16, 32, 64, 64, 128, 256],\n",
        "        'd761bae7'),\n",
        "    'dla60x_c': (\n",
        "        [1, 1, 1, 2, 3, 1],\n",
        "        [16, 32, 64, 64, 128, 256],\n",
        "        'b870c45c'),\n",
        "    'dla60': (\n",
        "        [1, 1, 1, 2, 3, 1],\n",
        "        [16, 32, 128, 256, 512, 1024],\n",
        "        '24839fc4'),\n",
        "    'dla60x': (\n",
        "        [1, 1, 1, 2, 3, 1],\n",
        "        [16, 32, 128, 256, 512, 1024],\n",
        "        'd15cacda'),\n",
        "    'dla102x': (\n",
        "        [1, 1, 1, 3, 4, 1], \n",
        "        [16, 32, 128, 256, 512, 1024],\n",
        "        'ad62be81'),\n",
        "    'dla102x2': (\n",
        "        [1, 1, 1, 3, 4, 1], \n",
        "        [16, 32, 128, 256, 512, 1024],\n",
        "        '262837b6'),\n",
        "    'dla169': (\n",
        "        [1, 1, 2, 3, 5, 1], \n",
        "        [16, 32, 128, 256, 512, 1024],\n",
        "        '0914e092'\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "def dla34(pretrained=True, **kwargs):  # DLA-34\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 128, 256, 512],\n",
        "                block=BasicBlock, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla34', hash='ba72cf86')\n",
        "    else:\n",
        "        print('Warning: No ImageNet pretrain!!')\n",
        "    return model\n",
        "\n",
        "def dla102(pretrained=None, **kwargs):  # DLA-102\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, residual_root=True, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla102', hash='d94d9790')\n",
        "    return model\n",
        "\n",
        "def dla46_c(pretrained=None, **kwargs):  # DLA-46-C\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=Bottleneck, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla46_c', hash='2bfd52c3')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla46x_c(pretrained=None, **kwargs):  # DLA-X-46-C\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 2, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla46x_c', hash='d761bae7')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60x_c(pretrained=None, **kwargs):  # DLA-X-60-C\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 64, 64, 128, 256],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla60x_c', hash='b870c45c')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60(pretrained=None, **kwargs):  # DLA-60\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla60', hash='24839fc4')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla60x(pretrained=None, **kwargs):  # DLA-X-60\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 2, 3, 1],\n",
        "                [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla60x', hash='d15cacda')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla102x(pretrained=None, **kwargs):  # DLA-X-102\n",
        "    BottleneckX.expansion = 2\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla102x', hash='ad62be81')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla102x2(pretrained=None, **kwargs):  # DLA-X-102 64\n",
        "    BottleneckX.cardinality = 64\n",
        "    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=BottleneckX, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla102x2', hash='262837b6')\n",
        "    return model\n",
        "\n",
        "\n",
        "def dla169(pretrained=None, **kwargs):  # DLA-169\n",
        "    Bottleneck.expansion = 2\n",
        "    model = DLA([1, 1, 2, 3, 5, 1], [16, 32, 128, 256, 512, 1024],\n",
        "                block=Bottleneck, residual_root=True, **kwargs)\n",
        "    if pretrained is not None:\n",
        "        model.load_pretrained_model(\n",
        "            data='imagenet', name='dla169', hash='0914e092')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "t2rBLsnO_pzu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### necks (DLAUp)"
      ],
      "metadata": {
        "id": "B9X4N2TvACSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "try:\n",
        "  from ..DCNv2.dcn_v2 import DCN\n",
        "except:\n",
        "  print('import DCN failed')\n",
        "  DCN = None\n",
        "\n",
        "BN_MOMENTUM = 0.1\n",
        "\n",
        "class Identity(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "def fill_fc_weights(layers):\n",
        "    for m in layers.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def fill_up_weights(up):\n",
        "    w = up.weight.data\n",
        "    f = math.ceil(w.size(2) / 2)\n",
        "    c = (2 * f - 1 - f % 2) / (2. * f)\n",
        "    for i in range(w.size(2)):\n",
        "        for j in range(w.size(3)):\n",
        "            w[0, 0, i, j] = \\\n",
        "                (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n",
        "    for c in range(1, w.size(0)):\n",
        "        w[c, 0, :, :] = w[0, 0, :, :]\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, chi, cho):\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(chi, cho, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(cho, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class GlobalConv(nn.Module):\n",
        "    def __init__(self, chi, cho, k=7, d=1):\n",
        "        super(GlobalConv, self).__init__()\n",
        "        gcl = nn.Sequential(\n",
        "            nn.Conv2d(chi, cho, kernel_size=(k, 1), stride=1, bias=False, \n",
        "                                dilation=d, padding=(d * (k // 2), 0)),\n",
        "            nn.Conv2d(cho, cho, kernel_size=(1, k), stride=1, bias=False, \n",
        "                                dilation=d, padding=(0, d * (k // 2))))\n",
        "        gcr = nn.Sequential(\n",
        "            nn.Conv2d(chi, cho, kernel_size=(1, k), stride=1, bias=False, \n",
        "                                dilation=d, padding=(0, d * (k // 2))),\n",
        "            nn.Conv2d(cho, cho, kernel_size=(k, 1), stride=1, bias=False, \n",
        "                                dilation=d, padding=(d * (k // 2), 0)))\n",
        "        fill_fc_weights(gcl)\n",
        "        fill_fc_weights(gcr)\n",
        "        self.gcl = gcl\n",
        "        self.gcr = gcr\n",
        "        self.act = nn.Sequential(\n",
        "            nn.BatchNorm2d(cho, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gcl(x) + self.gcr(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeformConv(nn.Module):\n",
        "    def __init__(self, chi, cho):\n",
        "        super(DeformConv, self).__init__()\n",
        "        self.actf = nn.Sequential(\n",
        "            nn.BatchNorm2d(cho, momentum=BN_MOMENTUM),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv = DCN(chi, cho, kernel_size=(3,3), stride=1, padding=1, dilation=1, deformable_groups=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.actf(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class IDAUp(nn.Module):\n",
        "    def __init__(self, o, channels, up_f, node_type=(DeformConv, DeformConv)):\n",
        "        super(IDAUp, self).__init__()\n",
        "        for i in range(1, len(channels)):\n",
        "            c = channels[i]\n",
        "            f = int(up_f[i])  \n",
        "            proj = node_type[0](c, o)\n",
        "            node = node_type[1](o, o)\n",
        "     \n",
        "            up = nn.ConvTranspose2d(o, o, f * 2, stride=f, \n",
        "                                    padding=f // 2, output_padding=0,\n",
        "                                    groups=o, bias=False)\n",
        "            fill_up_weights(up)\n",
        "\n",
        "            setattr(self, 'proj_' + str(i), proj)\n",
        "            setattr(self, 'up_' + str(i), up)\n",
        "            setattr(self, 'node_' + str(i), node)\n",
        "                 \n",
        "        \n",
        "    def forward(self, layers, startp, endp):\n",
        "        for i in range(startp + 1, endp):\n",
        "            upsample = getattr(self, 'up_' + str(i - startp))\n",
        "            project = getattr(self, 'proj_' + str(i - startp))\n",
        "            layers[i] = upsample(project(layers[i]))\n",
        "            node = getattr(self, 'node_' + str(i - startp))\n",
        "            layers[i] = node(layers[i] + layers[i - 1])\n",
        "\n",
        "\n",
        "\n",
        "class DLAUp(nn.Module):\n",
        "    def __init__(self, startp, channels, scales, in_channels=None, \n",
        "                 node_type=DeformConv):\n",
        "        super(DLAUp, self).__init__()\n",
        "        self.startp = startp\n",
        "        if in_channels is None:\n",
        "            in_channels = channels\n",
        "        self.channels = channels\n",
        "        channels = list(channels)\n",
        "        scales = np.array(scales, dtype=int)\n",
        "        for i in range(len(channels) - 1):\n",
        "            j = -i - 2\n",
        "            setattr(self, 'ida_{}'.format(i),\n",
        "                    IDAUp(channels[j], in_channels[j:],\n",
        "                          scales[j:] // scales[j],\n",
        "                          node_type=node_type))\n",
        "            scales[j + 1:] = scales[j]\n",
        "            in_channels[j + 1:] = [channels[j] for _ in channels[j + 1:]]\n",
        "\n",
        "    def forward(self, layers):\n",
        "        out = [layers[-1]] # start with 32\n",
        "        for i in range(len(layers) - self.startp - 1):\n",
        "            ida = getattr(self, 'ida_{}'.format(i))\n",
        "            ida(layers, len(layers) -i - 2, len(layers))\n",
        "            out.insert(0, layers[-1])\n",
        "        return out\n",
        "\n",
        "DLA_NODE = {\n",
        "    'dcn': (DeformConv, DeformConv),\n",
        "    'gcn': (Conv, GlobalConv),\n",
        "    'conv': (Conv, Conv),\n",
        "}\n",
        "\n",
        "class DLASeg(nn.Module):\n",
        "    def __init__(self, opt, channels):\n",
        "        super().__init__()\n",
        "        self.opt = opt\n",
        "        self.channels = channels\n",
        "        self.node_type = DLA_NODE[opt.dla_node]\n",
        "        print('Using node type:', self.node_type)\n",
        "        down_ratio = 4\n",
        "        self.first_level = int(np.log2(down_ratio))\n",
        "        self.last_level = 5\n",
        "\n",
        "        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n",
        "        self.dla_up = DLAUp(\n",
        "            self.first_level, channels[self.first_level:], scales,\n",
        "            node_type=self.node_type)\n",
        "        self.out_channel = channels[self.first_level]\n",
        "\n",
        "        self.ida_up = IDAUp(\n",
        "            self.out_channel, channels[self.first_level:self.last_level], \n",
        "            [2 ** i for i in range(self.last_level - self.first_level)],\n",
        "            node_type=self.node_type)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dla_up(x)\n",
        "        y = []\n",
        "        for i in range(self.last_level - self.first_level):\n",
        "            y.append(x[i].clone())\n",
        "        self.ida_up(y, 0, len(y))\n",
        "\n",
        "        return [y[-1]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HltIq73hAGKd",
        "outputId": "cbbc1545-299a-464c-bd19-b315f92161f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import DCN failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generic Network"
      ],
      "metadata": {
        "id": "68DTlcotAXGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "# from .backbones.dla import dla34\n",
        "# from .backbones.resnet import Resnet\n",
        "# from .backbones.mobilenet import MobileNetV2\n",
        "# from .necks.dlaup import DLASeg\n",
        "# from .necks.msraup import MSRAUp\n",
        "\n",
        "backbone_factory = {\n",
        "  'dla34': dla34,\n",
        "  # 'resnet': Resnet,\n",
        "  # 'mobilenet': MobileNetV2\n",
        "}\n",
        "\n",
        "neck_factory = {\n",
        "  'dlaup': DLASeg,\n",
        "  # 'msraup': MSRAUp\n",
        "}\n",
        "\n",
        "def fill_fc_weights(layers):\n",
        "    for m in layers.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "class GenericNetwork(nn.Module):\n",
        "    def __init__(self, num_layers, heads, head_convs, num_stacks=1, opt=None):\n",
        "        super(GenericNetwork, self).__init__()\n",
        "        print('Using generic model with backbone {} and neck {}'.format(\n",
        "          opt.backbone, opt.neck))\n",
        "        # assert (not opt.pre_hm) and (not opt.pre_img)\n",
        "        if opt is not None and opt.head_kernel != 3:\n",
        "          print('Using head kernel:', opt.head_kernel)\n",
        "          head_kernel = opt.head_kernel\n",
        "        else:\n",
        "          head_kernel = 3\n",
        "        self.opt = opt\n",
        "        self.backbone = backbone_factory[opt.backbone](opt=opt)\n",
        "        channels = self.backbone.channels\n",
        "        self.neck = neck_factory[opt.neck](opt=opt, channels=channels)\n",
        "        last_channel = self.neck.out_channel\n",
        "        self.num_stacks = num_stacks\n",
        "        self.heads = heads\n",
        "        for head in self.heads:\n",
        "            classes = self.heads[head]\n",
        "            head_conv = head_convs[head]\n",
        "            if len(head_conv) > 0:\n",
        "              out = nn.Conv2d(head_conv[-1], classes, \n",
        "                    kernel_size=1, stride=1, padding=0, bias=True)\n",
        "              conv = nn.Conv2d(last_channel, head_conv[0],\n",
        "                               kernel_size=head_kernel, \n",
        "                               padding=head_kernel // 2, bias=True)\n",
        "              convs = [conv]\n",
        "              for k in range(1, len(head_conv)):\n",
        "                  convs.append(nn.Conv2d(head_conv[k - 1], head_conv[k], \n",
        "                               kernel_size=1, bias=True))\n",
        "              if len(convs) == 1:\n",
        "                fc = nn.Sequential(conv, nn.ReLU(inplace=True), out)\n",
        "              elif len(convs) == 2:\n",
        "                fc = nn.Sequential(\n",
        "                  convs[0], nn.ReLU(inplace=True), \n",
        "                  convs[1], nn.ReLU(inplace=True), out)\n",
        "              elif len(convs) == 3:\n",
        "                fc = nn.Sequential(\n",
        "                    convs[0], nn.ReLU(inplace=True), \n",
        "                    convs[1], nn.ReLU(inplace=True), \n",
        "                    convs[2], nn.ReLU(inplace=True), out)\n",
        "              elif len(convs) == 4:\n",
        "                fc = nn.Sequential(\n",
        "                    convs[0], nn.ReLU(inplace=True), \n",
        "                    convs[1], nn.ReLU(inplace=True), \n",
        "                    convs[2], nn.ReLU(inplace=True), \n",
        "                    convs[3], nn.ReLU(inplace=True), out)\n",
        "              if 'hm' in head:\n",
        "                fc[-1].bias.data.fill_(opt.prior_bias)\n",
        "              else:\n",
        "                fill_fc_weights(fc)\n",
        "            else:\n",
        "              fc = nn.Conv2d(last_channel, classes, \n",
        "                  kernel_size=1, stride=1, padding=0, bias=True)\n",
        "              if 'hm' in head:\n",
        "                fc.bias.data.fill_(opt.prior_bias)\n",
        "              else:\n",
        "                fill_fc_weights(fc)\n",
        "            self.__setattr__(head, fc)\n",
        "\n",
        "    def forward(self, x, pre_img=None, pre_hm=None):\n",
        "      y = self.backbone(x, pre_img, pre_hm)\n",
        "      feats = self.neck(y)\n",
        "      out = []\n",
        "      if self.opt.model_output_list:\n",
        "        for s in range(self.num_stacks):\n",
        "          z = []\n",
        "          for head in sorted(self.heads):\n",
        "              z.append(self.__getattr__(head)(feats[s]))\n",
        "          out.append(z)\n",
        "      else:\n",
        "        for s in range(self.num_stacks):\n",
        "          z = {}\n",
        "          for head in self.heads:\n",
        "              z[head] = self.__getattr__(head)(feats[s])\n",
        "          out.append(z)\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "sKe2vs5w_T-l"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Model"
      ],
      "metadata": {
        "id": "e0psrYKjAhyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# from .networks.dla import DLASeg\n",
        "# from .networks.resdcn import PoseResDCN\n",
        "# from .networks.resnet import PoseResNet\n",
        "# from .networks.dlav0 import DLASegv0\n",
        "# from .networks.generic_network import GenericNetwork\n",
        "\n",
        "_network_factory = {\n",
        "  'resdcn': PoseResDCN,\n",
        "  'dla': DLASeg,\n",
        "  'res': PoseResNet,\n",
        "  'dlav0': DLASegv0,\n",
        "  'generic': GenericNetwork\n",
        "}\n",
        "\n",
        "def create_model(arch, head, head_conv, opt=None):\n",
        "  num_layers = int(arch[arch.find('_') + 1:]) if '_' in arch else 0\n",
        "  arch = arch[:arch.find('_')] if '_' in arch else arch\n",
        "  model_class = _network_factory[arch]\n",
        "  model = model_class(num_layers, heads=head, head_convs=head_conv, opt=opt)\n",
        "  return model\n",
        "\n",
        "def load_model(model, model_path, opt, optimizer=None):\n",
        "  start_epoch = 0\n",
        "  checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "  print('loaded {}, epoch {}'.format(model_path, checkpoint['epoch']))\n",
        "  state_dict_ = checkpoint['state_dict']\n",
        "  state_dict = {}\n",
        "   \n",
        "  # convert data_parallal to model\n",
        "  for k in state_dict_:\n",
        "    if k.startswith('module') and not k.startswith('module_list'):\n",
        "      state_dict[k[7:]] = state_dict_[k]\n",
        "    else:\n",
        "      state_dict[k] = state_dict_[k]\n",
        "  model_state_dict = model.state_dict()\n",
        "\n",
        "  # check loaded parameters and created model parameters\n",
        "  for k in state_dict:\n",
        "    if k in model_state_dict:\n",
        "      if (state_dict[k].shape != model_state_dict[k].shape) or \\\n",
        "        (opt.reset_hm and k.startswith('hm') and (state_dict[k].shape[0] in [80, 1])):\n",
        "        if opt.reuse_hm:\n",
        "          print('Reusing parameter {}, required shape{}, '\\\n",
        "                'loaded shape{}.'.format(\n",
        "            k, model_state_dict[k].shape, state_dict[k].shape))\n",
        "          # todo: bug in next line: both sides of < are the same\n",
        "          if state_dict[k].shape[0] < state_dict[k].shape[0]:\n",
        "            model_state_dict[k][:state_dict[k].shape[0]] = state_dict[k]\n",
        "          else:\n",
        "            model_state_dict[k] = state_dict[k][:model_state_dict[k].shape[0]]\n",
        "          state_dict[k] = model_state_dict[k]\n",
        "        \n",
        "        elif opt.warm_start_weights:\n",
        "          try:\n",
        "            print('Partially loading parameter {}, required shape{}, '\\\n",
        "                  'loaded shape{}.'.format(\n",
        "              k, model_state_dict[k].shape, state_dict[k].shape))\n",
        "            if state_dict[k].shape[1] < model_state_dict[k].shape[1]:\n",
        "              model_state_dict[k][:,:state_dict[k].shape[1]] = state_dict[k]\n",
        "            else:\n",
        "              model_state_dict[k] = state_dict[k][:,:model_state_dict[k].shape[1]]\n",
        "            state_dict[k] = model_state_dict[k]\n",
        "          except:\n",
        "            print('Skip loading parameter {}, required shape{}, '\\\n",
        "                'loaded shape{}.'.format(\n",
        "                k, model_state_dict[k].shape, state_dict[k].shape))\n",
        "            state_dict[k] = model_state_dict[k]\n",
        "        \n",
        "        else:\n",
        "          print('Skip loading parameter {}, required shape{}, '\\\n",
        "                'loaded shape{}.'.format(\n",
        "            k, model_state_dict[k].shape, state_dict[k].shape))\n",
        "          state_dict[k] = model_state_dict[k]\n",
        "    else:\n",
        "      print('Drop parameter {}.'.format(k))\n",
        "  for k in model_state_dict:\n",
        "    if not (k in state_dict):\n",
        "      print('No param {}.'.format(k))\n",
        "      state_dict[k] = model_state_dict[k]\n",
        "  model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "  # freeze backbone network\n",
        "  if opt.freeze_backbone:\n",
        "    for (name, module) in model.named_children():\n",
        "      if name in opt.layers_to_freeze:\n",
        "        for (name, layer) in module.named_children():\n",
        "          for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "  # resume optimizer parameters\n",
        "  if optimizer is not None and opt.resume:\n",
        "    if 'optimizer' in checkpoint:\n",
        "      start_epoch = checkpoint['epoch']\n",
        "      start_lr = opt.lr\n",
        "      for step in opt.lr_step:\n",
        "        if start_epoch >= step:\n",
        "          start_lr *= 0.1\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = start_lr\n",
        "      print('Resumed optimizer with start lr', start_lr)\n",
        "    else:\n",
        "      print('No optimizer parameters in checkpoint.')\n",
        "  if optimizer is not None:\n",
        "    return model, optimizer, start_epoch\n",
        "  else:\n",
        "    return model\n",
        "\n",
        "def save_model(path, epoch, model, optimizer=None):\n",
        "  if isinstance(model, torch.nn.DataParallel):\n",
        "    state_dict = model.module.state_dict()\n",
        "  else:\n",
        "    state_dict = model.state_dict()\n",
        "  data = {'epoch': epoch,\n",
        "          'state_dict': state_dict}\n",
        "  if not (optimizer is None):\n",
        "    data['optimizer'] = optimizer.state_dict()\n",
        "  torch.save(data, path)"
      ],
      "metadata": {
        "id": "mIPe1urL9BB6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Losses"
      ],
      "metadata": {
        "id": "54KhcP93JQed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Portions of this code are from\n",
        "# CornerNet (https://github.com/princeton-vl/CornerNet)\n",
        "# Copyright (c) 2018, University of Michigan\n",
        "# Licensed under the BSD 3-Clause License\n",
        "# ------------------------------------------------------------------------------\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from .utils import _tranpose_and_gather_feat, _nms, _topk\n",
        "import torch.nn.functional as F\n",
        "# from utils.image import draw_umich_gaussian\n",
        "\n",
        "def _slow_neg_loss(pred, gt):\n",
        "  '''focal loss from CornerNet'''\n",
        "  pos_inds = gt.eq(1).float()\n",
        "  neg_inds = gt.lt(1).float()\n",
        "\n",
        "  neg_weights = torch.pow(1 - gt[neg_inds], 4)\n",
        "\n",
        "  loss = 0\n",
        "  pos_pred = pred[pos_inds]\n",
        "  neg_pred = pred[neg_inds]\n",
        "\n",
        "  pos_loss = torch.log(pos_pred) * torch.pow(1 - pos_pred, 2)\n",
        "  neg_loss = torch.log(1 - neg_pred) * torch.pow(neg_pred, 2) * neg_weights\n",
        "\n",
        "  num_pos  = pos_inds.float().sum()\n",
        "  pos_loss = pos_loss.sum()\n",
        "  neg_loss = neg_loss.sum()\n",
        "\n",
        "  if pos_pred.nelement() == 0:\n",
        "    loss = loss - neg_loss\n",
        "  else:\n",
        "    loss = loss - (pos_loss + neg_loss) / num_pos\n",
        "  return loss\n",
        "\n",
        "def _neg_loss(pred, gt):\n",
        "  ''' Reimplemented focal loss. Exactly the same as CornerNet.\n",
        "      Runs faster and costs a little bit more memory\n",
        "    Arguments:\n",
        "      pred (batch x c x h x w)\n",
        "      gt_regr (batch x c x h x w)\n",
        "  '''\n",
        "  pos_inds = gt.eq(1).float()\n",
        "  neg_inds = gt.lt(1).float()\n",
        "\n",
        "  neg_weights = torch.pow(1 - gt, 4)\n",
        "\n",
        "  loss = 0\n",
        "  pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds\n",
        "  neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_weights * neg_inds\n",
        "\n",
        "  num_pos  = pos_inds.float().sum()\n",
        "  pos_loss = pos_loss.sum()\n",
        "  neg_loss = neg_loss.sum()\n",
        "  if num_pos == 0:\n",
        "    loss = loss - neg_loss\n",
        "  else:\n",
        "    loss = loss - (pos_loss + neg_loss) / num_pos\n",
        "  return loss\n",
        "\n",
        "\n",
        "def _only_neg_loss(pred, gt):\n",
        "  gt = torch.pow(1 - gt, 4)\n",
        "  neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * gt\n",
        "  return neg_loss.sum()\n",
        "\n",
        "class FastFocalLoss(nn.Module):\n",
        "  '''\n",
        "  Reimplemented focal loss, exactly the same as the CornerNet version.\n",
        "  Faster and costs much less memory.\n",
        "  '''\n",
        "  def __init__(self, opt=None):\n",
        "    super(FastFocalLoss, self).__init__()\n",
        "    self.only_neg_loss = _only_neg_loss\n",
        "\n",
        "  def forward(self, out, target, ind, mask, cat):\n",
        "    '''\n",
        "    Arguments:\n",
        "      out, target: B x C x H x W\n",
        "      ind, mask: B x M\n",
        "      cat (category id for peaks): B x M\n",
        "    '''\n",
        "    neg_loss = self.only_neg_loss(out, target)\n",
        "    pos_pred_pix = _tranpose_and_gather_feat(out, ind) # B x M x C\n",
        "    pos_pred = pos_pred_pix.gather(2, cat.unsqueeze(2)) # B x M\n",
        "    num_pos = mask.sum()\n",
        "    pos_loss = torch.log(pos_pred) * torch.pow(1 - pos_pred, 2) * \\\n",
        "               mask.unsqueeze(2)\n",
        "    pos_loss = pos_loss.sum()\n",
        "    if num_pos == 0:\n",
        "      return - neg_loss\n",
        "    return - (pos_loss + neg_loss) / num_pos\n",
        "\n",
        "\n",
        "def _reg_loss(regr, gt_regr, mask):\n",
        "  ''' L1 regression loss\n",
        "    Arguments:\n",
        "      regr (batch x max_objects x dim)\n",
        "      gt_regr (batch x max_objects x dim)\n",
        "      mask (batch x max_objects)\n",
        "  '''\n",
        "  num = mask.float().sum()\n",
        "  mask = mask.unsqueeze(2).expand_as(gt_regr).float()\n",
        "\n",
        "  regr = regr * mask\n",
        "  gt_regr = gt_regr * mask\n",
        "    \n",
        "  regr_loss = nn.functional.smooth_l1_loss(regr, gt_regr, reduction='sum')\n",
        "  regr_loss = regr_loss / (num + 1e-4)\n",
        "  return regr_loss\n",
        "\n",
        "\n",
        "class RegWeightedL1Loss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RegWeightedL1Loss, self).__init__()\n",
        "  \n",
        "  def forward(self, output, mask, ind, target):\n",
        "    pred = _tranpose_and_gather_feat(output, ind)\n",
        "    loss = F.l1_loss(pred * mask, target * mask, reduction='sum')\n",
        "    loss = loss / (mask.sum() + 1e-4)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class WeightedBCELoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(WeightedBCELoss, self).__init__()\n",
        "    self.bceloss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "  def forward(self, output, mask, ind, target):\n",
        "    # output: B x F x H x W\n",
        "    # ind: B x M\n",
        "    # mask: B x M x F\n",
        "    # target: B x M x F\n",
        "    pred = _tranpose_and_gather_feat(output, ind) # B x M x F\n",
        "    loss = mask * self.bceloss(pred, target)\n",
        "    loss = loss.sum() / (mask.sum() + 1e-4)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class BinRotLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BinRotLoss, self).__init__()\n",
        "  \n",
        "  def forward(self, output, mask, ind, rotbin, rotres):\n",
        "    pred = _tranpose_and_gather_feat(output, ind)\n",
        "    loss = compute_rot_loss(pred, rotbin, rotres, mask)\n",
        "    return loss\n",
        "\n",
        "def compute_res_loss(output, target):\n",
        "    return F.smooth_l1_loss(output, target, reduction='mean')\n",
        "\n",
        "def compute_bin_loss(output, target, mask):\n",
        "    mask = mask.expand_as(output)\n",
        "    output = output * mask.float()\n",
        "    return F.cross_entropy(output, target, reduction='mean')\n",
        "\n",
        "def compute_rot_loss(output, target_bin, target_res, mask):\n",
        "    # output: (B, 128, 8) [bin1_cls[0], bin1_cls[1], bin1_sin, bin1_cos, \n",
        "    #                 bin2_cls[0], bin2_cls[1], bin2_sin, bin2_cos]\n",
        "    # target_bin: (B, 128, 2) [bin1_cls, bin2_cls]\n",
        "    # target_res: (B, 128, 2) [bin1_res, bin2_res]\n",
        "    # mask: (B, 128, 1)\n",
        "    output = output.view(-1, 8)\n",
        "    target_bin = target_bin.view(-1, 2)\n",
        "    target_res = target_res.view(-1, 2)\n",
        "    mask = mask.view(-1, 1)\n",
        "    loss_bin1 = compute_bin_loss(output[:, 0:2], target_bin[:, 0], mask)\n",
        "    loss_bin2 = compute_bin_loss(output[:, 4:6], target_bin[:, 1], mask)\n",
        "    loss_res = torch.zeros_like(loss_bin1)\n",
        "    if target_bin[:, 0].nonzero().shape[0] > 0:\n",
        "        idx1 = target_bin[:, 0].nonzero()[:, 0]\n",
        "        valid_output1 = torch.index_select(output, 0, idx1.long())\n",
        "        valid_target_res1 = torch.index_select(target_res, 0, idx1.long())\n",
        "        loss_sin1 = compute_res_loss(\n",
        "          valid_output1[:, 2], torch.sin(valid_target_res1[:, 0]))\n",
        "        loss_cos1 = compute_res_loss(\n",
        "          valid_output1[:, 3], torch.cos(valid_target_res1[:, 0]))\n",
        "        loss_res += loss_sin1 + loss_cos1\n",
        "    if target_bin[:, 1].nonzero().shape[0] > 0:\n",
        "        idx2 = target_bin[:, 1].nonzero()[:, 0]\n",
        "        valid_output2 = torch.index_select(output, 0, idx2.long())\n",
        "        valid_target_res2 = torch.index_select(target_res, 0, idx2.long())\n",
        "        loss_sin2 = compute_res_loss(\n",
        "          valid_output2[:, 6], torch.sin(valid_target_res2[:, 1]))\n",
        "        loss_cos2 = compute_res_loss(\n",
        "          valid_output2[:, 7], torch.cos(valid_target_res2[:, 1]))\n",
        "        loss_res += loss_sin2 + loss_cos2\n",
        "    return loss_bin1 + loss_bin2 + loss_res\n",
        "  \n",
        "\n",
        "## Loss function for depth with support for class-based depth\n",
        "class DepthLoss(nn.Module):\n",
        "  def __init__(self, opt=None):\n",
        "    super(DepthLoss, self).__init__()\n",
        "\n",
        "  def forward(self, output, target, ind, mask, cat):\n",
        "    '''\n",
        "    Arguments:\n",
        "      out, target: B x C x H x W\n",
        "      ind, mask: B x M\n",
        "      cat (category id for peaks): B x M\n",
        "    '''\n",
        "    pred = _tranpose_and_gather_feat(output, ind) # B x M x (C)\n",
        "    if pred.shape[2] > 1:\n",
        "      pred = pred.gather(2, cat.unsqueeze(2)) # B x M\n",
        "    loss = F.l1_loss(pred * mask, target * mask, reduction='sum')\n",
        "    loss = loss / (mask.sum() + 1e-4)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "o59lnLoIJToS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decode"
      ],
      "metadata": {
        "id": "z1ueCL7OJfQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from .utils import _gather_feat, _tranpose_and_gather_feat\n",
        "# from .utils import _nms, _topk, _topk_channel\n",
        "\n",
        "\n",
        "def _update_kps_with_hm(\n",
        "  kps, output, batch, num_joints, K, bboxes=None, scores=None):\n",
        "  if 'hm_hp' in output:\n",
        "    hm_hp = output['hm_hp']\n",
        "    hm_hp = _nms(hm_hp)\n",
        "    thresh = 0.2\n",
        "    kps = kps.view(batch, K, num_joints, 2).permute(\n",
        "        0, 2, 1, 3).contiguous() # b x J x K x 2\n",
        "    reg_kps = kps.unsqueeze(3).expand(batch, num_joints, K, K, 2)\n",
        "    hm_score, hm_inds, hm_ys, hm_xs = _topk_channel(hm_hp, K=K) # b x J x K\n",
        "    if 'hp_offset' in output or 'reg' in output:\n",
        "        hp_offset = output['hp_offset'] if 'hp_offset' in output \\\n",
        "                    else output['reg']\n",
        "        hp_offset = _tranpose_and_gather_feat(\n",
        "            hp_offset, hm_inds.view(batch, -1))\n",
        "        hp_offset = hp_offset.view(batch, num_joints, K, 2)\n",
        "        hm_xs = hm_xs + hp_offset[:, :, :, 0]\n",
        "        hm_ys = hm_ys + hp_offset[:, :, :, 1]\n",
        "    else:\n",
        "        hm_xs = hm_xs + 0.5\n",
        "        hm_ys = hm_ys + 0.5\n",
        "    \n",
        "    mask = (hm_score > thresh).float()\n",
        "    hm_score = (1 - mask) * -1 + mask * hm_score\n",
        "    hm_ys = (1 - mask) * (-10000) + mask * hm_ys\n",
        "    hm_xs = (1 - mask) * (-10000) + mask * hm_xs\n",
        "    hm_kps = torch.stack([hm_xs, hm_ys], dim=-1).unsqueeze(\n",
        "        2).expand(batch, num_joints, K, K, 2)\n",
        "    dist = (((reg_kps - hm_kps) ** 2).sum(dim=4) ** 0.5)\n",
        "    min_dist, min_ind = dist.min(dim=3) # b x J x K\n",
        "    hm_score = hm_score.gather(2, min_ind).unsqueeze(-1) # b x J x K x 1\n",
        "    min_dist = min_dist.unsqueeze(-1)\n",
        "    min_ind = min_ind.view(batch, num_joints, K, 1, 1).expand(\n",
        "        batch, num_joints, K, 1, 2)\n",
        "    hm_kps = hm_kps.gather(3, min_ind)\n",
        "    hm_kps = hm_kps.view(batch, num_joints, K, 2)        \n",
        "    mask = (hm_score < thresh)\n",
        "    \n",
        "    if bboxes is not None:\n",
        "      l = bboxes[:, :, 0].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      t = bboxes[:, :, 1].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      r = bboxes[:, :, 2].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      b = bboxes[:, :, 3].view(batch, 1, K, 1).expand(batch, num_joints, K, 1)\n",
        "      mask = (hm_kps[..., 0:1] < l) + (hm_kps[..., 0:1] > r) + \\\n",
        "              (hm_kps[..., 1:2] < t) + (hm_kps[..., 1:2] > b) + mask\n",
        "    else:\n",
        "      l = kps[:, :, :, 0:1].min(dim=1, keepdim=True)[0]\n",
        "      t = kps[:, :, :, 1:2].min(dim=1, keepdim=True)[0]\n",
        "      r = kps[:, :, :, 0:1].max(dim=1, keepdim=True)[0]\n",
        "      b = kps[:, :, :, 1:2].max(dim=1, keepdim=True)[0]\n",
        "      margin = 0.25\n",
        "      l = l - (r - l) * margin\n",
        "      r = r + (r - l) * margin\n",
        "      t = t - (b - t) * margin\n",
        "      b = b + (b - t) * margin\n",
        "      mask = (hm_kps[..., 0:1] < l) + (hm_kps[..., 0:1] > r) + \\\n",
        "              (hm_kps[..., 1:2] < t) + (hm_kps[..., 1:2] > b) + mask\n",
        "      # sc = (kps[:, :, :, :].max(dim=1, keepdim=True) - kps[:, :, :, :].min(dim=1))\n",
        "    # mask = mask + (min_dist > 10)\n",
        "    mask = (mask > 0).float()\n",
        "    kps_score = (1 - mask) * hm_score + mask * \\\n",
        "      scores.unsqueeze(-1).expand(batch, num_joints, K, 1) # bJK1\n",
        "    kps_score = scores * kps_score.mean(dim=1).view(batch, K)\n",
        "    # kps_score[scores < 0.1] = 0\n",
        "    mask = mask.expand(batch, num_joints, K, 2)\n",
        "    kps = (1 - mask) * hm_kps + mask * kps\n",
        "    kps = kps.permute(0, 2, 1, 3).contiguous().view(\n",
        "        batch, K, num_joints * 2)\n",
        "    return kps, kps_score\n",
        "  else:\n",
        "    return kps, kps\n",
        "\n",
        "\n",
        "\n",
        "## Decoder with Radar point cloud fusion support\n",
        "def fusion_decode(output, K=100, opt=None):\n",
        "  if not ('hm' in output):\n",
        "    return {}\n",
        "\n",
        "  if opt.zero_tracking:\n",
        "    output['tracking'] *= 0\n",
        "  \n",
        "  heat = output['hm']\n",
        "  batch, cat, height, width = heat.size()\n",
        "\n",
        "  heat = _nms(heat)\n",
        "  scores, inds, clses, ys0, xs0 = _topk(heat, K=K)\n",
        "\n",
        "  clses  = clses.view(batch, K)\n",
        "  scores = scores.view(batch, K)\n",
        "  bboxes = None\n",
        "  cts = torch.cat([xs0.unsqueeze(2), ys0.unsqueeze(2)], dim=2)\n",
        "  ret = {'scores': scores, 'clses': clses.float(), \n",
        "         'xs': xs0, 'ys': ys0, 'cts': cts}\n",
        "  if 'reg' in output:\n",
        "    reg = output['reg']\n",
        "    reg = _tranpose_and_gather_feat(reg, inds)\n",
        "    reg = reg.view(batch, K, 2)\n",
        "    xs = xs0.view(batch, K, 1) + reg[:, :, 0:1]\n",
        "    ys = ys0.view(batch, K, 1) + reg[:, :, 1:2]\n",
        "  else:\n",
        "    xs = xs0.view(batch, K, 1) + 0.5\n",
        "    ys = ys0.view(batch, K, 1) + 0.5\n",
        "\n",
        "  if 'wh' in output:\n",
        "    wh = output['wh']\n",
        "    wh = _tranpose_and_gather_feat(wh, inds) # B x K x (F)\n",
        "    # wh = wh.view(batch, K, -1)\n",
        "    wh = wh.view(batch, K, 2)\n",
        "    wh[wh < 0] = 0\n",
        "    if wh.size(2) == 2 * cat: # cat spec\n",
        "      wh = wh.view(batch, K, -1, 2)\n",
        "      cats = clses.view(batch, K, 1, 1).expand(batch, K, 1, 2)\n",
        "      wh = wh.gather(2, cats.long()).squeeze(2) # B x K x 2\n",
        "    else:\n",
        "      pass\n",
        "    bboxes = torch.cat([xs - wh[..., 0:1] / 2, \n",
        "                        ys - wh[..., 1:2] / 2,\n",
        "                        xs + wh[..., 0:1] / 2, \n",
        "                        ys + wh[..., 1:2] / 2], dim=2)\n",
        "    ret['bboxes'] = bboxes\n",
        " \n",
        "  if 'ltrb' in output:\n",
        "    ltrb = output['ltrb']\n",
        "    ltrb = _tranpose_and_gather_feat(ltrb, inds) # B x K x 4\n",
        "    ltrb = ltrb.view(batch, K, 4)\n",
        "    bboxes = torch.cat([xs0.view(batch, K, 1) + ltrb[..., 0:1], \n",
        "                        ys0.view(batch, K, 1) + ltrb[..., 1:2],\n",
        "                        xs0.view(batch, K, 1) + ltrb[..., 2:3], \n",
        "                        ys0.view(batch, K, 1) + ltrb[..., 3:4]], dim=2)\n",
        "    ret['bboxes'] = bboxes\n",
        "\n",
        "  ## Decode depth with depth residual support\n",
        "  if 'dep' in output:\n",
        "    dep = output['dep']\n",
        "    dep = _tranpose_and_gather_feat(dep, inds) # B x K x (C)\n",
        "    # dep = dep.view(batch, K, -1)\n",
        "    # dep[dep < 0] = 0\n",
        "    cats = clses.view(batch, K, 1, 1)\n",
        "    if dep.size(2) == cat: # cat spec\n",
        "      dep = dep.view(batch, K, -1, 1) # B x K x C x 1\n",
        "      dep = dep.gather(2, cats.long()).squeeze(2) # B x K x 1\n",
        "    \n",
        "    # add depth residuals to estimated depth values\n",
        "    if 'dep_sec' in output:\n",
        "      dep_sec = output['dep_sec']\n",
        "      dep_sec = _tranpose_and_gather_feat(dep_sec, inds) # B x K x [C]\n",
        "      if dep_sec.size(2) == cat: # cat spec\n",
        "        dep_sec = dep_sec.view(batch, K, -1, 1) # B x K x C x 1\n",
        "        dep_sec = dep_sec.gather(2, cats.long()).squeeze(2) # B x K x 1\n",
        "        dep_sec_mask = torch.tensor(dep_sec_mask, device=dep_sec.device).unsqueeze(0).unsqueeze(0).unsqueeze(3)\n",
        "      dep = dep_sec\n",
        "    \n",
        "    ret['dep'] = dep\n",
        "  \n",
        "\n",
        "  regression_heads = ['tracking', 'rot', 'dim', 'amodel_offset',\n",
        "    'nuscenes_att', 'velocity', 'rot_sec']\n",
        "\n",
        "  for head in regression_heads:\n",
        "    if head in output:\n",
        "      ret[head] = _tranpose_and_gather_feat(\n",
        "        output[head], inds).view(batch, K, -1)\n",
        "  \n",
        "  if 'rot_sec' in output:\n",
        "    ret['rot'] = ret['rot_sec']\n",
        "\n",
        "  if 'ltrb_amodal' in output:\n",
        "    ltrb_amodal = output['ltrb_amodal']\n",
        "    ltrb_amodal = _tranpose_and_gather_feat(ltrb_amodal, inds) # B x K x 4\n",
        "    ltrb_amodal = ltrb_amodal.view(batch, K, 4)\n",
        "    bboxes_amodal = torch.cat([xs0.view(batch, K, 1) + ltrb_amodal[..., 0:1], \n",
        "                          ys0.view(batch, K, 1) + ltrb_amodal[..., 1:2],\n",
        "                          xs0.view(batch, K, 1) + ltrb_amodal[..., 2:3], \n",
        "                          ys0.view(batch, K, 1) + ltrb_amodal[..., 3:4]], dim=2)\n",
        "    ret['bboxes_amodal'] = bboxes_amodal\n",
        "    ret['bboxes'] = bboxes_amodal\n",
        "\n",
        "  if 'hps' in output:\n",
        "    kps = output['hps']\n",
        "    num_joints = kps.shape[1] // 2\n",
        "    kps = _tranpose_and_gather_feat(kps, inds)\n",
        "    kps = kps.view(batch, K, num_joints * 2)\n",
        "    kps[..., ::2] += xs0.view(batch, K, 1).expand(batch, K, num_joints)\n",
        "    kps[..., 1::2] += ys0.view(batch, K, 1).expand(batch, K, num_joints)\n",
        "    kps, kps_score = _update_kps_with_hm(\n",
        "      kps, output, batch, num_joints, K, bboxes, scores)\n",
        "    ret['hps'] = kps\n",
        "    ret['kps_score'] = kps_score\n",
        "\n",
        "  if 'pre_inds' in output and output['pre_inds'] is not None:\n",
        "    pre_inds = output['pre_inds'] # B x pre_K\n",
        "    pre_K = pre_inds.shape[1]\n",
        "    pre_ys = (pre_inds / width).int().float()\n",
        "    pre_xs = (pre_inds % width).int().float()\n",
        "\n",
        "    ret['pre_cts'] = torch.cat(\n",
        "      [pre_xs.unsqueeze(2), pre_ys.unsqueeze(2)], dim=2)\n",
        "  \n",
        "  return ret\n"
      ],
      "metadata": {
        "id": "rqRhL60DJrwe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aiIIeJ6YJupn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logger"
      ],
      "metadata": {
        "id": "-JoDqBsSC7_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import torch\n",
        "import subprocess\n",
        "USE_TENSORBOARD = True\n",
        "try:\n",
        "  import tensorboardX\n",
        "  print('Using tensorboardX')\n",
        "except:\n",
        "  USE_TENSORBOARD = False\n",
        "\n",
        "class Logger(object):\n",
        "  def __init__(self, opt):\n",
        "    \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "    if not os.path.exists(opt.save_dir):\n",
        "      os.makedirs(opt.save_dir)\n",
        "    if not os.path.exists(opt.debug_dir):\n",
        "      os.makedirs(opt.debug_dir)\n",
        "   \n",
        "    time_str = time.strftime('%Y-%m-%d-%H-%M')\n",
        "\n",
        "    args = dict((name, getattr(opt, name)) for name in dir(opt)\n",
        "                if not name.startswith('_'))\n",
        "    file_name = os.path.join(opt.save_dir, 'opt.txt')\n",
        "    with open(file_name, 'wt') as opt_file:\n",
        "      opt_file.write('==> commit hash: {}\\n'.format(\n",
        "        # subprocess.check_output([\"git\", \"describe\"])))\n",
        "        subprocess.check_output([\"git\", \"describe\", \"--always\"])))\n",
        "      opt_file.write('==> torch version: {}\\n'.format(torch.__version__))\n",
        "      opt_file.write('==> cudnn version: {}\\n'.format(\n",
        "        torch.backends.cudnn.version()))\n",
        "      opt_file.write('==> Cmd:\\n')\n",
        "      opt_file.write(str(sys.argv))\n",
        "      opt_file.write('\\n==> Opt:\\n')\n",
        "      for k, v in sorted(args.items()):\n",
        "        opt_file.write('  %s: %s\\n' % (str(k), str(v)))\n",
        "          \n",
        "    log_dir = opt.save_dir + '/logs_{}'.format(time_str)\n",
        "    if USE_TENSORBOARD:\n",
        "      self.writer = tensorboardX.SummaryWriter(log_dir=log_dir)\n",
        "    else:\n",
        "      if not os.path.exists(os.path.dirname(log_dir)):\n",
        "        os.mkdir(os.path.dirname(log_dir))\n",
        "      if not os.path.exists(log_dir):\n",
        "        os.mkdir(log_dir)\n",
        "    self.log = open(log_dir + '/log.txt', 'w')\n",
        "    try:\n",
        "      os.system('cp {}/opt.txt {}/'.format(opt.save_dir, log_dir))\n",
        "    except:\n",
        "      pass\n",
        "    self.start_line = True\n",
        "\n",
        "  def write(self, txt):\n",
        "    if self.start_line:\n",
        "      time_str = time.strftime('%Y-%m-%d-%H-%M')\n",
        "      self.log.write('{}: {}'.format(time_str, txt))\n",
        "    else:\n",
        "      self.log.write(txt)  \n",
        "    self.start_line = False\n",
        "    if '\\n' in txt:\n",
        "      self.start_line = True\n",
        "      self.log.flush()\n",
        "  \n",
        "  def close(self):\n",
        "    self.log.close()\n",
        "  \n",
        "  def scalar_summary(self, tag, value, step):\n",
        "    \"\"\"Log a scalar variable.\"\"\"\n",
        "    if USE_TENSORBOARD:\n",
        "      self.writer.add_scalar(tag, value, step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt_pRgptC9rj",
        "outputId": "7d145c2f-e19b-4b97-9a21-37b225ddd84b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorboardX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "zSj7TX8EDLB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from progress.bar import Bar\n",
        "\n",
        "# from model.data_parallel import DataParallel\n",
        "# from utils.utils import AverageMeter\n",
        "\n",
        "# from model.losses import FastFocalLoss, RegWeightedL1Loss\n",
        "# from model.losses import BinRotLoss, WeightedBCELoss\n",
        "# from model.decode import fusion_decode\n",
        "# from model.utils import _sigmoid, flip_tensor, flip_lr_off, flip_lr\n",
        "# from utils.debugger import Debugger\n",
        "# from utils.post_process import generic_post_process\n",
        "# from model.losses import DepthLoss\n",
        "# from utils.pointcloud import generate_pc_hm\n",
        "\n",
        "import cv2\n",
        "class GenericLoss(torch.nn.Module):\n",
        "  def __init__(self, opt):\n",
        "    super(GenericLoss, self).__init__()\n",
        "    self.crit = FastFocalLoss(opt=opt)\n",
        "    self.crit_reg = RegWeightedL1Loss()\n",
        "    if 'rot' in opt.heads:\n",
        "      self.crit_rot = BinRotLoss()\n",
        "    if 'nuscenes_att' in opt.heads:\n",
        "      self.crit_nuscenes_att = WeightedBCELoss()\n",
        "    self.opt = opt\n",
        "    self.crit_dep = DepthLoss()\n",
        "\n",
        "  def _sigmoid_output(self, output):\n",
        "    if 'hm' in output:\n",
        "      output['hm'] = _sigmoid(output['hm'])\n",
        "    if 'hm_hp' in output:\n",
        "      output['hm_hp'] = _sigmoid(output['hm_hp'])\n",
        "    if 'dep' in output:\n",
        "      output['dep'] = 1. / (output['dep'].sigmoid() + 1e-6) - 1.\n",
        "    if 'dep_sec' in output and self.opt.sigmoid_dep_sec:\n",
        "      output['dep_sec'] = 1. / (output['dep_sec'].sigmoid() + 1e-6) - 1.\n",
        "    return output\n",
        "\n",
        "  def forward(self, outputs, batch):\n",
        "    opt = self.opt\n",
        "    losses = {head: 0 for head in opt.heads}\n",
        "\n",
        "    for s in range(opt.num_stacks):\n",
        "      output = outputs[s]\n",
        "      output = self._sigmoid_output(output)\n",
        "\n",
        "      if 'hm' in output:\n",
        "        losses['hm'] += self.crit(\n",
        "          output['hm'], batch['hm'], batch['ind'], \n",
        "          batch['mask'], batch['cat']) / opt.num_stacks\n",
        "      \n",
        "      if 'dep' in output:\n",
        "        losses['dep'] += self.crit_dep(\n",
        "          output['dep'], batch['dep'], batch['ind'], \n",
        "          batch['dep_mask'], batch['cat']) / opt.num_stacks\n",
        "\n",
        "      regression_heads = [\n",
        "        'reg', 'wh', 'tracking', 'ltrb', 'ltrb_amodal', 'hps', \n",
        "        'dim', 'amodel_offset', 'velocity']\n",
        "\n",
        "      for head in regression_heads:\n",
        "        if head in output:\n",
        "          losses[head] += self.crit_reg(\n",
        "            output[head], batch[head + '_mask'],\n",
        "            batch['ind'], batch[head]) / opt.num_stacks\n",
        "      \n",
        "      if 'hm_hp' in output:\n",
        "        losses['hm_hp'] += self.crit(\n",
        "          output['hm_hp'], batch['hm_hp'], batch['hp_ind'], \n",
        "          batch['hm_hp_mask'], batch['joint']) / opt.num_stacks\n",
        "        if 'hp_offset' in output:\n",
        "          losses['hp_offset'] += self.crit_reg(\n",
        "            output['hp_offset'], batch['hp_offset_mask'],\n",
        "            batch['hp_ind'], batch['hp_offset']) / opt.num_stacks\n",
        "        \n",
        "      if 'rot' in output:\n",
        "        losses['rot'] += self.crit_rot(\n",
        "          output['rot'], batch['rot_mask'], batch['ind'], batch['rotbin'],\n",
        "          batch['rotres']) / opt.num_stacks\n",
        "\n",
        "      if 'nuscenes_att' in output:\n",
        "        losses['nuscenes_att'] += self.crit_nuscenes_att(\n",
        "          output['nuscenes_att'], batch['nuscenes_att_mask'],\n",
        "          batch['ind'], batch['nuscenes_att']) / opt.num_stacks\n",
        "      \n",
        "      if 'dep_sec' in output:\n",
        "        losses['dep_sec'] += self.crit_dep(\n",
        "          output['dep_sec'], batch['dep'], batch['ind'], \n",
        "          batch['dep_mask'], batch['cat']) / opt.num_stacks\n",
        "      \n",
        "      if 'rot_sec' in output:\n",
        "        losses['rot_sec'] += self.crit_rot(\n",
        "          output['rot_sec'], batch['rot_mask'], batch['ind'], batch['rotbin'],\n",
        "          batch['rotres']) / opt.num_stacks\n",
        "\n",
        "    losses['tot'] = 0\n",
        "    for head in opt.heads:\n",
        "      losses['tot'] += opt.weights[head] * losses[head]\n",
        "\n",
        "    return losses['tot'], losses\n",
        "\n",
        "\n",
        "class ModelWithLoss(torch.nn.Module):\n",
        "  def __init__(self, model, loss, opt):\n",
        "    super(ModelWithLoss, self).__init__()\n",
        "    self.opt = opt\n",
        "    self.model = model\n",
        "    self.loss = loss\n",
        "  \n",
        "  def forward(self, batch, phase):\n",
        "    pc_dep = batch.get('pc_dep', None)\n",
        "    pc_hm = batch.get('pc_hm', None)\n",
        "    calib = batch['calib'].squeeze(0)\n",
        "\n",
        "    ## run the first stage\n",
        "    outputs = self.model(batch['image'], pc_hm=pc_hm, pc_dep=pc_dep, calib=calib)\n",
        "    \n",
        "    loss, loss_stats = self.loss(outputs, batch)\n",
        "    return outputs[-1], loss, loss_stats\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "  def __init__(\n",
        "    self, opt, model, optimizer=None):\n",
        "    self.opt = opt\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_stats, self.loss = self._get_losses(opt)\n",
        "    self.model_with_loss = ModelWithLoss(model, self.loss, opt)\n",
        "\n",
        "  def set_device(self, gpus, chunk_sizes, device):\n",
        "    if len(gpus) > 1:\n",
        "      self.model_with_loss = DataParallel(\n",
        "        self.model_with_loss, device_ids=gpus, \n",
        "        chunk_sizes=chunk_sizes).to(device)\n",
        "    else:\n",
        "      self.model_with_loss = self.model_with_loss.to(device)\n",
        "    \n",
        "    for state in self.optimizer.state.values():\n",
        "      for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "          state[k] = v.to(device=device, non_blocking=True)\n",
        "\n",
        "  def run_epoch(self, phase, epoch, data_loader):\n",
        "    model_with_loss = self.model_with_loss\n",
        "    if phase == 'train':\n",
        "      model_with_loss.train()\n",
        "    else:\n",
        "      if len(self.opt.gpus) > 1:\n",
        "        model_with_loss = self.model_with_loss.module\n",
        "      model_with_loss.eval()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    opt = self.opt\n",
        "    results = {}\n",
        "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
        "    avg_loss_stats = {l: AverageMeter() for l in self.loss_stats \\\n",
        "                      if l == 'tot' or opt.weights[l] > 0}\n",
        "    num_iters = len(data_loader) if opt.num_iters < 0 else opt.num_iters\n",
        "    bar = Bar('{}/{}'.format(opt.task, opt.exp_id), max=num_iters)\n",
        "    end = time.time()\n",
        "    \n",
        "    for iter_id, batch in enumerate(data_loader):\n",
        "      if iter_id >= num_iters:\n",
        "        break\n",
        "      data_time.update(time.time() - end)\n",
        "      for k in batch:\n",
        "        if k != 'meta':\n",
        "          batch[k] = batch[k].to(device=opt.device, non_blocking=True)  \n",
        "      \n",
        "      # run one iteration \n",
        "      output, loss, loss_stats = model_with_loss(batch, phase)\n",
        "      \n",
        "      # backpropagate and step optimizer\n",
        "      loss = loss.mean()\n",
        "      if phase == 'train':\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "      batch_time.update(time.time() - end)\n",
        "      end = time.time()\n",
        "\n",
        "      Bar.suffix = '{phase}: [{0}][{1}/{2}]|Tot: {total:} |ETA: {eta:} '.format(\n",
        "        epoch, iter_id, num_iters, phase=phase,\n",
        "        total=bar.elapsed_td, eta=bar.eta_td)\n",
        "      for l in avg_loss_stats:\n",
        "        avg_loss_stats[l].update(\n",
        "          loss_stats[l].mean().item(), batch['image'].size(0))\n",
        "        Bar.suffix = Bar.suffix + '|{} {:.4f} '.format(l, avg_loss_stats[l].avg)\n",
        "      Bar.suffix = Bar.suffix + '|Data {dt.val:.3f}s({dt.avg:.3f}s) ' \\\n",
        "        '|Net {bt.avg:.3f}s'.format(dt=data_time, bt=batch_time)\n",
        "      if opt.print_iter > 0: # If not using progress bar\n",
        "        if iter_id % opt.print_iter == 0:\n",
        "          print('{}/{}| {}'.format(opt.task, opt.exp_id, Bar.suffix)) \n",
        "      else:\n",
        "        bar.next()\n",
        "      \n",
        "      if opt.debug > 0:\n",
        "        self.debug(batch, output, iter_id, dataset=data_loader.dataset)\n",
        "      \n",
        "      # generate detections for evaluation\n",
        "      if (phase == 'val' and (opt.run_dataset_eval or opt.eval)):\n",
        "        meta = batch['meta']\n",
        "        dets = fusion_decode(output, K=opt.K, opt=opt)\n",
        "\n",
        "        for k in dets:\n",
        "          dets[k] = dets[k].detach().cpu().numpy()\n",
        "\n",
        "        calib = meta['calib'].detach().numpy() if 'calib' in meta else None\n",
        "        dets = generic_post_process(opt, dets, \n",
        "          meta['c'].cpu().numpy(), meta['s'].cpu().numpy(),\n",
        "          output['hm'].shape[2], output['hm'].shape[3], self.opt.num_classes,\n",
        "          calib)\n",
        "\n",
        "        # merge results\n",
        "        result = []\n",
        "        for i in range(len(dets[0])):\n",
        "          if dets[0][i]['score'] > self.opt.out_thresh and all(dets[0][i]['dim'] > 0):\n",
        "            result.append(dets[0][i])\n",
        "\n",
        "        img_id = batch['meta']['img_id'].numpy().astype(np.int32)[0]\n",
        "        results[img_id] = result\n",
        " \n",
        "      del output, loss, loss_stats\n",
        "    \n",
        "    bar.finish()\n",
        "    ret = {k: v.avg for k, v in avg_loss_stats.items()}\n",
        "    ret['time'] = bar.elapsed_td.total_seconds() / 60.\n",
        "    return ret, results\n",
        "\n",
        "\n",
        "  def _get_losses(self, opt):\n",
        "    loss_order = ['hm', 'wh', 'reg', 'ltrb', 'hps', 'hm_hp', \\\n",
        "      'hp_offset', 'dep', 'dep_sec', 'dim', 'rot', 'rot_sec',\n",
        "      'amodel_offset', 'ltrb_amodal', 'tracking', 'nuscenes_att', 'velocity']\n",
        "    loss_states = ['tot'] + [k for k in loss_order if k in opt.heads]\n",
        "    loss = GenericLoss(opt)\n",
        "    return loss_states, loss\n",
        "\n",
        "\n",
        "  def debug(self, batch, output, iter_id, dataset):\n",
        "    opt = self.opt\n",
        "    if 'pre_hm' in batch:\n",
        "      output.update({'pre_hm': batch['pre_hm']})\n",
        "    dets = fusion_decode(output, K=opt.K, opt=opt)\n",
        "    for k in dets:\n",
        "      dets[k] = dets[k].detach().cpu().numpy()\n",
        "    dets_gt = batch['meta']['gt_det']\n",
        "    for i in range(1):\n",
        "      debugger = Debugger(opt=opt, dataset=dataset)\n",
        "      img = batch['image'][i].detach().cpu().numpy().transpose(1, 2, 0)\n",
        "      img = np.clip(((\n",
        "        img * dataset.std + dataset.mean) * 255.), 0, 255).astype(np.uint8)\n",
        "      pred = debugger.gen_colormap(output['hm'][i].detach().cpu().numpy())\n",
        "      gt = debugger.gen_colormap(batch['hm'][i].detach().cpu().numpy())\n",
        "      debugger.add_blend_img(img, pred, 'pred_hm', trans=self.opt.hm_transparency)\n",
        "      debugger.add_blend_img(img, gt, 'gt_hm', trans=self.opt.hm_transparency)\n",
        "      \n",
        "      debugger.add_img(img, img_id='img')\n",
        "      \n",
        "      # show point clouds\n",
        "      if opt.pointcloud:\n",
        "        pc_2d = batch['pc_2d'][i].detach().cpu().numpy()\n",
        "        pc_3d = None\n",
        "        pc_N = batch['pc_N'][i].detach().cpu().numpy()\n",
        "        debugger.add_img(img, img_id='pc')\n",
        "        debugger.add_pointcloud(pc_2d, pc_N, img_id='pc')\n",
        "        \n",
        "        if 'pc_hm' in opt.pc_feat_lvl:\n",
        "          channel = opt.pc_feat_channels['pc_hm']\n",
        "          pc_hm = debugger.gen_colormap(batch['pc_hm'][i][channel].unsqueeze(0).detach().cpu().numpy())\n",
        "          debugger.add_blend_img(img, pc_hm, 'pc_hm', trans=self.opt.hm_transparency)\n",
        "        if 'pc_dep' in opt.pc_feat_lvl:\n",
        "          channel = opt.pc_feat_channels['pc_dep']\n",
        "          pc_hm = batch['pc_hm'][i][channel].unsqueeze(0).detach().cpu().numpy()\n",
        "          pc_dep = debugger.add_overlay_img(img, pc_hm, 'pc_dep')\n",
        "          \n",
        "\n",
        "      if 'pre_img' in batch:\n",
        "        pre_img = batch['pre_img'][i].detach().cpu().numpy().transpose(1, 2, 0)\n",
        "        pre_img = np.clip(((\n",
        "          pre_img * dataset.std + dataset.mean) * 255), 0, 255).astype(np.uint8)\n",
        "        debugger.add_img(pre_img, 'pre_img_pred')\n",
        "        debugger.add_img(pre_img, 'pre_img_gt')\n",
        "        if 'pre_hm' in batch:\n",
        "          pre_hm = debugger.gen_colormap(\n",
        "            batch['pre_hm'][i].detach().cpu().numpy())\n",
        "          debugger.add_blend_img(pre_img, pre_hm, 'pre_hm', trans=self.opt.hm_transparency)\n",
        "\n",
        "      debugger.add_img(img, img_id='out_pred')\n",
        "      if 'ltrb_amodal' in opt.heads:\n",
        "        debugger.add_img(img, img_id='out_pred_amodal')\n",
        "        debugger.add_img(img, img_id='out_gt_amodal')\n",
        "\n",
        "      # Predictions\n",
        "      for k in range(len(dets['scores'][i])):\n",
        "        if dets['scores'][i, k] > opt.vis_thresh:\n",
        "          debugger.add_coco_bbox(\n",
        "            dets['bboxes'][i, k] * opt.down_ratio, dets['clses'][i, k],\n",
        "            dets['scores'][i, k], img_id='out_pred')\n",
        "\n",
        "          if 'ltrb_amodal' in opt.heads:\n",
        "            debugger.add_coco_bbox(\n",
        "              dets['bboxes_amodal'][i, k] * opt.down_ratio, dets['clses'][i, k],\n",
        "              dets['scores'][i, k], img_id='out_pred_amodal')\n",
        "\n",
        "          if 'hps' in opt.heads and int(dets['clses'][i, k]) == 0:\n",
        "            debugger.add_coco_hp(\n",
        "              dets['hps'][i, k] * opt.down_ratio, img_id='out_pred')\n",
        "\n",
        "          if 'tracking' in opt.heads:\n",
        "            debugger.add_arrow(\n",
        "              dets['cts'][i][k] * opt.down_ratio, \n",
        "              dets['tracking'][i][k] * opt.down_ratio, img_id='out_pred')\n",
        "            debugger.add_arrow(\n",
        "              dets['cts'][i][k] * opt.down_ratio, \n",
        "              dets['tracking'][i][k] * opt.down_ratio, img_id='pre_img_pred')\n",
        "\n",
        "      # Ground truth\n",
        "      debugger.add_img(img, img_id='out_gt')\n",
        "      for k in range(len(dets_gt['scores'][i])):\n",
        "        if dets_gt['scores'][i][k] > opt.vis_thresh:\n",
        "          if 'dep' in dets_gt.keys():\n",
        "            dist = dets_gt['dep'][i][k]\n",
        "            if len(dist)>1:\n",
        "              dist = dist[0]\n",
        "          else:\n",
        "            dist = -1\n",
        "          debugger.add_coco_bbox(\n",
        "            dets_gt['bboxes'][i][k] * opt.down_ratio, dets_gt['clses'][i][k],\n",
        "            dets_gt['scores'][i][k], img_id='out_gt', dist=dist)\n",
        "\n",
        "          if 'ltrb_amodal' in opt.heads:\n",
        "            debugger.add_coco_bbox(\n",
        "              dets_gt['bboxes_amodal'][i, k] * opt.down_ratio, \n",
        "              dets_gt['clses'][i, k],\n",
        "              dets_gt['scores'][i, k], img_id='out_gt_amodal')\n",
        "\n",
        "          if 'hps' in opt.heads and \\\n",
        "            (int(dets['clses'][i, k]) == 0):\n",
        "            debugger.add_coco_hp(\n",
        "              dets_gt['hps'][i][k] * opt.down_ratio, img_id='out_gt')\n",
        "\n",
        "          if 'tracking' in opt.heads:\n",
        "            debugger.add_arrow(\n",
        "              dets_gt['cts'][i][k] * opt.down_ratio, \n",
        "              dets_gt['tracking'][i][k] * opt.down_ratio, img_id='out_gt')\n",
        "            debugger.add_arrow(\n",
        "              dets_gt['cts'][i][k] * opt.down_ratio, \n",
        "              dets_gt['tracking'][i][k] * opt.down_ratio, img_id='pre_img_gt')\n",
        "\n",
        "      if 'hm_hp' in opt.heads:\n",
        "        pred = debugger.gen_colormap_hp(\n",
        "          output['hm_hp'][i].detach().cpu().numpy())\n",
        "        gt = debugger.gen_colormap_hp(batch['hm_hp'][i].detach().cpu().numpy())\n",
        "        debugger.add_blend_img(img, pred, 'pred_hmhp', trans=self.opt.hm_transparency)\n",
        "        debugger.add_blend_img(img, gt, 'gt_hmhp', trans=self.opt.hm_transparency)\n",
        "\n",
        "\n",
        "      if 'rot' in opt.heads and 'dim' in opt.heads and 'dep' in opt.heads:\n",
        "        dets_gt = {k: dets_gt[k].cpu().numpy() for k in dets_gt}\n",
        "        calib = batch['meta']['calib'].detach().numpy() \\\n",
        "                if 'calib' in batch['meta'] else None\n",
        "        det_pred = generic_post_process(opt, dets, \n",
        "          batch['meta']['c'].cpu().numpy(), batch['meta']['s'].cpu().numpy(),\n",
        "          output['hm'].shape[2], output['hm'].shape[3], self.opt.num_classes,\n",
        "          calib)\n",
        "        det_gt = generic_post_process(opt, dets_gt, \n",
        "          batch['meta']['c'].cpu().numpy(), batch['meta']['s'].cpu().numpy(),\n",
        "          output['hm'].shape[2], output['hm'].shape[3], self.opt.num_classes,\n",
        "          calib, is_gt=True)\n",
        "\n",
        "        debugger.add_3d_detection(\n",
        "          batch['meta']['img_path'][i], batch['meta']['flipped'][i],\n",
        "          det_pred[i], calib[i],\n",
        "          vis_thresh=opt.vis_thresh, img_id='add_pred')\n",
        "        debugger.add_3d_detection(\n",
        "          batch['meta']['img_path'][i], batch['meta']['flipped'][i], \n",
        "          det_gt[i], calib[i],\n",
        "          vis_thresh=opt.vis_thresh, img_id='add_gt')\n",
        "        \n",
        "        pc_3d = None\n",
        "        if opt.pointcloud:\n",
        "          pc_3d=batch['pc_3d'].cpu().numpy()\n",
        "\n",
        "        debugger.add_bird_views(det_pred[i], det_gt[i], vis_thresh=opt.vis_thresh, \n",
        "          img_id='bird_pred_gt', pc_3d=pc_3d, show_velocity=opt.show_velocity)\n",
        "        debugger.add_bird_views([], det_gt[i], vis_thresh=opt.vis_thresh, \n",
        "          img_id='bird_gt', pc_3d=pc_3d, show_velocity=opt.show_velocity)\n",
        "\n",
        "      if opt.debug == 4:\n",
        "        debugger.save_all_imgs(opt.debug_dir, prefix='{}'.format(iter_id))\n",
        "      else:\n",
        "        debugger.show_all_imgs(pause=True)\n",
        "  \n",
        "  def val(self, epoch, data_loader):\n",
        "    return self.run_epoch('val', epoch, data_loader)\n",
        "\n",
        "  def train(self, epoch, data_loader):\n",
        "    return self.run_epoch('train', epoch, data_loader)\n"
      ],
      "metadata": {
        "id": "EsMfiA7xDM82"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "1ioQc1WK7vEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "_cCxeISBOx87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import chunk\n",
        "\n",
        "# import _init_paths\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "# from opts import opts\n",
        "# from model.model import create_model, load_model, save_model\n",
        "# from model.data_parallel import DataParallel\n",
        "# from logger import Logger\n",
        "# from dataset.dataset_factory import get_dataset\n",
        "# from trainer import Trainer\n",
        "# from test import prefetch_test\n",
        "import json\n",
        "\n",
        "def get_optimizer(opt, model):\n",
        "  if opt.optim == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), opt.lr)\n",
        "  elif opt.optim == 'sgd':\n",
        "    print('Using SGD')\n",
        "    optimizer = torch.optim.SGD(\n",
        "      model.parameters(), opt.lr, momentum=0.9, weight_decay=0.0001)\n",
        "  else:\n",
        "    assert 0, opt.optim\n",
        "  return optimizer\n",
        "\n",
        "def main(opt):\n",
        "  torch.manual_seed(opt.seed)\n",
        "  torch.backends.cudnn.benchmark = not opt.not_cuda_benchmark and not opt.eval\n",
        "  Dataset = get_dataset(opt.dataset)\n",
        "  opt = opt.update_dataset_info_and_set_heads(opt, Dataset)\n",
        "  print(opt)\n",
        "  if not opt.not_set_cuda_env:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpus_str\n",
        "  opt.device = torch.device('cuda' if opt.gpus[0] >= 0 else 'cpu')\n",
        "  # logger = Logger(opt)\n",
        "\n",
        "  print('Creating model...')\n",
        "  model = create_model(opt.arch, opt.heads, opt.head_conv, opt=opt)\n",
        "  optimizer = get_optimizer(opt, model)\n",
        "  start_epoch = 0\n",
        "  lr = opt.lr\n",
        "\n",
        "  if opt.load_model != '':\n",
        "    model, optimizer, start_epoch = load_model(\n",
        "      model, opt.load_model, opt, optimizer)\n",
        "\n",
        "  trainer = Trainer(opt, model, optimizer)\n",
        "  print(opt.device, opt.chunk_sizes, opt.gpus)\n",
        "  trainer.set_device(opt.gpus, opt.chunk_sizes, opt.device)\n",
        "  \n",
        "  if opt.val_intervals < opt.num_epochs or opt.eval:\n",
        "    print('Setting up validation data...')\n",
        "    print(opt.val_split, opt)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "      Dataset(opt, opt.val_split), batch_size=1, shuffle=False, \n",
        "              num_workers=1, pin_memory=True)\n",
        "\n",
        "    if opt.eval:\n",
        "      _, preds = trainer.val(0, val_loader)\n",
        "      val_loader.dataset.run_eval(preds, opt.save_dir, n_plots=opt.eval_n_plots, \n",
        "                                  render_curves=opt.eval_render_curves)\n",
        "      return\n",
        "\n",
        "  print('Setting up train data...')\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      Dataset(opt, opt.train_split), batch_size=opt.batch_size, \n",
        "        shuffle=opt.shuffle_train, num_workers=opt.num_workers, \n",
        "        pin_memory=True, drop_last=True\n",
        "  )\n",
        "\n",
        "  print('Starting training...')\n",
        "  for epoch in range(start_epoch + 1, opt.num_epochs + 1):\n",
        "    mark = epoch if opt.save_all else 'last'\n",
        "\n",
        "    # log learning rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "      lr = param_group['lr']\n",
        "      logger.scalar_summary('LR', lr, epoch)\n",
        "      break\n",
        "    \n",
        "    # train one epoch\n",
        "    log_dict_train, _ = trainer.train(epoch, train_loader)\n",
        "    logger.write('epoch: {} |'.format(epoch))\n",
        "    \n",
        "    # log train results\n",
        "    for k, v in log_dict_train.items():\n",
        "      logger.scalar_summary('train_{}'.format(k), v, epoch)\n",
        "      logger.write('{} {:8f} | '.format(k, v))\n",
        "    \n",
        "    # evaluate\n",
        "    if opt.val_intervals > 0 and epoch % opt.val_intervals == 0:\n",
        "      save_model(os.path.join(opt.save_dir, 'model_{}.pth'.format(mark)), \n",
        "                 epoch, model, optimizer)\n",
        "      with torch.no_grad():\n",
        "        log_dict_val, preds = trainer.val(epoch, val_loader)\n",
        "        \n",
        "        # evaluate val set using dataset-specific evaluator\n",
        "        if opt.run_dataset_eval:\n",
        "          out_dir = val_loader.dataset.run_eval(preds, opt.save_dir, \n",
        "                                                n_plots=opt.eval_n_plots, \n",
        "                                                render_curves=opt.eval_render_curves)\n",
        "          \n",
        "          # log dataset-specific evaluation metrics\n",
        "          with open('{}/metrics_summary.json'.format(out_dir), 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "          logger.scalar_summary('AP/overall', metrics['mean_ap']*100.0, epoch)\n",
        "          for k,v in metrics['mean_dist_aps'].items():\n",
        "            logger.scalar_summary('AP/{}'.format(k), v*100.0, epoch)\n",
        "          for k,v in metrics['tp_errors'].items():\n",
        "            logger.scalar_summary('Scores/{}'.format(k), v, epoch)\n",
        "          logger.scalar_summary('Scores/NDS', metrics['nd_score'], epoch)\n",
        "      \n",
        "      # log eval results\n",
        "      for k, v in log_dict_val.items():\n",
        "        logger.scalar_summary('val_{}'.format(k), v, epoch)\n",
        "        logger.write('{} {:8f} | '.format(k, v))\n",
        "    \n",
        "    # save this checkpoint\n",
        "    else:\n",
        "      save_model(os.path.join(opt.save_dir, 'model_last.pth'), \n",
        "                 epoch, model, optimizer)\n",
        "    logger.write('\\n')\n",
        "    if epoch in opt.save_point:\n",
        "      save_model(os.path.join(opt.save_dir, 'model_{}.pth'.format(epoch)), \n",
        "                 epoch, model, optimizer)\n",
        "    \n",
        "    # update learning rate\n",
        "    if epoch in opt.lr_step:\n",
        "      lr = opt.lr * (0.1 ** (opt.lr_step.index(epoch) + 1))\n",
        "      print('Drop LR to', lr)\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = lr\n",
        "\n",
        "  logger.close()\n"
      ],
      "metadata": {
        "id": "-T8q3Q1MOzgS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Detector"
      ],
      "metadata": {
        "id": "D4ZXROYiOZDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import cv2\n",
        "import copy\n",
        "import numpy as np\n",
        "from progress.bar import Bar\n",
        "import time\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# from model.model import create_model, load_model\n",
        "# from model.decode import fusion_decode #, generic_decode\n",
        "# from model.utils import flip_tensor, flip_lr_off, flip_lr\n",
        "# from utils.image import get_affine_transform, affine_transform\n",
        "# from utils.image import draw_umich_gaussian, gaussian_radius\n",
        "# from utils.post_process import generic_post_process\n",
        "# from utils.debugger import Debugger\n",
        "# from utils.tracker import Tracker\n",
        "# from dataset.dataset_factory import get_dataset\n",
        "# from utils.pointcloud import generate_pc_hm\n",
        "\n",
        "\n",
        "class Detector(object):\n",
        "  def __init__(self, opt):\n",
        "    if opt.gpus[0] >= 0:\n",
        "      opt.device = torch.device('cuda')\n",
        "    else:\n",
        "      opt.device = torch.device('cpu')\n",
        "    \n",
        "    print('Creating model...')\n",
        "    self.model = create_model(\n",
        "      opt.arch, opt.heads, opt.head_conv, opt=opt)\n",
        "    self.model = load_model(self.model, opt.load_model, opt)\n",
        "    self.model = self.model.to(opt.device)\n",
        "    self.model.eval()\n",
        "\n",
        "    self.opt = opt\n",
        "    self.trained_dataset = get_dataset(opt.dataset)\n",
        "    self.mean = np.array(\n",
        "      self.trained_dataset.mean, dtype=np.float32).reshape(1, 1, 3)\n",
        "    self.std = np.array(\n",
        "      self.trained_dataset.std, dtype=np.float32).reshape(1, 1, 3)\n",
        "    self.pause = not opt.no_pause\n",
        "    self.rest_focal_length = self.trained_dataset.rest_focal_length \\\n",
        "      if self.opt.test_focal_length < 0 else self.opt.test_focal_length\n",
        "    self.flip_idx = self.trained_dataset.flip_idx\n",
        "    self.cnt = 0\n",
        "    self.pre_images = None\n",
        "    self.pre_image_ori = None\n",
        "    self.tracker = Tracker(opt)\n",
        "    self.debugger = Debugger(opt=opt, dataset=self.trained_dataset)\n",
        "\n",
        "\n",
        "  def run(self, image_or_path_or_tensor, meta={}):\n",
        "    load_time, pre_time, net_time, dec_time, post_time = 0, 0, 0, 0, 0\n",
        "    merge_time, track_time, tot_time, display_time = 0, 0, 0, 0\n",
        "    self.debugger.clear()\n",
        "    start_time = time.time()\n",
        "    pre_processed = False\n",
        "    pc_dep = None\n",
        "\n",
        "    if isinstance(image_or_path_or_tensor, np.ndarray):\n",
        "      image = image_or_path_or_tensor\n",
        "    elif type(image_or_path_or_tensor) == type (''): \n",
        "      image = cv2.imread(image_or_path_or_tensor)\n",
        "    else:\n",
        "      image = image_or_path_or_tensor['image'][0].numpy()\n",
        "      pre_processed_images = image_or_path_or_tensor\n",
        "      pc_dep = image_or_path_or_tensor.get('pc_dep', None)\n",
        "\n",
        "      if pc_dep is not None:\n",
        "        if self.opt.flip_test:\n",
        "          flipped = torch.flip(pc_dep, [3])\n",
        "          channel = self.opt.pc_feat_channels['pc_vx']\n",
        "          flipped[0, channel, :, :] *= -1\n",
        "          pc_dep = torch.cat((pc_dep, flipped), axis=0)\n",
        "\n",
        "        pc_dep = pc_dep.to(self.opt.device, non_blocking=self.opt.non_block_test)\n",
        "      pre_processed = True\n",
        "\n",
        "    loaded_time = time.time()\n",
        "    load_time += (loaded_time - start_time)\n",
        "    \n",
        "    detections = []\n",
        "    for scale in self.opt.test_scales:\n",
        "      scale_start_time = time.time()\n",
        "      if not pre_processed:\n",
        "        # not prefetch testing\n",
        "        images, meta = self.pre_process(image, scale, meta)\n",
        "      else:\n",
        "        # prefetch testing\n",
        "        images = pre_processed_images['images'][scale][0]\n",
        "        meta = pre_processed_images['meta'][scale]\n",
        "        meta = {k: v.numpy()[0] for k, v in meta.items()}\n",
        "        if 'pre_dets' in pre_processed_images['meta']:\n",
        "          meta['pre_dets'] = pre_processed_images['meta']['pre_dets']\n",
        "        if 'cur_dets' in pre_processed_images['meta']:\n",
        "          meta['cur_dets'] = pre_processed_images['meta']['cur_dets']\n",
        "        # if self.opt.flip_test:\n",
        "        #   images = torch.cat((images, torch.flip(images,[3])), axis=0)\n",
        "\n",
        "      images = images.to(self.opt.device, non_blocking=self.opt.non_block_test)\n",
        "      pre_hms, pre_inds = None, None\n",
        "      if self.opt.tracking:\n",
        "        if self.pre_images is None:\n",
        "          print('Initialize tracking!')\n",
        "          self.pre_images = images\n",
        "          self.tracker.init_track(meta['pre_dets'])\n",
        "        if self.opt.pre_hm:\n",
        "          pre_hms, pre_inds = self._get_additional_inputs(\n",
        "            self.tracker.tracks, meta, with_hm=not self.opt.zero_pre_hm)\n",
        "      \n",
        "      pre_process_time = time.time()\n",
        "      pre_time += pre_process_time - scale_start_time\n",
        "      \n",
        "      output, dets, forward_time = self.process(\n",
        "        images, self.pre_images, pre_hms, pre_inds, return_time=True, pc_dep=pc_dep, meta=meta)\n",
        "      net_time += forward_time - pre_process_time\n",
        "      decode_time = time.time()\n",
        "      dec_time += decode_time - forward_time\n",
        "      \n",
        "      dets = self.post_process(dets, meta, scale)\n",
        "      post_process_time = time.time()\n",
        "      post_time += post_process_time - decode_time\n",
        "\n",
        "      detections.append(dets)\n",
        "\n",
        "      if self.opt.debug >= 2:\n",
        "        self.debug(\n",
        "          self.debugger, images, dets, output, scale, \n",
        "          pre_images=self.pre_images if not self.opt.no_pre_img else None, \n",
        "          pre_hms=pre_hms)\n",
        "\n",
        "    results = self.merge_outputs(detections)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "    merge_time += end_time - post_process_time\n",
        "    \n",
        "    if self.opt.tracking:\n",
        "      public_det = meta['cur_dets'] if self.opt.public_det else None\n",
        "      results = self.tracker.step(results, public_det)\n",
        "      self.pre_images = images\n",
        "\n",
        "    tracking_time = time.time()\n",
        "    track_time += tracking_time - end_time\n",
        "    tot_time += tracking_time - start_time\n",
        "\n",
        "    if self.opt.debug >= 1:\n",
        "      self.show_results(self.debugger, image, results)\n",
        "    self.cnt += 1\n",
        "\n",
        "    show_results_time = time.time()\n",
        "    display_time += show_results_time - end_time\n",
        "    \n",
        "    ret = {'results': results, 'tot': tot_time, 'load': load_time,\n",
        "            'pre': pre_time, 'net': net_time, 'dec': dec_time,\n",
        "            'post': post_time, 'merge': merge_time, 'track': track_time,\n",
        "            'display': display_time}\n",
        "    if self.opt.save_video:\n",
        "      try:\n",
        "        ret.update({'generic': self.debugger.imgs['generic']})\n",
        "      except:\n",
        "        pass\n",
        "    return ret\n",
        "\n",
        "\n",
        "  def _transform_scale(self, image, scale=1):\n",
        "    height, width = image.shape[0:2]\n",
        "    new_height = int(height * scale)\n",
        "    new_width  = int(width * scale)\n",
        "    if self.opt.fix_short > 0:\n",
        "      if height < width:\n",
        "        inp_height = self.opt.fix_short\n",
        "        inp_width = (int(width / height * self.opt.fix_short) + 63) // 64 * 64\n",
        "      else:\n",
        "        inp_height = (int(height / width * self.opt.fix_short) + 63) // 64 * 64\n",
        "        inp_width = self.opt.fix_short\n",
        "      c = np.array([width / 2, height / 2], dtype=np.float32)\n",
        "      s = np.array([width, height], dtype=np.float32)\n",
        "    elif self.opt.fix_res:\n",
        "      inp_height, inp_width = self.opt.input_h, self.opt.input_w\n",
        "      c = np.array([new_width / 2., new_height / 2.], dtype=np.float32)\n",
        "      s = max(height, width) * 1.0\n",
        "      # s = np.array([inp_width, inp_height], dtype=np.float32)\n",
        "    else:\n",
        "      inp_height = (new_height | self.opt.pad) + 1\n",
        "      inp_width = (new_width | self.opt.pad) + 1\n",
        "      c = np.array([new_width // 2, new_height // 2], dtype=np.float32)\n",
        "      s = np.array([inp_width, inp_height], dtype=np.float32)\n",
        "    resized_image = cv2.resize(image, (new_width, new_height))\n",
        "    return resized_image, c, s, inp_width, inp_height, height, width\n",
        "\n",
        "\n",
        "  def pre_process(self, image, scale, input_meta={}):\n",
        "    resized_image, c, s, inp_width, inp_height, height, width = \\\n",
        "      self._transform_scale(image)\n",
        "    trans_input = get_affine_transform(c, s, 0, [inp_width, inp_height])\n",
        "    out_height =  inp_height // self.opt.down_ratio\n",
        "    out_width =  inp_width // self.opt.down_ratio\n",
        "    trans_output = get_affine_transform(c, s, 0, [out_width, out_height])\n",
        "\n",
        "    inp_image = cv2.warpAffine(\n",
        "      resized_image, trans_input, (inp_width, inp_height),\n",
        "      flags=cv2.INTER_LINEAR)\n",
        "    inp_image = ((inp_image / 255. - self.mean) / self.std).astype(np.float32)\n",
        "\n",
        "    images = inp_image.transpose(2, 0, 1).reshape(1, 3, inp_height, inp_width)\n",
        "    if self.opt.flip_test:\n",
        "      images = np.concatenate((images, images[:, :, :, ::-1]), axis=0)\n",
        "    images = torch.from_numpy(images)\n",
        "    meta = {'calib': np.array(input_meta['calib'], dtype=np.float32) \\\n",
        "             if 'calib' in input_meta else \\\n",
        "             self._get_default_calib(width, height)}\n",
        "    meta.update({'c': c, 's': s, 'height': height, 'width': width,\n",
        "            'out_height': out_height, 'out_width': out_width,\n",
        "            'inp_height': inp_height, 'inp_width': inp_width,\n",
        "            'trans_input': trans_input, 'trans_output': trans_output})\n",
        "    if 'pre_dets' in input_meta:\n",
        "      meta['pre_dets'] = input_meta['pre_dets']\n",
        "    if 'cur_dets' in input_meta:\n",
        "      meta['cur_dets'] = input_meta['cur_dets']\n",
        "    return images, meta\n",
        "\n",
        "\n",
        "  def _trans_bbox(self, bbox, trans, width, height):\n",
        "    bbox = np.array(copy.deepcopy(bbox), dtype=np.float32)\n",
        "    bbox[:2] = affine_transform(bbox[:2], trans)\n",
        "    bbox[2:] = affine_transform(bbox[2:], trans)\n",
        "    bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, width - 1)\n",
        "    bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, height - 1)\n",
        "    return bbox\n",
        "\n",
        "\n",
        "  def _get_additional_inputs(self, dets, meta, with_hm=True):\n",
        "    trans_input, trans_output = meta['trans_input'], meta['trans_output']\n",
        "    inp_width, inp_height = meta['inp_width'], meta['inp_height']\n",
        "    out_width, out_height = meta['out_width'], meta['out_height']\n",
        "    input_hm = np.zeros((1, inp_height, inp_width), dtype=np.float32)\n",
        "\n",
        "    output_inds = []\n",
        "    for det in dets:\n",
        "      if det['score'] < self.opt.pre_thresh:\n",
        "        continue\n",
        "      bbox = self._trans_bbox(det['bbox'], trans_input, inp_width, inp_height)\n",
        "      bbox_out = self._trans_bbox(\n",
        "        det['bbox'], trans_output, out_width, out_height)\n",
        "      h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]\n",
        "      if (h > 0 and w > 0):\n",
        "        radius = gaussian_radius((math.ceil(h), math.ceil(w)))\n",
        "        radius = max(0, int(radius))\n",
        "        ct = np.array(\n",
        "          [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)\n",
        "        ct_int = ct.astype(np.int32)\n",
        "        if with_hm:\n",
        "          draw_umich_gaussian(input_hm[0], ct_int, radius)\n",
        "        ct_out = np.array(\n",
        "          [(bbox_out[0] + bbox_out[2]) / 2, \n",
        "           (bbox_out[1] + bbox_out[3]) / 2], dtype=np.int32)\n",
        "        output_inds.append(ct_out[1] * out_width + ct_out[0])\n",
        "    if with_hm:\n",
        "      input_hm = input_hm[np.newaxis]\n",
        "      if self.opt.flip_test:\n",
        "        input_hm = np.concatenate((input_hm, input_hm[:, :, :, ::-1]), axis=0)\n",
        "      input_hm = torch.from_numpy(input_hm).to(self.opt.device)\n",
        "    output_inds = np.array(output_inds, np.int64).reshape(1, -1)\n",
        "    output_inds = torch.from_numpy(output_inds).to(self.opt.device)\n",
        "    return input_hm, output_inds\n",
        "\n",
        "\n",
        "  def _get_default_calib(self, width, height):\n",
        "    calib = np.array([[self.rest_focal_length, 0, width / 2, 0], \n",
        "                        [0, self.rest_focal_length, height / 2, 0], \n",
        "                        [0, 0, 1, 0]])\n",
        "    return calib\n",
        "\n",
        "\n",
        "  def _sigmoid_output(self, output):\n",
        "    if 'hm' in output:\n",
        "      output['hm'] = output['hm'].sigmoid_()\n",
        "    if 'hm_hp' in output:\n",
        "      output['hm_hp'] = output['hm_hp'].sigmoid_()\n",
        "    if 'dep' in output:\n",
        "      output['dep'] = 1. / (output['dep'].sigmoid() + 1e-6) - 1.\n",
        "      output['dep'] *= self.opt.depth_scale\n",
        "    if 'dep_sec' in output and self.opt.sigmoid_dep_sec:\n",
        "      output['dep_sec'] = 1. / (output['dep_sec'].sigmoid() + 1e-6) - 1.\n",
        "    return output\n",
        "\n",
        "\n",
        "  def _flip_output(self, output):\n",
        "    average_flips = ['hm', 'wh', 'dep', 'dim', 'dep_sec']\n",
        "    neg_average_flips = ['amodel_offset']\n",
        "    single_flips = ['ltrb', 'nuscenes_att', 'velocity', 'ltrb_amodal', 'reg',\n",
        "      'hp_offset', 'rot', 'tracking', 'pre_hm', 'rot_sec']\n",
        "    for head in output:\n",
        "      if head in average_flips:\n",
        "        output[head] = (output[head][0:1] + flip_tensor(output[head][1:2])) / 2\n",
        "      if head in neg_average_flips:\n",
        "        flipped_tensor = flip_tensor(output[head][1:2])\n",
        "        flipped_tensor[:, 0::2] *= -1\n",
        "        output[head] = (output[head][0:1] + flipped_tensor) / 2\n",
        "      if head in single_flips:\n",
        "        output[head] = output[head][0:1]\n",
        "      if head == 'hps':\n",
        "        output['hps'] = (output['hps'][0:1] + \n",
        "          flip_lr_off(output['hps'][1:2], self.flip_idx)) / 2\n",
        "      if head == 'hm_hp':\n",
        "        output['hm_hp'] = (output['hm_hp'][0:1] + \\\n",
        "          flip_lr(output['hm_hp'][1:2], self.flip_idx)) / 2\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "  def process(self, images, pre_images=None, pre_hms=None,\n",
        "    pre_inds=None, return_time=False, pc_dep=None, meta=None):\n",
        "    with torch.no_grad():\n",
        "      calib = torch.from_numpy(meta['calib']).float().to(images.device).squeeze(0)\n",
        "      torch.cuda.synchronize()\n",
        "      output = self.model(images, pc_dep=pc_dep, calib=calib)[-1]\n",
        "\n",
        "      output = self._sigmoid_output(output)\n",
        "      output.update({'pre_inds': pre_inds})\n",
        "      if self.opt.flip_test:\n",
        "        output = self._flip_output(output)\n",
        "      torch.cuda.synchronize()\n",
        "      forward_time = time.time()\n",
        "      \n",
        "      # dets = generic_decode(output, K=self.opt.K, opt=self.opt)\n",
        "      dets = fusion_decode(output, K=self.opt.K, opt=self.opt)\n",
        "      torch.cuda.synchronize()\n",
        "      for k in dets:\n",
        "        dets[k] = dets[k].detach().cpu().numpy()\n",
        "    if return_time:\n",
        "      return output, dets, forward_time\n",
        "    else:\n",
        "      return output, dets\n",
        "\n",
        "  def post_process(self, dets, meta, scale=1):\n",
        "    dets = generic_post_process(\n",
        "      self.opt, dets, [meta['c']], [meta['s']],\n",
        "      meta['out_height'], meta['out_width'], self.opt.num_classes,\n",
        "      [meta['calib']], meta['height'], meta['width'])\n",
        "    self.this_calib = meta['calib']\n",
        "    \n",
        "    if scale != 1:\n",
        "      for i in range(len(dets[0])):\n",
        "        for k in ['bbox', 'hps']:\n",
        "          if k in dets[0][i]:\n",
        "            dets[0][i][k] = (np.array(\n",
        "              dets[0][i][k], np.float32) / scale).tolist()\n",
        "    return dets[0]\n",
        "\n",
        "  def merge_outputs(self, detections):\n",
        "    assert len(self.opt.test_scales) == 1, 'multi_scale not supported!'\n",
        "    results = []\n",
        "    counter = 0\n",
        "    for i in range(len(detections[0])):\n",
        "      det = detections[0][i]\n",
        "      # filter out detections with low score and negative dimensions\n",
        "      if det['score'] > self.opt.out_thresh and all(det['dim'] > 0):\n",
        "        results.append(det)\n",
        "    return results\n",
        "\n",
        "  def debug(self, debugger, images, dets, output, scale=1, \n",
        "    pre_images=None, pre_hms=None):\n",
        "    img = images[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
        "    img = np.clip(((\n",
        "      img * self.std + self.mean) * 255.), 0, 255).astype(np.uint8)\n",
        "    pred = debugger.gen_colormap(output['hm'][0].detach().cpu().numpy())\n",
        "    debugger.add_blend_img(img, pred, 'pred_hm', trans=self.opt.hm_transparency)\n",
        "    if 'hm_hp' in output:\n",
        "      pred = debugger.gen_colormap_hp(\n",
        "        output['hm_hp'][0].detach().cpu().numpy())\n",
        "      debugger.add_blend_img(img, pred, 'pred_hmhp', trans=self.opt.hm_transparency)\n",
        "\n",
        "    if pre_images is not None:\n",
        "      pre_img = pre_images[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
        "      pre_img = np.clip(((\n",
        "        pre_img * self.std + self.mean) * 255.), 0, 255).astype(np.uint8)\n",
        "      debugger.add_img(pre_img, 'pre_img')\n",
        "      if pre_hms is not None:\n",
        "        pre_hm = debugger.gen_colormap(\n",
        "          pre_hms[0].detach().cpu().numpy())\n",
        "        debugger.add_blend_img(pre_img, pre_hm, 'pre_hm', trans=self.opt.hm_transparency)\n",
        "\n",
        "\n",
        "  def show_results(self, debugger, image, results):\n",
        "    debugger.add_img(image, img_id='generic')\n",
        "    if self.opt.tracking:\n",
        "      debugger.add_img(self.pre_image_ori if self.pre_image_ori is not None else image, \n",
        "        img_id='previous')\n",
        "      self.pre_image_ori = image\n",
        "    \n",
        "    for j in range(len(results)):\n",
        "      if results[j]['score'] > self.opt.vis_thresh:\n",
        "        item = results[j]\n",
        "        if ('bbox' in item):\n",
        "          sc = item['score'] if self.opt.demo == '' or \\\n",
        "            not ('tracking_id' in item) else item['tracking_id']\n",
        "          sc = item['tracking_id'] if self.opt.show_track_color else sc\n",
        "          \n",
        "          debugger.add_coco_bbox(\n",
        "            item['bbox'], item['class'] - 1, sc, img_id='generic')\n",
        "\n",
        "        if 'tracking' in item:\n",
        "          debugger.add_arrow(item['ct'], item['tracking'], img_id='generic')\n",
        "        \n",
        "        tracking_id = item['tracking_id'] if 'tracking_id' in item else -1\n",
        "        if 'tracking_id' in item and self.opt.demo == '' and \\\n",
        "          not self.opt.show_track_color:\n",
        "          debugger.add_tracking_id(\n",
        "            item['ct'], item['tracking_id'], img_id='generic')\n",
        "\n",
        "        if (item['class'] in [1, 2]) and 'hps' in item:\n",
        "          debugger.add_coco_hp(item['hps'], tracking_id=tracking_id,\n",
        "            img_id='generic')\n",
        "\n",
        "    if len(results) > 0 and \\\n",
        "      'dep' in results[0] and 'alpha' in results[0] and 'dim' in results[0]:\n",
        "      debugger.add_3d_detection(\n",
        "        image if not self.opt.qualitative else cv2.resize(\n",
        "          debugger.imgs['pred_hm'], (image.shape[1], image.shape[0])), \n",
        "        False, results, self.this_calib,\n",
        "        vis_thresh=self.opt.vis_thresh, img_id='ddd_pred')\n",
        "      debugger.add_bird_view(\n",
        "        results, vis_thresh=self.opt.vis_thresh,\n",
        "        img_id='bird_pred', cnt=self.cnt)\n",
        "      if self.opt.show_track_color and self.opt.debug == 4:\n",
        "        del debugger.imgs['generic'], debugger.imgs['bird_pred']\n",
        "    if 'ddd_pred' in debugger.imgs:\n",
        "      debugger.imgs['generic'] = debugger.imgs['ddd_pred']\n",
        "    if self.opt.debug == 4:\n",
        "      debugger.save_all_imgs(self.opt.debug_dir, prefix='{}'.format(self.cnt))\n",
        "    else:\n",
        "      debugger.show_all_imgs(pause=self.pause)\n",
        "  \n",
        "\n",
        "  def reset_tracking(self):\n",
        "    self.tracker.reset()\n",
        "    self.pre_images = None\n",
        "    self.pre_image_ori = None\n"
      ],
      "metadata": {
        "id": "6jFpvn-8ObWb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "ononB62gOMG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# import _init_paths\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from progress.bar import Bar\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "# from opts import opts\n",
        "# from logger import Logger\n",
        "# from utils.utils import AverageMeter\n",
        "# from dataset.dataset_factory import dataset_factory\n",
        "# from detector import Detector\n",
        "\n",
        "\n",
        "class PrefetchDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, opt, dataset, pre_process_func):\n",
        "    self.images = dataset.images\n",
        "    self.load_image_func = dataset.coco.loadImgs\n",
        "    self.img_dir = dataset.img_dir\n",
        "    self.pre_process_func = pre_process_func\n",
        "    self.get_default_calib = dataset.get_default_calib\n",
        "    self.opt = opt\n",
        "    self.dataset = dataset\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img_id = self.images[index]\n",
        "    img_info = self.load_image_func(ids=[img_id])[0]\n",
        "    img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
        "    image = cv2.imread(img_path)\n",
        "    images, meta = {}, {}\n",
        "    for scale in opt.test_scales:\n",
        "      input_meta = {}\n",
        "      calib = img_info['calib'] if 'calib' in img_info \\\n",
        "        else self.get_default_calib(image.shape[1], image.shape[0])\n",
        "      input_meta['calib'] = calib\n",
        "      images[scale], meta[scale] = self.pre_process_func(\n",
        "        image, scale, input_meta)\n",
        "      \n",
        "    ret = {'images': images, 'image': image, 'meta': meta}\n",
        "    if 'frame_id' in img_info and img_info['frame_id'] == 1:\n",
        "      ret['is_first_frame'] = 1\n",
        "      ret['video_id'] = img_info['video_id']\n",
        "    \n",
        "    # add point cloud\n",
        "    if opt.pointcloud:\n",
        "      assert len(opt.test_scales)==1, \"Multi-scale testing not supported with pointcloud.\"\n",
        "      scale = opt.test_scales[0]\n",
        "      pc_2d, pc_N, pc_dep, pc_3d = self.dataset._load_pc_data(image, img_info, \n",
        "        meta[scale]['trans_input'], meta[scale]['trans_output'])\n",
        "      ret['pc_2d'] = pc_2d\n",
        "      ret['pc_N'] = pc_N\n",
        "      ret['pc_dep'] = pc_dep\n",
        "      ret['pc_3d'] = pc_3d\n",
        "\n",
        "    return img_id, ret\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "def prefetch_test(opt):\n",
        "  if not opt.not_set_cuda_env:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpus_str\n",
        "  Dataset = dataset_factory[opt.test_dataset]\n",
        "  opt = opts().update_dataset_info_and_set_heads(opt, Dataset)\n",
        "  print(opt)\n",
        "  Logger(opt)\n",
        "  \n",
        "  split = 'val' if not opt.trainval else 'test'\n",
        "  if split == 'val':\n",
        "    split = opt.val_split\n",
        "  dataset = Dataset(opt, split)\n",
        "  detector = Detector(opt)\n",
        "  \n",
        "  if opt.load_results != '':\n",
        "    load_results = json.load(open(opt.load_results, 'r'))\n",
        "    for img_id in load_results:\n",
        "      for k in range(len(load_results[img_id])):\n",
        "        if load_results[img_id][k]['class'] - 1 in opt.ignore_loaded_cats:\n",
        "          load_results[img_id][k]['score'] = -1\n",
        "  else:\n",
        "    load_results = {}\n",
        "\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "    PrefetchDataset(opt, dataset, detector.pre_process), \n",
        "    batch_size=1, shuffle=False, num_workers=1, pin_memory=True)\n",
        "\n",
        "  results = {}\n",
        "  num_iters = len(data_loader) if opt.num_iters < 0 else opt.num_iters\n",
        "  bar = Bar('{}'.format(opt.exp_id), max=num_iters)\n",
        "  time_stats = ['tot', 'load', 'pre', 'net', 'dec', 'post', 'merge', 'track']\n",
        "  avg_time_stats = {t: AverageMeter() for t in time_stats}\n",
        "  if opt.use_loaded_results:\n",
        "    for img_id in data_loader.dataset.images:\n",
        "      results[img_id] = load_results['{}'.format(img_id)]\n",
        "    num_iters = 0\n",
        "  for ind, (img_id, pre_processed_images) in enumerate(data_loader):\n",
        "    if ind >= num_iters:\n",
        "      break\n",
        "    if opt.tracking and ('is_first_frame' in pre_processed_images):\n",
        "      if '{}'.format(int(img_id.numpy().astype(np.int32)[0])) in load_results:\n",
        "        pre_processed_images['meta']['pre_dets'] = \\\n",
        "          load_results['{}'.format(int(img_id.numpy().astype(np.int32)[0]))]\n",
        "      else:\n",
        "        print()\n",
        "        print('No pre_dets for', int(img_id.numpy().astype(np.int32)[0]), \n",
        "          '. Use empty initialization.')\n",
        "        pre_processed_images['meta']['pre_dets'] = []\n",
        "      detector.reset_tracking()\n",
        "      print('Start tracking video', int(pre_processed_images['video_id']))\n",
        "    if opt.public_det:\n",
        "      if '{}'.format(int(img_id.numpy().astype(np.int32)[0])) in load_results:\n",
        "        pre_processed_images['meta']['cur_dets'] = \\\n",
        "          load_results['{}'.format(int(img_id.numpy().astype(np.int32)[0]))]\n",
        "      else:\n",
        "        print('No cur_dets for', int(img_id.numpy().astype(np.int32)[0]))\n",
        "        pre_processed_images['meta']['cur_dets'] = []\n",
        "    \n",
        "    ret = detector.run(pre_processed_images)\n",
        "    results[int(img_id.numpy().astype(np.int32)[0])] = ret['results']\n",
        "    \n",
        "    Bar.suffix = '[{0}/{1}]|Tot: {total:} |ETA: {eta:} '.format(\n",
        "                   ind, num_iters, total=bar.elapsed_td, eta=bar.eta_td)\n",
        "    for t in avg_time_stats:\n",
        "      avg_time_stats[t].update(ret[t])\n",
        "      Bar.suffix = Bar.suffix + '|{} {tm.val:.3f}s ({tm.avg:.3f}s) '.format(\n",
        "        t, tm = avg_time_stats[t])\n",
        "    if opt.print_iter > 0:\n",
        "      if ind % opt.print_iter == 0:\n",
        "        print('{}/{}| {}'.format(opt.task, opt.exp_id, Bar.suffix))\n",
        "    else:\n",
        "      bar.next()\n",
        "  bar.finish()\n",
        "  if opt.save_results:\n",
        "    print('saving results to', opt.save_dir + '/save_results_{}{}.json'.format(\n",
        "      opt.test_dataset, opt.dataset_version))\n",
        "    json.dump(_to_list(copy.deepcopy(results)), \n",
        "              open(opt.save_dir + '/save_results_{}{}.json'.format(\n",
        "                opt.test_dataset, opt.dataset_version), 'w'))\n",
        "  dataset.run_eval(results, opt.save_dir, n_plots=opt.eval_n_plots, \n",
        "                   render_curves=opt.eval_render_curves)\n",
        "\n",
        "def test(opt):\n",
        "  os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpus_str\n",
        "\n",
        "  Dataset = dataset_factory[opt.test_dataset]\n",
        "  opt = opts().update_dataset_info_and_set_heads(opt, Dataset)\n",
        "  print(opt)\n",
        "  Logger(opt)\n",
        "  \n",
        "  split = 'val' if not opt.trainval else 'test'\n",
        "  if split == 'val':\n",
        "    split = opt.val_split\n",
        "  dataset = Dataset(opt, split)\n",
        "  detector = Detector(opt)\n",
        "\n",
        "  if opt.load_results != '': # load results in json\n",
        "    load_results = json.load(open(opt.load_results, 'r'))\n",
        "\n",
        "  results = {}\n",
        "  num_iters = len(dataset) if opt.num_iters < 0 else opt.num_iters\n",
        "  bar = Bar('{}'.format(opt.exp_id), max=num_iters)\n",
        "  time_stats = ['tot', 'load', 'pre', 'net', 'dec', 'post', 'merge']\n",
        "  avg_time_stats = {t: AverageMeter() for t in time_stats}\n",
        "  for ind in range(num_iters):\n",
        "    img_id = dataset.images[ind]\n",
        "    img_info = dataset.coco.loadImgs(ids=[img_id])[0]\n",
        "    img_path = os.path.join(dataset.img_dir, img_info['file_name'])\n",
        "    input_meta = {}\n",
        "    if 'calib' in img_info:\n",
        "      input_meta['calib'] = img_info['calib']\n",
        "    if (opt.tracking and ('frame_id' in img_info) and img_info['frame_id'] == 1):\n",
        "      detector.reset_tracking()\n",
        "      input_meta['pre_dets'] = load_results[img_id]\n",
        "\n",
        "    ret = detector.run(img_path, input_meta)    \n",
        "    results[img_id] = ret['results']\n",
        "\n",
        "    Bar.suffix = '[{0}/{1}]|Tot: {total:} |ETA: {eta:} '.format(\n",
        "                   ind, num_iters, total=bar.elapsed_td, eta=bar.eta_td)\n",
        "    for t in avg_time_stats:\n",
        "      avg_time_stats[t].update(ret[t])\n",
        "      Bar.suffix = Bar.suffix + '|{} {:.3f} '.format(t, avg_time_stats[t].avg)\n",
        "    bar.next()\n",
        "  bar.finish()\n",
        "  if opt.save_results:\n",
        "    print('saving results to', opt.save_dir + '/save_results_{}{}.json'.format(\n",
        "      opt.test_dataset, opt.dataset_version))\n",
        "    json.dump(_to_list(copy.deepcopy(results)), \n",
        "              open(opt.save_dir + '/save_results_{}{}.json'.format(\n",
        "                opt.test_dataset, opt.dataset_version), 'w'))\n",
        "  dataset.run_eval(results, opt.save_dir, n_plots=opt.eval_n_plots, \n",
        "                   trairender_curves=opt.eval_render_curves)\n",
        "\n",
        "\n",
        "def _to_list(results):\n",
        "  for img_id in results:\n",
        "    for t in range(len(results[img_id])):\n",
        "      for k in results[img_id][t]:\n",
        "        if isinstance(results[img_id][t][k], (np.ndarray, np.float32)):\n",
        "          results[img_id][t][k] = results[img_id][t][k].tolist()\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "V2OVckWxL_Tx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Run"
      ],
      "metadata": {
        "id": "uH2EfyKRQ3Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    'root': \"/content\",\n",
        "    'task': 'ddd',\n",
        "    'exp_id': 'centerfusion',\n",
        "    'shuffle_train': True,\n",
        "    'train_split': 'mini_train',\n",
        "    'val_split': 'mini_val',\n",
        "    'val_intervals': '1',\n",
        "    'run_dataset_eval': True,\n",
        "    'nuscenes_att': True,\n",
        "    'velocity': True,\n",
        "    'batch_size': 1,\n",
        "    'lr': 2.5e-4,\n",
        "    'num_epochs': '40',\n",
        "    'lr_step': '50',\n",
        "    'save_point': '20,40,50',\n",
        "    'gpus': '0',\n",
        "    'not_rand_crop': True,\n",
        "    'flip': 0.5,\n",
        "    'shift': 0.1,\n",
        "    'pointcloud': True,\n",
        "    'radar_sweeps': 3,\n",
        "    'pc_z_offset': 0.0,\n",
        "    'pillar_dims': \"1.0, 0.2, 0.2\",\n",
        "    'max_pc_dist': 60.0,\n",
        "    'load_model': \"../models/centernet_baseline_e170.pth\",\n",
        "    'freeze_backbone': True,\n",
        "    'resume':True,\n",
        "\n",
        "}\n",
        "args.update(args_dict)\n",
        "args.parse(args)\n",
        "\n",
        "# train\n",
        "main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "K81Ckd8RMqOV",
        "outputId": "ee50f8bc-c9c8-44ea-b560-924e3b5f1f52"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keep resolution testing.\n",
            "training chunk_sizes: [1]\n",
            "input h w: 448 800\n",
            "heads {'hm': 10, 'reg': 2, 'wh': 2, 'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2, 'dep_sec': 1, 'rot_sec': 8, 'ltrb': 4, 'ltrb_amodal': 4, 'nuscenes_att': 8, 'velocity': 3}\n",
            "weights {'hm': 1, 'reg': 1, 'wh': 0.1, 'dep': 1, 'rot': 1, 'dim': 1, 'amodel_offset': 1, 'dep_sec': 1, 'rot_sec': 1, 'ltrb': 0.1, 'ltrb_amodal': 0.1, 'nuscenes_att': 1, 'velocity': 1}\n",
            "head conv {'hm': [256], 'reg': [256], 'wh': [256], 'dep': [256], 'rot': [256], 'dim': [256], 'amodel_offset': [256], 'dep_sec': [256, 256, 256], 'rot_sec': [256, 256, 256], 'ltrb': [256], 'ltrb_amodal': [256], 'nuscenes_att': [256, 256, 256], 'velocity': [256, 256, 256]}\n",
            "{'task': 'ddd', 'dataset': 'nuscenes', 'test_dataset': 'nuscenes', 'exp_id': 'centerfusion', 'eval': True, 'debug': 0, 'no_pause': True, 'demo': '', 'load_model': '../models/centernet_baseline_e170.pth', 'resume': True, 'gpus': [0], 'num_workers': 4, 'not_cuda_benchmark': True, 'seed': 317, 'not_set_cuda_env': True, 'print_iter': 0, 'save_all': True, 'vis_thresh': 0.3, 'debugger_theme': 'white', 'run_dataset_eval': True, 'save_imgs': [], 'save_img_suffix': '', 'skip_first': -1, 'save_video': True, 'save_framerate': 30, 'resize_video': True, 'video_h': 512, 'video_w': 512, 'transpose_video': True, 'show_track_color': True, 'not_show_bbox': True, 'not_show_number': True, 'qualitative': True, 'tango_color': True, 'arch': 'dla_34', 'dla_node': 'dcn', 'head_conv': {'hm': [256], 'reg': [256], 'wh': [256], 'dep': [256], 'rot': [256], 'dim': [256], 'amodel_offset': [256], 'dep_sec': [256, 256, 256], 'rot_sec': [256, 256, 256], 'ltrb': [256], 'ltrb_amodal': [256], 'nuscenes_att': [256, 256, 256], 'velocity': [256, 256, 256]}, 'num_head_conv': 1, 'head_kernel': 3, 'down_ratio': 4, 'num_classes': 10, 'num_resnet_layers': 101, 'backbone': 'dla34', 'neck': 'dlaup', 'msra_outchannel': 256, 'prior_bias': -4.6, 'heads': {'hm': 10, 'reg': 2, 'wh': 2, 'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2, 'dep_sec': 1, 'rot_sec': 8, 'ltrb': 4, 'ltrb_amodal': 4, 'nuscenes_att': 8, 'velocity': 3}, 'input_res': 800, 'input_h': 448, 'input_w': 800, 'dataset_version': '', 'optim': 'adam', 'lr': 0.00025, 'lr_step': [50], 'save_point': [20, 40, 50], 'num_epochs': '40', 'batch_size': 1, 'master_batch_size': 1, 'num_iters': -1, 'val_intervals': '1', 'trainval': True, 'ltrb': True, 'ltrb_weight': 0.1, 'reset_hm': True, 'reuse_hm': True, 'dense_reg': 1, 'shuffle_train': True, 'flip_test': True, 'test_scales': [1.0], 'nms': True, 'K': 100, 'not_prefetch_test': True, 'fix_short': -1, 'keep_res': True, 'out_thresh': -1, 'depth_scale': 1, 'save_results': True, 'load_results': '', 'use_loaded_results': True, 'ignore_loaded_cats': [], 'model_output_list': True, 'non_block_test': True, 'vis_gt_bev': '', 'test_focal_length': -1, 'not_rand_crop': True, 'not_max_crop': True, 'shift': 0.1, 'scale': 0, 'aug_rot': 0, 'rotate': 0, 'flip': 0.5, 'no_color_aug': True, 'tracking': True, 'pre_hm': True, 'same_aug_pre': True, 'zero_pre_hm': True, 'hm_disturb': 0, 'lost_disturb': 0, 'fp_disturb': 0, 'pre_thresh': -1, 'track_thresh': 0.3, 'new_thresh': 0.3, 'max_frame_dist': 3, 'ltrb_amodal': True, 'ltrb_amodal_weight': 0.1, 'public_det': True, 'zero_tracking': True, 'hungarian': True, 'max_age': -1, 'tracking_weight': 1, 'reg_loss': 'l1', 'hm_weight': 1, 'off_weight': 1, 'wh_weight': 0.1, 'hp_weight': 1, 'hm_hp_weight': 1, 'amodel_offset_weight': 1, 'dep_weight': 1, 'dep_res_weight': 1, 'dim_weight': 1, 'rot_weight': 1, 'nuscenes_att': True, 'nuscenes_att_weight': 1, 'velocity': True, 'velocity_weight': 1, 'custom_dataset_img_path': '', 'custom_dataset_ann_path': '', 'pointcloud': True, 'train_split': 'mini_train', 'val_split': 'mini_val', 'max_pc': 1000, 'r_a': 250, 'r_b': 5, 'img_format': 'jpg', 'max_pc_dist': 60.0, 'freeze_backbone': True, 'radar_sweeps': 3, 'warm_start_weights': True, 'pc_z_offset': 0.0, 'eval_n_plots': 0, 'eval_render_curves': True, 'hm_transparency': 0.7, 'iou_thresh': 0, 'pillar_dims': [1.5, 0.2, 0.2], 'show_velocity': True, 'root': '/content', 'gpus_str': '0', 'pre_img': False, 'fix_res': False, 'pad': 31, 'num_stacks': 1, 'chunk_sizes': [1], 'root_dir': '/', 'data_dir': '/data', 'exp_dir': '/exp/ddd', 'save_dir': '/exp/ddd/centerfusion', 'debug_dir': '/exp/ddd/centerfusion/debug', 'pc_atts': ['x', 'y', 'z', 'dyn_prop', 'id', 'rcs', 'vx', 'vy', 'vx_comp', 'vy_comp', 'is_quality_valid', 'ambig_state', 'x_rms', 'y_rms', 'invalid_state', 'pdh0', 'vx_rms', 'vy_rms'], 'num_img_channels': 3, 'hm_dist_thresh': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0}, 'sigmoid_dep_sec': True, 'hm_to_box_ratio': 0.3, 'secondary_heads': ['velocity', 'nuscenes_att', 'dep_sec', 'rot_sec'], 'custom_head_convs': {'dep_sec': 3, 'rot_sec': 3, 'velocity': 3, 'nuscenes_att': 3}, 'normalize_depth': True, 'disable_frustum': False, 'layers_to_freeze': ['base', 'dla_up', 'ida_up'], 'pc_roi_method': 'pillars', 'pc_feat_lvl': ['pc_dep', 'pc_vx', 'pc_vz'], 'frustumExpansionRatio': 0.0, 'sort_det_by_dist': False, 'pc_feat_channels': {'pc_dep': 0, 'pc_vx': 1, 'pc_vz': 2}, 'output_h': 112, 'output_w': 200, 'output_res': 200, 'weights': {'hm': 1, 'reg': 1, 'wh': 0.1, 'dep': 1, 'rot': 1, 'dim': 1, 'amodel_offset': 1, 'dep_sec': 1, 'rot_sec': 1, 'ltrb': 0.1, 'ltrb_amodal': 0.1, 'nuscenes_att': 1, 'velocity': 1}}\n",
            "Creating model...\n",
            "{'hm': 10, 'reg': 2, 'wh': 2, 'dep': 1, 'rot': 8, 'dim': 3, 'amodel_offset': 2, 'dep_sec': 1, 'rot_sec': 8, 'ltrb': 4, 'ltrb_amodal': 4, 'nuscenes_att': 8, 'velocity': 3}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-fee19b725383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-60e4b033fb68>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-e07475e1eeb9>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(arch, head, head_conv, opt)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0march\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_network_factory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_convs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'heads'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o2nLzX-eMtgA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}